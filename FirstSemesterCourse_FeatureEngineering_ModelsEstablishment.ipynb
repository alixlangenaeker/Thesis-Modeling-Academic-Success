{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eY_RTLCZZSb"
   },
   "source": [
    "# Exporting excel files into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "re-_rT_HrpRs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npoqPrsERqVE",
    "outputId": "ad9af786-84ea-4d3a-d893-024ef385c993"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "directory_path = 'Directory Path'\n",
    "file_pattern = '*.*'\n",
    "file_paths = glob.glob(f'{directory_path}/{file_pattern}')\n",
    "data = {}\n",
    "\n",
    "for file_path in file_paths:\n",
    "    file_name = file_path.split('/')[-1]\n",
    "\n",
    "    try:\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            print(file_name)\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_name.endswith('.csv'):\n",
    "            print(file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            # Handle other file types if needed\n",
    "            print(f\"Ignoring file {file_name} as it has an unsupported extension.\")\n",
    "            continue\n",
    "\n",
    "        data[file_name] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Printing keys and basic information about each DataFrame in the data dictionary\n",
    "for file_name, df in data.items():\n",
    "    print(f\"File Name: {file_name}\")\n",
    "    print(f\"DataFrame Info:\\n{df.info()}\\n{'='*30}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ2frXWDRqVF"
   },
   "source": [
    "# Time Filter for the First Semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XUVmkbtnqyC",
    "outputId": "30573069-b0f2-4ddf-c168-2500dabad52a"
   },
   "outputs": [],
   "source": [
    "def filter_data_by_semester(df, start_date, end_date, date_columns):\n",
    "    # Convert specified columns to datetime\n",
    "    df[date_columns] = df[date_columns].apply(pd.to_datetime, errors='coerce', dayfirst=True)\n",
    "\n",
    "    # Check if all cells in the row are empty\n",
    "    all_empty_mask = df[date_columns].isnull().all(axis=1)\n",
    "\n",
    "    # Check if any non-empty cell in date_columns is within the range\n",
    "    any_within_range_mask = ((df[date_columns].notnull()) &\n",
    "                         ~(df[date_columns] < start_date) &\n",
    "                         ~(df[date_columns] > end_date)).any(axis=1)\n",
    "\n",
    "    # Check if some cells are empty and some are not, but at least one non-empty cell is within the range\n",
    "    mixed_mask = ((df[date_columns].notnull().any(axis=1)) &  # At least one non-empty cell\n",
    "              (any_within_range_mask))  # At least one non-empty cell is within the range\n",
    "\n",
    "    # Combine the masks using logical OR (|)\n",
    "    mask = all_empty_mask | any_within_range_mask | mixed_mask\n",
    "\n",
    "    # Filter DataFrame based on the mask\n",
    "    filtered_df = df[mask].copy()\n",
    "\n",
    "    # Print the indices and date column values of filtered rows\n",
    "    print(f\"Filtered rows for file {file_name}:\")\n",
    "    for index, row in df[~mask].iterrows():\n",
    "        print(f\"Index: {index}, Date column values: {', '.join(str(row[col]) for col in date_columns if not pd.isnull(row[col]))}\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "# Define start and end dates for the two semesters\n",
    "first_semester_start = pd.to_datetime('2020-09-21')\n",
    "second_semester_start = pd.to_datetime('2021-02-08')\n",
    "end_sem_HW1 = pd.to_datetime('2021-01-14')\n",
    "end_sem_HW2 = pd.to_datetime('2021-06-02')\n",
    "end_sem_recht = pd.to_datetime('2021-06-20')\n",
    "end_sem_acc = pd.to_datetime('2021-01-06')\n",
    "end_sem_bi = pd.to_datetime('2021-01-22')\n",
    "end_sem_mark = pd.to_datetime('2021-06-12')\n",
    "end_sem_mp = pd.to_datetime('2021-01-28')\n",
    "end_sem_globale = pd.to_datetime('2021-06-25')\n",
    "end_sem_bafi = pd.to_datetime('2021-06-08')\n",
    "\n",
    "# Define the date columns for each Excel file\n",
    "date_columns = {\n",
    "    '_1_course_info.xlsx': [],\n",
    "    '_4_enrollment_info.xlsx': ['ENROLLMENT_DATE'],\n",
    "    '_7_course_contents_info.xlsx': ['DTCREATED', 'DTMODIFIED', 'START_DATE', 'END_DATE'],\n",
    "    '_7b_course_contents_position.xlsx' : [],\n",
    "    '_8_course_contents_hierarchy.xlsx' : [],\n",
    "     '_9_course_contents_adaptive_release_summary.xlsx' : [],\n",
    "    '_10_course_contents_adaptive_release_global.xlsx': ['AR_START_DATE','AR_END_DATE'],\n",
    "    '_13_announcement_info.xlsx': ['DTCREATED', 'DTMODIFIED', 'START_DATE', 'END_DATE'],\n",
    "    '_14_gradable_items.xlsx': ['DUE_DATE'],\n",
    "    '_15_attempts_and_grades.xlsx': ['ATTEMPT_START_DATE','ATTEMPT_DATE'],\n",
    "    '_16_posts_info.xlsx': ['DTCREATED', 'DTMODIFIED'],\n",
    "    '_17_student_post_interaction.xlsx' : [],\n",
    "    '_18_student_individual_posts_interaction.xlsx': ['FIRST_VIEW', 'LAST_VIEW'],\n",
    "    '_19_activity_accumulator_logs.csv': ['TIMESTAMP'],\n",
    "    '_20_activity_accumulator_sessions.csv': ['FIRST_ACTIVITY', 'LAST_ACTIVITY'],\n",
    "    '_21_toledo_user_activity_logs.csv': ['TIMESTAMP'],\n",
    "    'Grades.xlsx' : []\n",
    "}\n",
    "\n",
    "# Filter data based on the first semester for each file\n",
    "for file_name, df in data.items():\n",
    "    filtered_df_second_semester = filter_data_by_semester(df, first_semester_start, end_sem_HW1, date_columns[file_name])\n",
    "    data[file_name] = filtered_df_second_semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcwaEUV1nqyC",
    "outputId": "9e73fa73-e801-459b-d954-2cd6b0f620aa"
   },
   "outputs": [],
   "source": [
    "# Print out the first few rows of each DataFrame after filtering\n",
    "# Filter data based on the first semester\n",
    "for file_name, df in data.items():\n",
    "    if file_name in date_columns:\n",
    "        print(f\"Filtered data for {file_name} based on the first semester:\")\n",
    "        print(df.head())  # Print the first few rows of the filtered DataFrame\n",
    "        print(\"-\" * 50)  # Print a separator line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFJjXmxRRqVG"
   },
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPo5RhmIfw3u",
    "outputId": "d4721c7f-7069-4fc9-effb-c22b79c1c53f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make empty dictionaries for train- and testsets\n",
    "train_sets = {}\n",
    "test_sets = {}\n",
    "\n",
    "# Loop for each DataFrame in the data dictionary\n",
    "for file_name, df in data.items():\n",
    "    print(f\"File Name: {file_name}\")\n",
    "\n",
    "    try:\n",
    "        nummer = file_name.split('_')[1].split('.')[0]\n",
    "    except IndexError:\n",
    "        print(f\"Warning: File {file_name} does not contain a number in its name. Assigning default number.\")\n",
    "        nummer = \"Default\"\n",
    "\n",
    "    # Name the train- and testsets on the base of the number\n",
    "    if nummer == \"Default\":\n",
    "        train_name = \"train_df_grades\"\n",
    "        test_name = \"test_df_grades\"\n",
    "    else:\n",
    "        train_name = f\"train_df{nummer}\"\n",
    "        test_name = f\"test_df{nummer}\"\n",
    "\n",
    "    # Create an empty DataFrame with the same columns as the original DataFrame\n",
    "    empty_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    # Train-test split for each DataFrame\n",
    "    if len(df) < 2:\n",
    "        train_df = empty_df  # Assign the empty DataFrame\n",
    "        test_df = empty_df   # Assign the empty DataFrame\n",
    "    else:\n",
    "        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Add the train- and testsets to the dictionaries\n",
    "    train_sets[train_name] = train_df\n",
    "    test_sets[test_name] = test_df\n",
    "\n",
    "    print(f\"Train set named: {train_name}\")\n",
    "    print(f\"Test set named: {test_name}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUebFVm4Zdok"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qn7b5YosRqVH",
    "outputId": "644e49ee-aac7-4a37-8dc2-64378324ddfd"
   },
   "outputs": [],
   "source": [
    "#_15_attempts_and_grades.xlsx\n",
    "print(train_sets['train_df15'].info())\n",
    "train_sets['train_df15'][\"ATTEMPT_START_DATE\"] = pd.to_datetime(train_sets['train_df15'][\"ATTEMPT_START_DATE\"],dayfirst=True)\n",
    "train_sets['train_df15'][\"ATTEMPT_DATE\"] = pd.to_datetime(train_sets['train_df15'][\"ATTEMPT_DATE\"],dayfirst=True)\n",
    "train_sets['train_df15'][\"COURSE_PK1\"] = train_sets['train_df15'][\"COURSE_PK1\"].astype(str)\n",
    "train_sets['train_df15'][\"CONTENT_PK1\"] = train_sets['train_df15'][\"CONTENT_PK1\"].astype(str)\n",
    "train_sets['train_df15'][\"USER_PK1\"] = train_sets['train_df15'][\"USER_PK1\"].astype(str)\n",
    "train_sets['train_df15'][\"GRADEBOOK_COLUMN_PK1\"] = train_sets['train_df15'][\"GRADEBOOK_COLUMN_PK1\"].astype(str)\n",
    "one_hot_encoded2 = pd.get_dummies(train_sets['train_df15']['ASSESSMENT_TYPE'], prefix='ASSESSMENT_TYPE_attemptsandgrades')\n",
    "train_sets['train_df15'] = pd.concat([train_sets['train_df15'], one_hot_encoded2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3alAIdT9RqVH",
    "outputId": "0911aadc-e4ba-4357-efd0-710cfb83f750"
   },
   "outputs": [],
   "source": [
    "#_16_posts_info.xlsx\n",
    "print(train_sets['train_df16'].info())\n",
    "train_sets['train_df16'][\"DTCREATED_postsinfo\"] = pd.to_datetime(train_sets['train_df16'][\"DTCREATED\"],dayfirst=True)\n",
    "train_sets['train_df16'][\"DTMODIFIED_postsinfo\"] = pd.to_datetime(train_sets['train_df16'][\"DTMODIFIED\"],dayfirst=True)\n",
    "train_sets['train_df16']['BINNED_MSG_TEXT_LENGTH'] = pd.cut(train_sets['train_df16']['MSG_TEXT_LENGTH'],bins=[-np.inf, 10, 50,100,200,400, np.inf],labels=[\"<10\",\"10-50\",\"50-100\",\"100-200\",\"200-400\",\">400\"])\n",
    "train_sets['train_df16'][\"CONTEXT_PK1\"] = train_sets['train_df16'][\"CONTEXT_PK1\"].astype(str)\n",
    "train_sets['train_df16'][\"THREAD_PK1\"] = train_sets['train_df16'][\"THREAD_PK1\"].astype(str)\n",
    "train_sets['train_df16'][\"REPLY_TO_POST_PK1\"] = train_sets['train_df16'][\"REPLY_TO_POST_PK1\"].astype(str)\n",
    "train_sets['train_df16'][\"POST_PK1\"] = train_sets['train_df16'][\"POST_PK1\"].astype(str)\n",
    "train_sets['train_df16'][\"USER_PK1\"] = train_sets['train_df16'][\"USER_PK1\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKTg9ZysRqVI",
    "outputId": "4bc70775-cf8c-4483-82a3-0ea0eb087de0"
   },
   "outputs": [],
   "source": [
    "#_17_student_post_interaction.xlsx\n",
    "print(train_sets['train_df17'].info())\n",
    "train_sets['train_df17'][\"COURSE_PK1\"] = train_sets['train_df17'][\"COURSE_PK1\"].astype(str)\n",
    "train_sets['train_df17'][\"USER_PK1\"] = train_sets['train_df17'][\"USER_PK1\"].astype(str)\n",
    "train_sets['train_df17'][\"CONTEXT_PK1\"] = train_sets['train_df17'][\"CONTEXT_PK1\"].astype(str)\n",
    "one_hot_encoded3 = pd.get_dummies(train_sets['train_df17']['ACTIVITYTYPE'], prefix='ACTIVITYTYPE')\n",
    "train_sets['train_df17'] = pd.concat([train_sets['train_df17'], one_hot_encoded3], axis=1)\n",
    "train_sets['train_df17']['binned_NUM_READ_POSTS'] = pd.cut(train_sets['train_df17']['NUM_READ_POSTS'],bins=[-np.inf, 5, 10,20,50, np.inf],labels=[\"<5\",\"5-10\",\"10-20\",\"20-50\",\">50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Liyvvf0dRqVI",
    "outputId": "9602155f-1257-446a-9c42-5f7f86dfab95"
   },
   "outputs": [],
   "source": [
    "#_18_student_individual_posts_interaction.xlsx\n",
    "print(train_sets['train_df18'].info())\n",
    "train_sets['train_df18'][\"FIRST_VIEW\"] = pd.to_datetime(train_sets['train_df18'][\"FIRST_VIEW\"],dayfirst=True)\n",
    "train_sets['train_df18'][\"LAST_VIEW\"] = pd.to_datetime(train_sets['train_df18'][\"LAST_VIEW\"],dayfirst=True)\n",
    "train_sets['train_df18'][\"COURSE_PK1\"] = train_sets['train_df18'][\"COURSE_PK1\"].astype(str)\n",
    "train_sets['train_df18'][\"CONTEXT_PK1\"] = train_sets['train_df18'][\"CONTEXT_PK1\"].astype(str)\n",
    "train_sets['train_df18'][\"MESSAGE_PK1\"] = train_sets['train_df18'][\"MESSAGE_PK1\"].astype(str)\n",
    "train_sets['train_df18'][\"USER_PK1\"] = train_sets['train_df18'][\"USER_PK1\"].astype(str)\n",
    "one_hot_encoded4 = pd.get_dummies(train_sets['train_df18']['ACTIVITY_TYPE'], prefix='ACTIVITY_TYPE')\n",
    "train_sets['train_df18'] = pd.concat([train_sets['train_df18'], one_hot_encoded4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DXL5hPBZfdJ",
    "outputId": "c1162376-c457-4ba6-847f-ee9eaa43d164"
   },
   "outputs": [],
   "source": [
    "#_19_activity_accumulator_logs.csv\n",
    "print(train_sets['train_df19'].info())\n",
    "train_sets['train_df19'][\"TIMESTAMP_activity_accumulator_logs\"] = pd.to_datetime(train_sets['train_df19'][\"TIMESTAMP\"],dayfirst=True)\n",
    "train_sets['train_df19'][\"COURSE_PK1\"] = train_sets['train_df19'][\"COURSE_PK1\"].astype(str)\n",
    "train_sets['train_df19'][\"USER_PK1\"] = train_sets['train_df19'][\"USER_PK1\"].astype(str)\n",
    "train_sets['train_df19'][\"CONTENT_PK1\"] = train_sets['train_df19'][\"CONTENT_PK1\"].astype(str)\n",
    "train_sets['train_df19'][\"SESSION_ID\"] = train_sets['train_df19'][\"SESSION_ID\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6oKyEtrRqVJ",
    "outputId": "033b302c-0ea5-4309-aa38-292913288e06"
   },
   "outputs": [],
   "source": [
    "#_21_toledo_user_activity_logs.csv\n",
    "print(train_sets['train_df21'].info())\n",
    "train_sets['train_df21'][\"TIMESTAMP_toledo_user_activity_logs\"] = pd.to_datetime(train_sets['train_df21'][\"TIMESTAMP\"], dayfirst=True)\n",
    "train_sets['train_df21'][\"COURSE_PK1\"] = train_sets['train_df21'][\"COURSE_PK1\"].astype(str)\n",
    "train_sets['train_df21'][\"USER_PK1\"] = train_sets['train_df21'][\"USER_PK1\"].astype(str)\n",
    "train_sets['train_df21'][\"CONTENT_PK1\"] = train_sets['train_df21'][\"CONTENT_PK1\"].astype(str)\n",
    "one_hot_encoded6 = pd.get_dummies(train_sets['train_df21']['EVENT'], prefix='EVENT')\n",
    "train_sets['train_df21'] = pd.concat([train_sets['train_df21'], one_hot_encoded6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOvxs9Fzp0gr",
    "outputId": "032554ac-24e4-4971-cfe7-79b9f7b5f51e"
   },
   "outputs": [],
   "source": [
    "# Preprocessing Y-variabele\n",
    "workgrades = train_sets['train_df_grades'][['USER_PK1','OPO_ID','Score januari','Score juni']]\n",
    "\n",
    "workgrades['Score januari'] = workgrades['Score januari'].replace({'': 0, '#': 0})\n",
    "workgrades['Score januari'] = workgrades['Score januari'].replace([np.nan, np.inf, -np.inf], 0)\n",
    "workgrades['Score januari'] = pd.to_numeric(workgrades['Score januari']).astype(int)\n",
    "\n",
    "bins = [0,7,9,15,20]\n",
    "labels = ['fail (0-7)','deliberation (8-9)','pass (10-15)','very good (16-20)']\n",
    "workgrades['Score_bins'] = pd.cut(workgrades['Score januari'],bins=bins,labels=labels,include_lowest=True)\n",
    "\n",
    "print((workgrades['Score_bins'] == 'fail (0-7)').min())\n",
    "print(workgrades.info())\n",
    "print(workgrades.head())\n",
    "workgrades['Score_bins'] = workgrades['Score_bins'].astype('category')\n",
    "workgrades_encoded = pd.get_dummies(workgrades, columns=['Score_bins'])\n",
    "print(workgrades_encoded)\n",
    "workgrades= pd.merge(workgrades, workgrades_encoded, on=['USER_PK1','Score januari','Score juni','OPO_ID'])\n",
    "workgrades['USER_PK1'] = workgrades['USER_PK1'].astype(str)\n",
    "print(workgrades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOi6ulT_ZhE0"
   },
   "source": [
    "# **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhYJggNXA6Vs"
   },
   "source": [
    "## **Time Management and Regularity Features**\n",
    "\n",
    "Paper 1: \"Profiling students' self-regulation with learning analytics: a proof of concept\", by Liz-Dominguez et al. (2022)\n",
    "\n",
    "Paper 2: \"Predictive power of regularity of pre-class activities in a flipped classroom\" by Jovanovic et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA7JI1lsRqVJ"
   },
   "source": [
    "### Feature N°1: PERCENTAGE_WEEKS_ACTIVITY_ABOVE_MEDIAN\n",
    "\"The relative amount of weeks in which the activity of the user was higher than the median of all students’ activities that week. With activity each event in file 19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrJS8E3nRqVK"
   },
   "outputs": [],
   "source": [
    "time19 = train_sets['train_df19']['TIMESTAMP_activity_accumulator_logs']\n",
    "train_sets['train_df19']['week_of_year'] = time19.dt.isocalendar().week\n",
    "work19 = train_sets['train_df19'][['USER_PK1','week_of_year']]\n",
    "weeklygrouped = work19.groupby(['USER_PK1','week_of_year']).size().reset_index(name='user_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lndKtK2WRqVK"
   },
   "source": [
    "Boxplots representing the user count for each \"week_of_year\" value representing the active students that week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfX6JbxTQYA-",
    "outputId": "3f3ad3cf-2272-42cd-bda3-113a586a1a97"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.boxplot(x='week_of_year',y='user_count',data=weeklygrouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLB6KiPhRqVK"
   },
   "source": [
    "A way to measure procrastination of students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XFeCPeHQajo",
    "outputId": "d1414c4f-a958-4758-c8d5-dc98a652d463"
   },
   "outputs": [],
   "source": [
    "# Median of medians of all weeks\n",
    "if 'user_count_weekly_median' not in weeklygrouped.columns:\n",
    "    median_per_week_train = weeklygrouped.groupby('week_of_year')['user_count'].median().reset_index()\n",
    "    #print(median_per_week.head(40))\n",
    "    weeklygrouped = weeklygrouped.merge(median_per_week_train, on='week_of_year', suffixes=('','_weekly_median'))\n",
    "\n",
    "print(weeklygrouped.info())\n",
    "print(weeklygrouped.head())\n",
    "\n",
    "# Was the activity of user x in week y higher than the median activity of all users that week?\n",
    "weeklygrouped['above_median'] = weeklygrouped['user_count'].values > weeklygrouped['user_count_weekly_median'].values\n",
    "print(weeklygrouped.head())\n",
    "\n",
    "# How many weeks in total were user x's activity above the weekly median?\n",
    "above_median_count = weeklygrouped.groupby('USER_PK1')['above_median'].sum().reset_index()\n",
    "above_median_count.columns = ['USER_PK1','above_median_count']\n",
    "print(above_median_count.head())\n",
    "\n",
    "num_weeks = weeklygrouped['week_of_year'].nunique()\n",
    "print(num_weeks)\n",
    "\n",
    "above_median_count['percentageweeks_above_median'] = (above_median_count['above_median_count']/num_weeks)*100\n",
    "print(above_median_count.head(20))\n",
    "print(above_median_count.info())\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merge = pd.merge(above_median_count, dfG, on = 'USER_PK1', how = 'inner')\n",
    "work = merge[['USER_PK1','above_median_count','percentageweeks_above_median','Score_bins']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zobLXjqUQv23"
   },
   "source": [
    "### Feature N°2: DAYS_UNTIL_FIRST_LOGIN\n",
    "\"Number of days it took each student to login on the course page for the first time. Taking the start of the semester as a starting point\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHoA83O6QxhS",
    "outputId": "b0b2dd05-44d0-4c00-c07c-90c1bd849f70"
   },
   "outputs": [],
   "source": [
    "# Weekly activity (number of events) by students\n",
    "time19 = train_sets['train_df19']\n",
    "print(time19.head())\n",
    "grouped_data = time19.groupby('USER_PK1')\n",
    "\n",
    "start = pd.to_datetime('2020-09-21')\n",
    "end_sem1 = pd.to_datetime('2021-01-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK76-tdjQz4R",
    "outputId": "c391f325-05a8-4f44-8559-c2d7f1d351ea"
   },
   "outputs": [],
   "source": [
    "first_login = grouped_data['TIMESTAMP_activity_accumulator_logs'].min()\n",
    "days_until_first_login = (first_login - start).dt.days\n",
    "print(days_until_first_login)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(days_until_first_login.reset_index(name='days_until_first_login'), on='USER_PK1', how='inner')\n",
    "workingdata_FT2 = merged[['USER_PK1','days_until_first_login']]\n",
    "print(workingdata_FT2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGlYF2QoQjl0"
   },
   "source": [
    "### Feature N°3: ENTROPY_WEEKLY_SESSION_COUNTS\n",
    "\"Entropy of weekly session counts per student. Once again, the sessions are here the events as in file 19, with no operations performed on these\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a73pepd1uzZ0"
   },
   "outputs": [],
   "source": [
    "weekly_session_counts_per_student = time19.groupby(['USER_PK1', 'week_of_year']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAQKWkX2XePX",
    "outputId": "95d7194b-f4af-4197-e054-99ed5d7e668f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Calculating the probability distribution for each student\n",
    "prob_distribution_per_student = weekly_session_counts_per_student.apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "# Calculating the entropy for each student\n",
    "entropy_per_student = prob_distribution_per_student.apply(lambda x: entropy(x, base=2) if x.sum() > 0 else 0, axis=1)\n",
    "\n",
    "print(f\"Entropy of weekly session counts per student:\")\n",
    "print(entropy_per_student)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(entropy_per_student.reset_index(name='entropy_per_student'), on='USER_PK1', how='inner')\n",
    "workingdata_FT5 = merged[['USER_PK1','entropy_per_student']]\n",
    "print(workingdata_FT5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrstKBzPRqVM"
   },
   "source": [
    "## Regularity features\n",
    "\n",
    "Paper 1: \"Differences by course discipline on student behaviorn persistence, and achievement in online courses of undergraduate general education\" by Finnegan et al.\n",
    "\n",
    "Paper 2: \"Predictive power of regularity of pre-class activities in a flipped classroom\" by Jovanovic et al.\n",
    "\n",
    "Paper 3: \"How learning analytics can early predict under-achieving students in a blended medical education course\" by Saqr et al.\n",
    "\n",
    "Paper 4: \"Students matter the most in learning analytics: The effects of internal and instructional conditions in predicting academic success\" by Jovanovic et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN67H0j0BNz0"
   },
   "source": [
    "### Feature N°1: FREQ_CHANGE_ENGAGEMENT_PATTERN\n",
    "\"Frequency of change in a student’s engagement pattern over the days of the week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms2u9nAbBagJ",
    "outputId": "27315c30-fea7-4041-c3e7-8cecf654e679"
   },
   "outputs": [],
   "source": [
    "df19 = train_sets['train_df19']\n",
    "print(df19.info())\n",
    "\n",
    "session_duration = df19.groupby(\"SESSION_ID\")[\"TIMESTAMP_activity_accumulator_logs\"].agg(['min','max'])\n",
    "session_duration['duration'] = session_duration['max'] - session_duration['min']\n",
    "# Converting column to hours\n",
    "session_duration['duration_hours'] = session_duration['duration'] / pd.Timedelta(hours=1)\n",
    "# Limiting durations longer than 8 hours to 8 hours\n",
    "session_duration['duration_hours'] = session_duration['duration_hours'].clip(upper=8)\n",
    "# Converting duration back to normal format\n",
    "session_duration['duration'] = pd.to_timedelta(session_duration['duration_hours'], unit='h')\n",
    "print(session_duration.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDnoxpRuBgcp",
    "outputId": "4122e62f-0f83-47c4-fc40-b85213791cd0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creation of DataFrame with daily activity relative to their weekly total for each student\n",
    "# Extracting week number\n",
    "df19['week_number'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.isocalendar().week\n",
    "\n",
    "# Extracting day of the week\n",
    "df19['day_of_week'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.dayofweek + 1\n",
    "\n",
    "# Specifying the Semester\n",
    "work19_FR1 = df19[(df19['week_number']>=39) | (df19['week_number']<=5)]\n",
    "all_weeks = list(range(39, 53)) + list(range(1,6)) # Assuming weeks are numbered from 1 to 52\n",
    "\n",
    "# Grouping the data based on the student, the week number and the day of the week\n",
    "grouped_data_FR1 = work19_FR1.groupby(['USER_PK1', 'week_number', 'day_of_week'])\n",
    "\n",
    "# Calculating total sessions per day per week per student\n",
    "total_sessions_per_day_per_week_per_student = grouped_data_FR1['SESSION_ID'].nunique().unstack(fill_value=0)\n",
    "print(total_sessions_per_day_per_week_per_student.head(10))\n",
    "\n",
    "# Calculating total sessions per week per student\n",
    "total_sessions_per_week_per_student = total_sessions_per_day_per_week_per_student.sum(axis=1)\n",
    "\n",
    "# Reindexing to ensure all combinations of students and weeks are included, filling missing values with zeros\n",
    "all_students = work19_FR1['USER_PK1'].unique()\n",
    "new_index = pd.MultiIndex.from_product([all_students, all_weeks], names=['USER_PK1', 'week_number'])\n",
    "total_sessions_per_day_per_week_per_student = total_sessions_per_day_per_week_per_student.reindex(new_index, fill_value=0)\n",
    "\n",
    "# Replacing NaN values in weeks with no activity with 0\n",
    "total_sessions_per_day_per_week_per_student.fillna(0, inplace=True)\n",
    "\n",
    "# Recalculating total sessions per week per student after reindexing\n",
    "total_sessions_per_week_per_student = total_sessions_per_day_per_week_per_student.sum(axis=1)\n",
    "print(total_sessions_per_week_per_student.head())\n",
    "\n",
    "# Calculating relative weights per week per student\n",
    "relative_weights_per_week_per_student = total_sessions_per_day_per_week_per_student.div(total_sessions_per_week_per_student, axis=0)\n",
    "\n",
    "# Replacing NaN values in weeks with no activity with 0\n",
    "relative_weights_per_week_per_student.fillna(0,inplace=True)\n",
    "\n",
    "print(relative_weights_per_week_per_student.head())\n",
    "\n",
    "# Calculating the mean squared difference between the vectors of two consecutive weeks\n",
    "# Calculate the difference between consecutive weeks\n",
    "diff_between_weeks = relative_weights_per_week_per_student.diff(axis=0)\n",
    "\n",
    "# Square each difference\n",
    "squared_diff = diff_between_weeks ** 2\n",
    "\n",
    "# Calculate the mean squared difference for each student\n",
    "mean_squared_diff_per_student = squared_diff.mean(axis=1)\n",
    "\n",
    "# Calculate the aggregate number over all weeks for each student\n",
    "aggregate_mean_squared_diff_per_student = mean_squared_diff_per_student.groupby('USER_PK1').sum()\n",
    "\n",
    "print(\"Mean squared difference per student between consecutive weeks:\")\n",
    "print(mean_squared_diff_per_student.head())\n",
    "\n",
    "print(\"\\nAggregate mean squared difference over all weeks for each student:\")\n",
    "print(aggregate_mean_squared_diff_per_student)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "df_grades = workgrades\n",
    "merged_FR1 = df_grades.merge(aggregate_mean_squared_diff_per_student.reset_index(name='aggregate_mean_squared_diff_per_student'), on='USER_PK1', how='inner')\n",
    "print(merged_FR1.head())\n",
    "\n",
    "workingdata_FR1 = merged_FR1[['USER_PK1', 'aggregate_mean_squared_diff_per_student']]\n",
    "print(workingdata_FR1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fyrFbpPB2tE"
   },
   "source": [
    "### Feature N°2: WEEKLY_LOGIN_ENGAGEMENT\n",
    "\"Measuring engagement by looking at login behavior. A student was considered engaged in a certain week when having logged in 3 days or more in that week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtDUCIsiCB_6",
    "outputId": "9e48efd7-db0c-4d10-848c-c1b3d11d00a3"
   },
   "outputs": [],
   "source": [
    "df19 = train_sets['train_df19']\n",
    "\n",
    "# 1. Creating a new column to register the active days per week\n",
    "df19['week_number'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.isocalendar().week\n",
    "df19['day_of_week'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.dayofweek + 1\n",
    "\n",
    "# 2. Grouping the data based on the student and the week number\n",
    "grouped_data_FR2 = df19.groupby(['USER_PK1', 'week_number'])\n",
    "\n",
    "# 3. Calculating the total active days per week for each student\n",
    "active_days_per_week = grouped_data_FR2['day_of_week'].nunique()\n",
    "print(active_days_per_week)\n",
    "\n",
    "# 4. Assigning a score of 1 or 0 based on whether the total active days per week is >= 3\n",
    "weekly_scores = (active_days_per_week >= 3).astype(int)\n",
    "print(weekly_scores)\n",
    "\n",
    "# 5. Summing the weekly scores to calculate the total score for each student\n",
    "total_scores_per_student_FR2 = weekly_scores.groupby('USER_PK1').sum().reset_index(name='total_score_FR2')\n",
    "\n",
    "print(total_scores_per_student_FR2)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "df_grades_FR2 = workgrades\n",
    "merged_FR2 = df_grades_FR2.merge(total_scores_per_student_FR2.reset_index(), on='USER_PK1', how='inner')\n",
    "print(merged_FR2.head())\n",
    "\n",
    "workingdata_FR2 = merged_FR2[['USER_PK1', 'total_score_FR2']]\n",
    "print(workingdata_FR2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUZT-ff8COrO"
   },
   "source": [
    "### Feature N°3: FORUM_ENGAGEMENT\n",
    "\"Measuring engagement by looking at the forum posts views. A score of one was assigned when a student views the course materials more than a Z-score of mean course views (using -1.96 and +1.96 for a two-sided 95% confidence interval)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUnKtm0yCiK2",
    "outputId": "c42e4746-4873-4355-ac0a-778dff674afb"
   },
   "outputs": [],
   "source": [
    "df18 = train_sets['train_df18']\n",
    "\n",
    "num_content_items = df18['MESSAGE_PK1'].nunique()\n",
    "print(num_content_items)\n",
    "\n",
    "message_avg_views = df18.groupby('MESSAGE_PK1')['NUM_VIEWS'].mean()\n",
    "df18 = df18.merge(message_avg_views, on='MESSAGE_PK1', suffixes=('','_avg'))\n",
    "df18['Z_score'] = (df18['NUM_VIEWS'] - df18['NUM_VIEWS_avg']) / df18['NUM_VIEWS'].std()\n",
    "\n",
    "Z_score_threshold = 1.96\n",
    "df18['exceptional_view'] = np.where(df18['Z_score'] > Z_score_threshold, 1, 0)\n",
    "\n",
    "student_scores_FR3 = df18.groupby('USER_PK1')['exceptional_view'].sum().reset_index()\n",
    "student_scores_FR3['exceptional_view_percentage'] = round((student_scores_FR3['exceptional_view'] / num_content_items) * 100, 2)\n",
    "print(student_scores_FR3.head(30))\n",
    "\n",
    "# Merging the data with the students' results\n",
    "df_grades_FR3 = workgrades\n",
    "merged_FR3 = df_grades_FR3.merge(student_scores_FR3.reset_index(), on='USER_PK1', how='inner')\n",
    "print(merged_FR3.head())\n",
    "\n",
    "workingdata_FR3 = merged_FR3[['USER_PK1', 'exceptional_view']]\n",
    "print(workingdata_FR3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26dOO7IiCkPV"
   },
   "source": [
    "### Feature N°4: ASSESSMENT_ENGAGEMENT\n",
    "\"Measuring engagement by looking at assessment attempts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVLF2XcnCvfG",
    "outputId": "36ac0ee3-4b20-4973-998d-7f7821c6125d"
   },
   "outputs": [],
   "source": [
    "df15 = train_sets['train_df15']\n",
    "df15['CONTENT_PK1'].fillna('nan',inplace=True)\n",
    "df15_filtered_FR4 = df15[df15['CONTENT_PK1'] != 'nan']\n",
    "print(df15_filtered_FR4.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUn9Eq2bCxIR",
    "outputId": "4d60a9cf-cb05-42b7-8165-f31472a4e50b"
   },
   "outputs": [],
   "source": [
    "# Creation of column where each row represents an attempt of a specific USER_PK1 & CONTENT_PK1 combination\n",
    "df15_filtered_FR4['attempts'] = 1\n",
    "attempts_counts_FR4 = df15_filtered_FR4.groupby(['USER_PK1','CONTENT_PK1'])['attempts'].size().reset_index(name=('attempts'))\n",
    "total_attempts_per_student_FR4 = attempts_counts_FR4.groupby('USER_PK1')['attempts'].sum().reset_index(name='total_score')\n",
    "print(total_attempts_per_student_FR4)\n",
    "\n",
    "# Look at each unique value of CONTENT_PK1 and check whether this appears in the students' attempts\n",
    "content_items = df15['CONTENT_PK1'].unique()\n",
    "print(len(content_items)) # Aantal content items in totaal\n",
    "\n",
    "user_scores_FR4 = {}\n",
    "for content in content_items:\n",
    "    content_data = df15[df15['CONTENT_PK1'] == content]\n",
    "    content_data = content_data.merge(attempts_counts_FR4, on=['USER_PK1', 'CONTENT_PK1'], how='left')\n",
    "    processed_users = set() # Keep track of processed users such that a user-content item combination is not counted twice\n",
    "    for index, row in content_data.iterrows():\n",
    "        user = row['USER_PK1']\n",
    "        if user in processed_users:\n",
    "            continue # Skip the following loop if user has already been processed for this content item\n",
    "        # Does the user already exist in the dictionary?!\n",
    "        if user in user_scores_FR4:\n",
    "            user_scores_FR4[user] += 1 if row['attempts'] > 0 else 0\n",
    "        else:\n",
    "            user_scores_FR4[user] = 1 if row['attempts'] > 0 else 0\n",
    "        processed_users.add(user) # Add the user to the set of processed users\n",
    "total_scores_FR4 = pd.DataFrame(list(user_scores_FR4.items()), columns=['USER_PK1','total_score'])\n",
    "print(total_scores_FR4)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "df_grades_FR4 = workgrades\n",
    "merged_FR4 = df_grades_FR4.merge(total_scores_FR4.reset_index(), on='USER_PK1', how='inner')\n",
    "print(merged_FR4.head())\n",
    "workingdata_FR4 = merged_FR4[['USER_PK1', 'total_score']]\n",
    "print(workingdata_FR4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuVuNo76BM43"
   },
   "source": [
    "## **INTERACTION FEATURES**\n",
    "\n",
    "Paper: \"Learning at distance: Effects of interaction traces on academic achievement\" by Joksimovic et al. (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6zBoQDXHAEs"
   },
   "outputs": [],
   "source": [
    "df17 = train_sets['train_df17']\n",
    "grouped_data = df17.groupby('USER_PK1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JGzySd9B9iC"
   },
   "source": [
    "### Feature N°1: TOTAL_NUM_POSTS_READ\n",
    "\"Number of forum posts read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeYSSJPd7a2X",
    "outputId": "24fef92e-ef90-410f-b2ca-d83ab6d0af79"
   },
   "outputs": [],
   "source": [
    "sum_read_posts = grouped_data['NUM_READ_POSTS'].sum()\n",
    "print(sum_read_posts)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_read_posts.reset_index(name='sum_read_posts'), on='USER_PK1', how='inner')\n",
    "workingdata_FI1 = merged[['USER_PK1','sum_read_posts']]\n",
    "print(workingdata_FI1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w4Ua97CB4-G"
   },
   "source": [
    " ### Feature N°2: TOTAL_NUM_COMMENTS\n",
    " \"Number of comments on forum posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnKXaaNB-U64",
    "outputId": "4a801390-fdb7-4954-f1d0-81f2f4bf555d"
   },
   "outputs": [],
   "source": [
    "sum_comments = grouped_data['NUM_REPLIES_COMMENTS'].sum()\n",
    "print(sum_comments)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_comments.reset_index(name='sum_comments'), on='USER_PK1', how='inner')\n",
    "workingdata_FI2 = merged[['USER_PK1','sum_comments']]\n",
    "print(workingdata_FI2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfYpM2IbZLTr"
   },
   "outputs": [],
   "source": [
    "df18 = train_sets['train_df18']\n",
    "grouped_data = df18.groupby('USER_PK1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1c05pJAZ-wE"
   },
   "source": [
    "### Feature N°3: TOTAL_NUM_VIEWS\n",
    "\"Total number of views on posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2lK_x1jarBX",
    "outputId": "53bbd40e-7128-4277-8631-2e3b124b92a5"
   },
   "outputs": [],
   "source": [
    "sum_views = grouped_data['NUM_VIEWS'].sum()\n",
    "print(sum_views)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_views.reset_index(name='sum_views'), on='USER_PK1', how='inner')\n",
    "workingdata_FI3 = merged[['USER_PK1','sum_views']]\n",
    "print(workingdata_FI3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUYU1vqdcKyr"
   },
   "source": [
    "### Feature N°4: TOTAL_POSTS_INDICATED_READ\n",
    "\"Number of posts indicated as read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyOPoMokcPIN",
    "outputId": "e42544e1-cc8b-4421-c057-b41f17fc727f"
   },
   "outputs": [],
   "source": [
    "sum_ind_read = grouped_data['READ_STATE_IND'].sum()\n",
    "print(grouped_data.head())\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_ind_read.reset_index(name='sum_ind_read'), on='USER_PK1', how='inner')\n",
    "workingdata_FI4 = merged[['USER_PK1','sum_ind_read']]\n",
    "print(workingdata_FI4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOZxgwYXtEuP"
   },
   "source": [
    "### Feature N°5: MEAN_TEXT_LENGTH_OF_POSTS\n",
    "\"Mean of students’ post length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhcRr7tDJr5D"
   },
   "outputs": [],
   "source": [
    "df16 = train_sets['train_df16']\n",
    "grouped_data = df16.groupby('USER_PK1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_2A6Ye6tLhW",
    "outputId": "547c7dbe-6323-4458-a6db-b9503105af0c"
   },
   "outputs": [],
   "source": [
    "mean_text_length = grouped_data['MSG_TEXT_LENGTH'].mean()\n",
    "print(mean_text_length)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(mean_text_length.reset_index(name='mean_text_length'), on='USER_PK1', how='inner')\n",
    "workingdata_FI5 = merged[['USER_PK1','mean_text_length']]\n",
    "print(workingdata_FI5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiH_HRlq5Jt3"
   },
   "source": [
    "## Second set of features\n",
    "\n",
    "Paper: \"Discovering Unusual Study Patterns Using Anomaly Detection and XAI\" by Tiukhova et al. (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60c6b66a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import entropy\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d001266"
   },
   "outputs": [],
   "source": [
    "# Filling empty cells\n",
    "train_sets['train_df20']['COURSE_PK1'] = train_sets['train_df20']['COURSE_PK1'].fillna(0)\n",
    "train_sets['train_df20']['COURSE_PK1'] = train_sets['train_df20']['COURSE_PK1'].astype(np.int64)\n",
    "\n",
    "train_sets['train_df16']['USER_PK1'] = train_sets['train_df16']['USER_PK1'].fillna(0).astype(int)\n",
    "\n",
    "train_sets['train_df_grades']['Score januari'].replace('#', np.nan, inplace=True)\n",
    "train_sets['train_df_grades']['Score juni'].replace('#', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57aa7125"
   },
   "outputs": [],
   "source": [
    "data_19 = train_sets['train_df19'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ecda813"
   },
   "outputs": [],
   "source": [
    "data_19 = data_19.assign(TIMESTAMP_NEW=pd.to_datetime(data_19['TIMESTAMP'],dayfirst=True))\n",
    "data_19['WEEK'] = data_19.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[1], axis = 1)\n",
    "data_19['YEAR'] =  data_19.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[0], axis = 1)\n",
    "data_19['WEEK'] = data_19['WEEK'].astype(str)\n",
    "data_19['YEAR'] = data_19['YEAR'].astype(str)\n",
    "data_19['WEEK and YEAR'] = 'Week ' + data_19['WEEK'] + ' of ' + data_19['YEAR']\n",
    "data_19 = data_19.drop(columns = ['Unnamed: 0'], errors = 'ignore')\n",
    "\n",
    "data_19['CONTENT_PK1'] = data_19['CONTENT_PK1'].fillna(0)\n",
    "data_19['CONTENT_PK1'] = data_19['CONTENT_PK1'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K5Jz4kUMC2v"
   },
   "outputs": [],
   "source": [
    "train_sets['train_df7']['CONTENT_PK1'] = train_sets['train_df7']['CONTENT_PK1'].astype(str)\n",
    "train_sets['train_df7']['COURSE_PK1'] = train_sets['train_df7']['COURSE_PK1'].astype(str)\n",
    "data_19['CONTENT_PK1'] = data_19['CONTENT_PK1'].astype(str)\n",
    "data_19['COURSE_PK1'] = data_19['COURSE_PK1'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e73fdfc"
   },
   "outputs": [],
   "source": [
    "# we merge activity data with the course content information to get more details on the activity\n",
    "data_19 = data_19.merge(train_sets['train_df7'][['COURSE_PK1', 'CONTENT_PK1',\n",
    "         'CONTENT_TYPE', 'TITLE', 'PATH']], on = ['COURSE_PK1', 'CONTENT_PK1'], how = 'left')\n",
    "data_19['CONTENT_TYPE'] = data_19.apply(lambda x: x['DATA'] if pd.isna(x['CONTENT_TYPE']) else x['CONTENT_TYPE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5df00c29"
   },
   "outputs": [],
   "source": [
    "def new_session_id_func(session_ids, df):\n",
    "    sessions= []\n",
    "    for session_id in tqdm(session_ids):\n",
    "        session = df[df['SESSION_ID'] == session_id]\n",
    "        session = session.reset_index()\n",
    "        indices = session[session['TIME_DIFF_SESSION']>= 7200].index\n",
    "        n = len(indices)-1\n",
    "        for i in sorted(indices,reverse=True):\n",
    "            session.loc[:i-1, 'new_session_id'] = f'session_{session_id}_{n}'\n",
    "            n = n - 1\n",
    "\n",
    "        session['new_session_id'] = session['new_session_id'].fillna(f'session_{session_id}_{len(indices)}')\n",
    "        sessions.append(session)\n",
    "    return pd.concat(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33ed4832"
   },
   "outputs": [],
   "source": [
    "def change_sessions(data):\n",
    "    data = data.sort_values(by = ['SESSION_ID', 'TIMESTAMP_NEW']) # first sort data within a session\n",
    "    data['TIME_DIFF_SESSION'] = data.groupby(by = ['SESSION_ID'])['TIMESTAMP_NEW'].diff().dt.seconds.fillna(0) #calculate the time difference between learning activities within a session\n",
    "    sessions_to_divide = set(data[data['TIME_DIFF_SESSION'] > 7200]['SESSION_ID']) #get the sessions where the difference between learning activities is larger than 2h\n",
    "    imputed_sessions = new_session_id_func(sessions_to_divide, data[data['SESSION_ID'].isin(sessions_to_divide)]) #apply a function that will create a new session id - subsession\n",
    "    imputed_sessions = imputed_sessions.drop(columns = 'index')\n",
    "    imputed_sessions = imputed_sessions.sort_values(by = ['SESSION_ID', 'TIMESTAMP_NEW'])\n",
    "    imputed_sessions = imputed_sessions.reset_index()\n",
    "    imputed_sessions = imputed_sessions.drop(columns = 'index')\n",
    "\n",
    "    left_session = data[~data['SESSION_ID'].isin(sessions_to_divide)] #get the rest of sessions that does not need to be separated\n",
    "    left_session['new_session_id'] = left_session['SESSION_ID'].copy()\n",
    "    new_data = pd.concat([imputed_sessions, left_session])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "874ac0b2",
    "outputId": "ff9facd1-9e7c-4e12-d72a-d3a683cb7138"
   },
   "outputs": [],
   "source": [
    "data_19_new = pd.DataFrame()\n",
    "for course in set(data_19['COURSE_PK1']):\n",
    "    course_data = data_19[data_19['COURSE_PK1'] == course]\n",
    "    course_data_new = change_sessions(course_data)\n",
    "    data_19_new = pd.concat([data_19_new, course_data_new])\n",
    "\n",
    "    print(course_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c23473d7",
    "outputId": "d7b99328-de8c-446b-bea8-7effc87ede63"
   },
   "outputs": [],
   "source": [
    "# each row in the table_19 represents an event (a unique combination of timestamp and content PK)\n",
    "data_19_new['EVENTS'] = 1\n",
    "\n",
    "print(data_19_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4098fc97"
   },
   "outputs": [],
   "source": [
    "# we take only those posts that we have interactions for.\n",
    "data_16 = train_sets['train_df16'][train_sets['train_df16']['CONTEXT_PK1'].isin(list(set(train_sets['train_df17']['CONTEXT_PK1'])))]\n",
    "data_16 = data_16.assign(DTCREATED_NEW=pd.to_datetime(data_16['DTCREATED'],dayfirst=True))\n",
    "data_16['WEEK_CREATED'] = data_16.apply(lambda x: x['DTCREATED_NEW'].isocalendar()[1], axis = 1)\n",
    "data_16['YEAR'] = data_16.apply(lambda x: x['DTCREATED_NEW'].isocalendar()[0], axis = 1)\n",
    "data_16['WEEK_CREATED'] = data_16['WEEK_CREATED'].astype(str)\n",
    "data_16['YEAR'] = data_16['YEAR'].astype(str)\n",
    "data_16['WEEK and YEAR created'] = 'Week ' + data_16['WEEK_CREATED'] + ' of ' + data_16['YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94dee738"
   },
   "outputs": [],
   "source": [
    "data_17 = train_sets['train_df17'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06c130c9"
   },
   "outputs": [],
   "source": [
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "grades = workgrades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cbf7be9"
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a86af75",
    "outputId": "1f247513-2e8d-4043-a615-a62d108c31d0"
   },
   "outputs": [],
   "source": [
    "course_start, course_finish, exam_weeks, semester_weeks  = pd.to_datetime('2020-09-21'),end_sem1,('Week 1 of 2021','Week 2 of 2021','Week 3 of 2021','Week 4 of 2021', 'Week 5 of 2021'), ('Week 39 of 2020','Week 40 of 2020','Week 41 of 2020','Week 42 of 2020','Week 43 of 2020','Week 44 of 2020','Week 45 of 2020','Week 46 of 2020','Week 47 of 2020','Week 48 of 2020','Week 49 of 2020','Week 50 of 2020','Week 51 of 2020','Week 52 of 2020')\n",
    "\n",
    "print(course_start)\n",
    "print(exam_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49e2c10e",
    "outputId": "dae1381f-0d25-4dd7-cfef-ed9343bc1322"
   },
   "outputs": [],
   "source": [
    "course_duration = (course_finish - course_start).days\n",
    "course_duration_weeks = course_duration/7\n",
    "\n",
    "print(course_duration_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3017c474",
    "outputId": "87e1dcb8-2b28-45c1-b9be-16a61f2ab6ad"
   },
   "outputs": [],
   "source": [
    "course_data = data_19_new\n",
    "# filter out the data outside the course timespan\n",
    "course_data = course_data[(course_data['TIMESTAMP_NEW'] > course_start) &(course_data['TIMESTAMP_NEW'] < course_finish)]\n",
    "course_data['DATE'] = course_data['TIMESTAMP_NEW'].dt.date\n",
    "course_data['DATE'] = pd.to_datetime(course_data['DATE'])\n",
    "course_data['PATH'] = course_data['PATH'].fillna('not specified')\n",
    "\n",
    "print(course_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3682f046",
    "outputId": "b26859ef-e159-4115-a328-e830d1583591"
   },
   "outputs": [],
   "source": [
    "course_data_posts = data_16\n",
    "# we only want to take into account the posts of the current academic year\n",
    "course_data_posts = course_data_posts[(course_data_posts['DTCREATED_NEW']>=course_start) & (course_data_posts['DTCREATED_NEW'] <= course_finish)]\n",
    "course_data_posts['DATE_CREATED'] = course_data_posts['DTCREATED_NEW'].dt.date\n",
    "\n",
    "print(course_data_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6d85089"
   },
   "outputs": [],
   "source": [
    "course_data_posts_consume = data_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1214338a"
   },
   "source": [
    "## Overall level of activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9bc2682"
   },
   "source": [
    "The feature engineering is based on the paper  J. Jovanovi ́c, M. Saqr, S. Joksimovi ́c, D. Gaˇsevi ́c, Students matter the most in learning\n",
    "analytics: The effects of internal and instructional conditions in predicting academic\n",
    "success, Computers & Education 172 (2021) 104251."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60e8fdc7"
   },
   "source": [
    "### Feature N°1: ZERO_SESSION_COUNT\n",
    "\"Number of sessions with duration of zero\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9441849c",
    "outputId": "1dbc03b7-d45c-41ad-ab1e-2c470148c77b"
   },
   "outputs": [],
   "source": [
    "# then we need to calculate each session's duration. To do that, we find the max and min timestamp value per session and substract latter from the former\n",
    "# we do it to filter out very long/short sessions\n",
    "session_duration = pd.pivot_table(course_data, values='TIMESTAMP_NEW', index=['COURSE_PK1','USER_PK1', 'new_session_id'],aggfunc='max').fillna(0) - pd.pivot_table(course_data, values='TIMESTAMP_NEW', index=['COURSE_PK1','USER_PK1', 'new_session_id'],aggfunc='min').fillna(0)\n",
    "session_duration = session_duration.reset_index()\n",
    "\n",
    "session_duration['SECONDS'] = session_duration['TIMESTAMP_NEW'] / np.timedelta64(1, 's')\n",
    "session_duration['MINUTES'] = session_duration['TIMESTAMP_NEW'] / np.timedelta64(1, 'm')\n",
    "session_duration['HOURS'] = session_duration['TIMESTAMP_NEW'] / np.timedelta64(1, 'h')\n",
    "\n",
    "print(session_duration.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18f18a2a",
    "outputId": "0a3774a1-ab61-4156-ad46-9b9eb9cb0285"
   },
   "outputs": [],
   "source": [
    "# we calculate the number of sessions with no duration - could be quick access to the announcements\n",
    "session_count_zero_duration = pd.pivot_table(session_duration[session_duration['SECONDS'] == 0], values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'new_session_id':'SESSION_COUNT'})\n",
    "\n",
    "print(session_count_zero_duration.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b00caf0",
    "outputId": "6593ae46-9b74-461f-90bb-bf47e1f6a1d7"
   },
   "outputs": [],
   "source": [
    "plt.hist(session_count_zero_duration['SESSION_COUNT'], bins= 20)\n",
    "plt.title('TOTAL ZERO LENGTH SESSION COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boxGumkbUQTv"
   },
   "source": [
    "### Feature N°2: NON_ZERO_SESSION_COUNT\n",
    "\"Number of sessions, excluding the ones with duration of zero\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c486f6d9"
   },
   "outputs": [],
   "source": [
    "# We exclude sessions of length 0\n",
    "course_data_non_zero = course_data.drop(course_data[course_data['new_session_id'].isin(set(session_duration[session_duration['SECONDS'] == 0]['new_session_id']))].index).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc7e159f"
   },
   "source": [
    "### Reducing length of the 8 hours+ sessions to 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ae2edd4",
    "outputId": "516387a4-30ef-4f36-ebf8-baf0c36b547c"
   },
   "outputs": [],
   "source": [
    "# We cut sessions with duration of more than 8 hours to 8 hours\n",
    "session_duration['HOURS'] = np.where(session_duration['HOURS'] > 8, 8, session_duration['HOURS'])\n",
    "\n",
    "# We exclude sessions of duration 0\n",
    "session_duration = session_duration.drop(session_duration[session_duration['SECONDS']  == 0].index) # including sessions with 0 duration can spoil the average\n",
    "print(session_duration.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1601c008",
    "outputId": "06600684-ffe7-40e9-ce7b-5fb8f4a6267f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session_duration_plt = plt.hist(session_duration['MINUTES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "706ab4db",
    "outputId": "cbfaf931-18a4-4390-8d05-79b24b9d2abc"
   },
   "outputs": [],
   "source": [
    "# getting session count - non-zero sessions\n",
    "session_count = pd.pivot_table(course_data_non_zero, values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'new_session_id':'SESSION_COUNT'})\n",
    "\n",
    "print(session_count.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e947cd7",
    "outputId": "ede2df54-c173-4129-8fe9-036b919bcf5e"
   },
   "outputs": [],
   "source": [
    "median_number_sessions = int(session_count['SESSION_COUNT'].median())\n",
    "\n",
    "print(median_number_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b2e5141",
    "outputId": "a1427922-2d9f-4007-9e7f-5c558da9fdbe"
   },
   "outputs": [],
   "source": [
    "plt.hist(session_count['SESSION_COUNT'], bins= 20)\n",
    "plt.title('TOTAL SESSION COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cacf50c6"
   },
   "source": [
    "###  Feature N°3: AVERAGE_ACTIONS_PER_SESSION\n",
    "\"Average number of  clicks / learning actions each session\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "436ad4f4",
    "outputId": "0ea269ed-ebae-4b2b-95cf-239c0c8f3f55"
   },
   "outputs": [],
   "source": [
    "# getting the number of events per timestamp for future filtering\n",
    "clicks_per_timestamp = pd.pivot_table(course_data_non_zero, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'new_session_id','TIMESTAMP_NEW'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "089b0169",
    "outputId": "b4b8c412-b5eb-40e6-ad8f-5ff9c73810d0"
   },
   "outputs": [],
   "source": [
    "# replace the rows with #events > 1 by 1\n",
    "# as Toledo logs all the subfolders of an opened parent folder as a separate event, we replace this multiple events by just 1 event of openening a folder\n",
    "mask = clicks_per_timestamp['EVENTS'] > 1\n",
    "column_name = 'EVENTS'\n",
    "clicks_per_timestamp.loc[mask, column_name] = 1\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e57c817",
    "outputId": "87156e87-1313-4a17-ce51-0cc22e49294f"
   },
   "outputs": [],
   "source": [
    "# getting the median number of events (learning actions) per sessions of a user\n",
    "clicks_per_session = pd.pivot_table(clicks_per_timestamp, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'new_session_id'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "clicks_per_user = pd.pivot_table(clicks_per_session, values='EVENTS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').fillna(0).reset_index().rename(columns = {'EVENTS':'CLICKS_PER_SESSION_AVG'})\n",
    "\n",
    "print(clicks_per_session.head()) \n",
    "print(clicks_per_user.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f31332d4",
    "outputId": "48e459ff-a73d-49e5-d5ee-a4cfc4d025b8"
   },
   "outputs": [],
   "source": [
    "plt.hist(clicks_per_user['CLICKS_PER_SESSION_AVG'], bins= 20)\n",
    "plt.title('MEDIAN #LEARNING ACTIONS PER SESSION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba9c60ee"
   },
   "source": [
    "### Feature N°4: TOTAL_SESSION_DURATION\n",
    "\"The total duration of all sessions of each student (in seconds)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccc0605e"
   },
   "outputs": [],
   "source": [
    "# getting total session length per user (in seconds)\n",
    "session_duration_user = pd.pivot_table(session_duration, values='SECONDS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SECONDS':'SESSION_DURATION'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "066b669d",
    "outputId": "29aae297-3692-49ba-cbc2-738f15437685"
   },
   "outputs": [],
   "source": [
    "plt.hist(session_duration_user['SESSION_DURATION'], bins= 20)\n",
    "plt.title('TOTAL SESSION LENGTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1427bc21"
   },
   "source": [
    "### Feature N°5: AVERAGE_SESSION_DURATION\n",
    "\"The median session length of each user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73320a80"
   },
   "outputs": [],
   "source": [
    "# getting average (median) session length per user\n",
    "session_duration_user_avg = pd.pivot_table(session_duration, values='SECONDS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').fillna(0).reset_index().rename(columns = {'SECONDS':'SESSION_DURATION_AVG'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30dfac94",
    "outputId": "a4fadd1a-f262-4799-eddc-66f62a1175b0"
   },
   "outputs": [],
   "source": [
    "plt.hist(session_duration_user_avg['SESSION_DURATION_AVG'], bins= 20)\n",
    "plt.title('MEDIAN SESSION LENGTH - SECONDS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41116e2c"
   },
   "source": [
    "### Feature N°6: PROPORTION_ACTIVE_WEEKS\n",
    "\"The proportion of weeks a user has had an active week - with this being defined as follows; a week is active when the number of active days is higher than the average\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dd9ab47"
   },
   "outputs": [],
   "source": [
    "active_days = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE':'ACTIVE_DAYS'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5a6ce48",
    "outputId": "4b8d86a3-4d5b-4429-8000-f3021cead85b"
   },
   "outputs": [],
   "source": [
    "# getting the number of active days per week per user\n",
    "active_days_per_week = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE':'ACTIVE_DAYS'})\n",
    "\n",
    "print(active_days_per_week.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8235b35e",
    "outputId": "5302ca23-7828-4490-df14-8983f8b2e893"
   },
   "outputs": [],
   "source": [
    "# getting the average number of active days per week for this course\n",
    "active_days_avg_course_train = pd.pivot_table(active_days_per_week, values='ACTIVE_DAYS', index=['COURSE_PK1'],\n",
    "                     aggfunc='mean')\n",
    "active_days_avg_course_train = active_days_avg_course_train.to_dict()\n",
    "\n",
    "print(active_days_avg_course_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93beb600",
    "outputId": "79122992-f677-4886-e072-b0d3b1eab630",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# leaving only those weeks with #active days higher than average #active days in a week per course\n",
    "active_days_per_week = active_days_per_week[active_days_per_week.apply(lambda x: x['ACTIVE_DAYS'] >= active_days_avg_course_train['ACTIVE_DAYS'][x['COURSE_PK1']], axis = 1)]\n",
    "\n",
    "print(active_days_per_week.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0573a992",
    "outputId": "3d55e502-bd2a-4fd2-d166-971e9b2a5b88"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active weeks\n",
    "num_active_weeks = pd.pivot_table(active_days_per_week, values='WEEK and YEAR', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'WEEK and YEAR':'ACTIVE_WEEKS'})\n",
    "num_active_weeks['PROPORTION_ACTIVE_WEEKS'] = num_active_weeks['ACTIVE_WEEKS']/course_duration_weeks\n",
    "\n",
    "print(num_active_weeks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97722753",
    "outputId": "2c9aa571-f1cb-477c-bcee-24f34a7ff442"
   },
   "outputs": [],
   "source": [
    "plt.hist(num_active_weeks['PROPORTION_ACTIVE_WEEKS'], bins= 10)\n",
    "plt.title('PROPORTION OF ACTIVE WEEKS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b95ea5fa"
   },
   "source": [
    "###  Feature N°7: ACTIVE_DAYS_PROPORTION\n",
    "\"By considering the course duration, a calculation is made on the amount of active days of each user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "870cd322",
    "outputId": "5c9ce801-cca0-415a-cac0-7e0b57796be6"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active days\n",
    "active_days['ACTIVE_DAYS_PROPORTION'] = active_days['ACTIVE_DAYS']/course_duration\n",
    "\n",
    "print(active_days.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6078913d",
    "outputId": "26a3db96-9400-47bd-ed54-33743a664eab"
   },
   "outputs": [],
   "source": [
    "plt.hist(active_days['ACTIVE_DAYS_PROPORTION'], bins= 20)\n",
    "plt.title('PROPORTION OF ACTIVE DAYS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de6c679b"
   },
   "source": [
    "### Feature N°8: AVERAGE_ACTIVE_DAYS_PER_WEEK\n",
    "\"The median number of active days per week for each user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "210d90a9",
    "outputId": "45a82bf0-c032-4d48-8cbd-fb5d3987598a"
   },
   "outputs": [],
   "source": [
    "# getting a median number of active days per week\n",
    "active_days_per_week = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index()\n",
    "active_days_per_week_per_user = pd.pivot_table(active_days_per_week, values='DATE', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').reset_index().rename(columns = {'DATE':'AVERAGE_ACTIVE_DAYS_PER_WEEK'})\n",
    "\n",
    "print(active_days_per_week.head())\n",
    "print(active_days_per_week_per_user.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02fd1cc8",
    "outputId": "5a0ca234-6663-4562-b49c-9902aa0b8614"
   },
   "outputs": [],
   "source": [
    "plt.hist(active_days_per_week_per_user['AVERAGE_ACTIVE_DAYS_PER_WEEK'], bins= 10)\n",
    "plt.title('MEDIAN OF #ACTIVE DAYS PER WEEK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "164fa138"
   },
   "source": [
    "### Feature N°9: AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS\n",
    "\"For each user, the median time distance between two consecutive active days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b2f1a7e"
   },
   "outputs": [],
   "source": [
    "course_data = course_data.sort_values(by = ['COURSE_PK1','USER_PK1', 'TIMESTAMP_NEW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3c40ea5",
    "outputId": "f3799596-4c94-446b-e39b-becdcffc8796"
   },
   "outputs": [],
   "source": [
    "max_day_session = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1', 'new_session_id'],\n",
    "                     aggfunc='max').reset_index().sort_values(by = ['COURSE_PK1','USER_PK1','DATE'])\n",
    "\n",
    "max_day_session['DIFF'] = max_day_session['DATE'].diff().dt.days\n",
    "max_day_session['DIFF'] = max_day_session['DIFF'].clip(lower=0)\n",
    "max_day_session['DIFF'] = max_day_session['DIFF'].fillna(0)\n",
    "\n",
    "print(max_day_session.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae6e6695"
   },
   "source": [
    "We are considering only the date and drop the dubplicates so that we do not affect the difference between days when there are more than 1 active study section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00cf9067"
   },
   "outputs": [],
   "source": [
    "max_day_session = max_day_session.drop_duplicates(subset = ['COURSE_PK1','USER_PK1','DATE'], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3511c130",
    "outputId": "b1bd3e16-74e2-45ef-f856-c0575a2355ea"
   },
   "outputs": [],
   "source": [
    "# getting median time distance between two consecutive active days\n",
    "avg_time_distance_user = pd.pivot_table(max_day_session, values='DIFF', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').reset_index().rename(columns = {'DIFF':'AVG_DIFF'})\n",
    "\n",
    "print(avg_time_distance_user.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d22f90f",
    "outputId": "585c2062-bcf2-499c-d004-4907831e000b"
   },
   "outputs": [],
   "source": [
    "plt.hist(avg_time_distance_user['AVG_DIFF'], bins= 20)\n",
    "plt.title('MEDIAN TIME DISTANCE BETWEEN CONSECUTIVE ACTIVE DAYS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "134a0828"
   },
   "source": [
    "## Learning Action Specific Level of Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°1: ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION\n",
    "\"Proportion of active days for forum contribution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ece552c0"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active days for the posts written\n",
    "active_days_posts_written = pd.pivot_table(course_data_posts, values='DATE_CREATED', index=['COURSE_PK1','USER_PK1'],aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE_CREATED':'ACTIVE_DAYS'})\n",
    "active_days_posts_written['ACTIVE_DAYS_PROPORTION'] = active_days_posts_written['ACTIVE_DAYS']/course_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5acb7a58",
    "outputId": "af437997-2574-4025-932e-3996328527b1"
   },
   "outputs": [],
   "source": [
    "plt.hist(active_days_posts_written['ACTIVE_DAYS_PROPORTION'], bins= 10)\n",
    "plt.title('PROPORTION OF ACTIVE DAYS FOR FORUM CONTRIBUTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1535f26f"
   },
   "outputs": [],
   "source": [
    "# getting the number of active days with posts written per week per user\n",
    "active_days_per_week_posts_written = pd.pivot_table(course_data_posts, values='DATE_CREATED', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR created'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE_CREATED':'ACTIVE_DAYS'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c80d2fad"
   },
   "outputs": [],
   "source": [
    "# getting the average number of active days per week per course\n",
    "active_days_avg_posts_written = pd.pivot_table(active_days_per_week_posts_written, values='ACTIVE_DAYS', index=['COURSE_PK1'],\n",
    "                     aggfunc='mean')\n",
    "active_days_avg_posts_written_train = active_days_avg_posts_written.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2530242d"
   },
   "outputs": [],
   "source": [
    "# leaving only those weeks with #active days higher than average #active days in a week per course\n",
    "active_days_per_week_posts_written_train = active_days_per_week_posts_written[active_days_per_week_posts_written.apply(lambda x: x['ACTIVE_DAYS'] >= active_days_avg_posts_written['ACTIVE_DAYS'][x['COURSE_PK1']], axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c693818f"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active weeks for the writing posts activity type\n",
    "num_active_weeks_posts_written = pd.pivot_table(active_days_per_week_posts_written, values='WEEK and YEAR created', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'WEEK and YEAR created':'ACTIVE_WEEKS'})\n",
    "num_active_weeks_posts_written['PROPORTION_ACTIVE_WEEKS'] = num_active_weeks_posts_written['ACTIVE_WEEKS']/course_duration_weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddd838f3",
    "outputId": "6ac46f63-bd18-48b5-9005-18cbec9698f3"
   },
   "outputs": [],
   "source": [
    "plt.hist(num_active_weeks_posts_written['PROPORTION_ACTIVE_WEEKS'], bins= 5)\n",
    "plt.title('PROPORTION OF ACTIVE WEEKS FOR FORUM CONTRIBUTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74cf3951"
   },
   "source": [
    "###  Feature N°2: PROPORTION_POSTS_READ\n",
    "\"The proportion of posts a student has read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8894d1f",
    "outputId": "14557bf4-1a67-4faf-d339-ecc655c71a88"
   },
   "outputs": [],
   "source": [
    "total_number_posts = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'TOTAL'})\n",
    "\n",
    "print(total_number_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60caeb6f",
    "outputId": "835f9b7f-38a2-41f8-d436-098530d4feac"
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'TOTAL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff3a2519",
    "outputId": "2c5dd867-983f-4284-f46f-e64bbe1b93fa"
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(course_data_posts_consume, values='NUM_READ_POSTS', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc='max').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ad0686d",
    "outputId": "b5a153e0-3ba6-45b0-bd5e-d11841df2f84"
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCzWEmz4I_99"
   },
   "outputs": [],
   "source": [
    "course_data_posts['COURSE_PK1'] = course_data_posts['COURSE_PK1'].astype(int)\n",
    "course_data_posts['CONTEXT_PK1'] = course_data_posts['CONTEXT_PK1'].astype(int)\n",
    "\n",
    "course_data_posts_consume['COURSE_PK1'] =course_data_posts_consume['COURSE_PK1'].astype(int)\n",
    "course_data_posts_consume['CONTEXT_PK1'] = course_data_posts_consume['CONTEXT_PK1'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "440f45ba"
   },
   "outputs": [],
   "source": [
    "course_data_posts_consume = course_data_posts_consume.merge(pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index(), on = ['COURSE_PK1','CONTEXT_PK1'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b0a3e1c"
   },
   "outputs": [],
   "source": [
    "course_data_posts_consume['NEW_NUM_READ_POSTS'] = course_data_posts_consume.apply(lambda x: x['NUM_READ_POSTS'] if x['NUM_READ_POSTS'] < x['POST_PK1'] else x['POST_PK1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f624984"
   },
   "outputs": [],
   "source": [
    "proportion_posts_read = pd.pivot_table(course_data_posts_consume, values='NEW_NUM_READ_POSTS', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc='sum').reset_index()\n",
    "proportion_posts_read = proportion_posts_read.merge(total_number_posts, on = ['COURSE_PK1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26d2258e"
   },
   "outputs": [],
   "source": [
    "proportion_posts_read['PROPORTION_POSTS_READ'] = proportion_posts_read['NEW_NUM_READ_POSTS']/proportion_posts_read['TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae9035c7",
    "outputId": "6222530f-4b7c-46d2-a19e-1b14d6bbd7f8"
   },
   "outputs": [],
   "source": [
    "plt.hist(proportion_posts_read['PROPORTION_POSTS_READ'], bins= 5)\n",
    "plt.title('PROPORTION OF POSTS READ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc5e0ca5"
   },
   "source": [
    "### Feature N°3: POSTS_CREATED\n",
    "\"The amount of posts a student has created\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d05ac716",
    "outputId": "1d34306f-dab8-430c-b28b-906c9f9601af"
   },
   "outputs": [],
   "source": [
    "posts_created = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_CREATED'})\n",
    "\n",
    "print(posts_created.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd02ec7f",
    "outputId": "85b7a847-0c16-4d8b-881a-54cc1a2b206d"
   },
   "outputs": [],
   "source": [
    "plt.hist(posts_created['POSTS_CREATED'], bins= 15)\n",
    "plt.title('POSTS_CREATED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "648b9901"
   },
   "source": [
    "## Overall Regularity of Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6848e840"
   },
   "source": [
    "###  Feature N°1: ENTROPY_CLICKS\n",
    "\"Entropy of clicks/learning action counts per session\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28232be9"
   },
   "outputs": [],
   "source": [
    "# calculating total number of clicks per student per course\n",
    "sum_clicks_dict = pd.pivot_table(clicks_per_session, values='EVENTS', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc='sum').to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "491f9dee",
    "outputId": "7f6d5ee3-f58d-45b0-cdbe-b84df3aafb25"
   },
   "outputs": [],
   "source": [
    "clicks_per_session['TOTAL_CLICKS'] = clicks_per_session.apply(lambda x: sum_clicks_dict['EVENTS'][(x['COURSE_PK1'], x['USER_PK1'])], axis = 1)\n",
    "\n",
    "print(clicks_per_session.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f26295c",
    "outputId": "5f4c72de-78c2-4358-b750-6496659eb6b4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clicks_per_session['PROBA'] = clicks_per_session['EVENTS']/clicks_per_session['TOTAL_CLICKS']\n",
    "\n",
    "print(clicks_per_session.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63344eec",
    "outputId": "25f61835-82dc-4ca4-82ce-c523a7555407"
   },
   "outputs": [],
   "source": [
    "# Entropy of learning action counts per session (session of length of 0 are excluded)\n",
    "clicks_entropy = pd.DataFrame(clicks_per_session.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index()\n",
    "\n",
    "print(clicks_entropy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "446dcd9b",
    "outputId": "ab5c7351-ba98-41c0-ad3d-0614e51cfa2b"
   },
   "outputs": [],
   "source": [
    "plt.hist(clicks_entropy['PROBA'], bins= 10)\n",
    "\n",
    "plt.title('ENTROPY OF LEARNING ACTION COUNTS PER SESSION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8240be7d"
   },
   "source": [
    "### Feature N°2: ENTROPY_SESSION_LENGTH\n",
    "\"Entropy of students’ session lengths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baa30027"
   },
   "outputs": [],
   "source": [
    "session_duration_user_dict = pd.pivot_table(session_duration, values='SECONDS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).rename(columns = {'SECONDS':'SESSION_DURATION'}).to_dict()\n",
    "session_duration['TOTAL_DURATION'] = session_duration.apply(lambda x: session_duration_user_dict['SESSION_DURATION'][(x['COURSE_PK1'], x['USER_PK1'])], axis = 1)\n",
    "session_duration['PROBA'] = session_duration['SECONDS']/session_duration['TOTAL_DURATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ac80a4a",
    "outputId": "c40dbe1d-1e7b-4bfa-9ff8-37ddd7d87522"
   },
   "outputs": [],
   "source": [
    "# Entropy of session length\n",
    "duration_entropy = pd.DataFrame(session_duration.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index()\n",
    "\n",
    "print(duration_entropy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a198ba0d",
    "outputId": "657ad8ec-4c86-4fe4-c3e2-9a52d09fcbf7"
   },
   "outputs": [],
   "source": [
    "plt.hist(duration_entropy['PROBA'], bins= 10)\n",
    "\n",
    "plt.title('ENTROPY OF SESSION LENGTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff5e3e23"
   },
   "source": [
    "### Feature N°3: PROPORTION_WEEKS_FIRST_DAY_ACTIVE\n",
    "\"The proportion of weeks for which the students were active on monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc446023"
   },
   "outputs": [],
   "source": [
    "course_data['DAY_OF_WEEK'] = course_data.apply(lambda x: x['DATE'].isoweekday(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc9303f9",
    "outputId": "b16099a9-afe2-4e34-e5e6-c157e295e4ca"
   },
   "outputs": [],
   "source": [
    "weeks_with_active_first_day = pd.pivot_table(course_data[course_data['DAY_OF_WEEK'] == 1], values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'WEEK and YEAR':'ACTIVE_WEEKS'})\n",
    "weeks_with_active_first_day\n",
    "weeks_with_active_first_day['PROPORTION_ACTIVE_WEEKS'] = weeks_with_active_first_day['ACTIVE_WEEKS']/course_duration_weeks\n",
    "\n",
    "print(weeks_with_active_first_day.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f374bfd",
    "outputId": "4b90424b-05ba-4cf9-9cb5-73f4051c888c"
   },
   "outputs": [],
   "source": [
    "plt.hist(weeks_with_active_first_day['PROPORTION_ACTIVE_WEEKS'], bins= 10)\n",
    "\n",
    "plt.title('PROPORTION OF WEEKS WITH ACTIVE FIRST DAY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89b236cf"
   },
   "source": [
    "### Feature N°4: PROPORTION_LA_FIRST_DAY_OF_WEEK\n",
    "\"Proportion of learning actions on monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b16bca2"
   },
   "outputs": [],
   "source": [
    "course_data_non_zero['DAY_OF_WEEK'] = course_data_non_zero.apply(lambda x: x['DATE'].isoweekday(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e35c453b",
    "outputId": "c7265dc6-1489-4574-e64d-8a119c8a0b00"
   },
   "outputs": [],
   "source": [
    "clicks_per_timestamp = pd.pivot_table(course_data_non_zero, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'new_session_id','TIMESTAMP_NEW'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8e03b19",
    "outputId": "755b6fc6-66a2-4170-bec6-cc80b7d56605"
   },
   "outputs": [],
   "source": [
    "# replace the rows with #events > 1 by 1\n",
    "# as Toledo logs all the subfolders of an opened parent folder as a separate event, we replace this multiple events by just 1 event of openening a folder\n",
    "mask = clicks_per_timestamp['EVENTS'] > 1\n",
    "column_name = 'EVENTS'\n",
    "clicks_per_timestamp.loc[mask, column_name] = 1\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c964052"
   },
   "outputs": [],
   "source": [
    "clicks_per_timestamp['DAY_OF_WEEK'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].isoweekday(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "539b4a70"
   },
   "outputs": [],
   "source": [
    "clicks_per_timestamp['WEEK'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[1], axis = 1)\n",
    "clicks_per_timestamp['YEAR'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[0], axis = 1)\n",
    "clicks_per_timestamp['WEEK'] = clicks_per_timestamp['WEEK'].astype(str)\n",
    "clicks_per_timestamp['YEAR'] = clicks_per_timestamp['YEAR'].astype(str)\n",
    "clicks_per_timestamp['WEEK and YEAR'] = 'Week ' + clicks_per_timestamp['WEEK'] + ' of ' + clicks_per_timestamp['YEAR']\n",
    "clicks_per_timestamp['DATE'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].date(), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec7b0d4f",
    "outputId": "efd206ef-aa1e-4664-93e0-463da129b67b"
   },
   "outputs": [],
   "source": [
    "total_learning_actions = pd.pivot_table(clicks_per_timestamp, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "print(total_learning_actions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2ed0813",
    "outputId": "1ec6dbc5-4788-41ca-c1a8-aa1d8a6cc565"
   },
   "outputs": [],
   "source": [
    "learning_actions_first_day = pd.pivot_table(clicks_per_timestamp[clicks_per_timestamp['DAY_OF_WEEK'] == 1], values='EVENTS', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'EVENTS':'EVENTS_FIRST_DAY'})\n",
    "\n",
    "print(learning_actions_first_day.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acef48c1",
    "outputId": "29392cbc-2070-4b21-9fe0-fee0b005ed3e"
   },
   "outputs": [],
   "source": [
    "total_learning_actions = total_learning_actions.merge(learning_actions_first_day, on = ['COURSE_PK1','USER_PK1','WEEK and YEAR'], how = 'left').fillna(0)\n",
    "total_learning_actions['PROPORTION'] = total_learning_actions['EVENTS_FIRST_DAY']/total_learning_actions['EVENTS']\n",
    "\n",
    "print(total_learning_actions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1688cf73",
    "outputId": "dab7580c-5649-4003-c0fd-66a76ff11823"
   },
   "outputs": [],
   "source": [
    "average_proportion_la_first_day = pd.pivot_table(total_learning_actions, values='PROPORTION', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').fillna(0).reset_index()\n",
    "\n",
    "print(average_proportion_la_first_day.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41772d4c",
    "outputId": "c6addfb0-68f5-4715-89e8-adea088fcea5"
   },
   "outputs": [],
   "source": [
    "plt.hist(average_proportion_la_first_day['PROPORTION'], bins= 15)\n",
    "\n",
    "plt.title('PROPORTION OF LA ON THE FIRST DAY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1170e93d"
   },
   "source": [
    "## Learning Actions Specific Regularity of Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f87f4dc"
   },
   "outputs": [],
   "source": [
    "n_weeks = len(set(course_data_non_zero['WEEK and YEAR']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5628cb5d"
   },
   "source": [
    "### Feature N°1: ENTROPY_FORUM_CONTRIBUTION_DAILY\n",
    "\"Entropy of daily posts written \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "087de89a"
   },
   "source": [
    "Entropy for the missing cases is set to the maximal possible entropy - missing not at random cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43ed8802",
    "outputId": "b5ab3d41-f737-4c91-bb87-b27f9a53eec0"
   },
   "outputs": [],
   "source": [
    "posts_per_day = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1','USER_PK1', 'DATE_CREATED'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_PER_DAY'})\n",
    "\n",
    "print(posts_per_day.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d349fd8a",
    "outputId": "97103129-4b2e-4394-ac92-e05643af4024"
   },
   "outputs": [],
   "source": [
    "posts_total = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_TOTAL'})\n",
    "\n",
    "print(posts_total.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71fb54c1"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcd554ec",
    "outputId": "c52ad3b3-df6e-4812-e921-4fdb941c6220"
   },
   "outputs": [],
   "source": [
    "posts_per_day = posts_per_day.merge(posts_total, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "posts_per_day['PROBA'] = (posts_per_day['POSTS_PER_DAY']/posts_per_day['POSTS_TOTAL']).fillna(0)\n",
    "\n",
    "posts_per_day = pd.DataFrame(posts_per_day.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(course_duration))\n",
    "\n",
    "print(posts_per_day.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f34f8fd5",
    "outputId": "09708e15-5563-4c48-928b-af7e1fea71eb"
   },
   "outputs": [],
   "source": [
    "plt.hist(posts_per_day['PROBA'], bins= 10)\n",
    "\n",
    "plt.title('ENTROPY OF DAILY POSTS WRITTEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2552b399"
   },
   "source": [
    "### Feature N°2: ENTROPY_FORUM_CONTRIBUTION_WEEKLY\n",
    "\"Entropy of weekly posts written\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cd9681c"
   },
   "outputs": [],
   "source": [
    "posts_per_week = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR created'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_PER_WEEK'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d503d8d"
   },
   "outputs": [],
   "source": [
    "posts_per_week = posts_per_week.merge(posts_total, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "posts_per_week['PROBA'] = (posts_per_week['POSTS_PER_WEEK']/posts_per_week['POSTS_TOTAL']).fillna(0)\n",
    "\n",
    "posts_per_week = pd.DataFrame(posts_per_week.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(n_weeks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a057534f",
    "outputId": "96684819-3b0e-4e4d-be54-56093dd21f13"
   },
   "outputs": [],
   "source": [
    "plt.hist(posts_per_week['PROBA'], bins= 15)\n",
    "\n",
    "plt.title('ENTROPY OF WEEKLY POSTS WRITTEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eca59f56"
   },
   "source": [
    "# MoneyWalks feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5291376"
   },
   "source": [
    "Based on the paper: V. K. Singh, B. Bozkaya, and A. Pentland, “Money walks: implicit mobility behavior\n",
    "and financial well-being,” PloS one, vol. 10, no. 8, p. e0136628, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3467fcfe"
   },
   "source": [
    "### Feature N°1: DIVERSITY_OVERALL\n",
    "\"A measure that refers  to how evenly sessions are distributed across weeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "de1dff12",
    "outputId": "68d53649-11c2-4657-ca60-d189e7e0fb9f"
   },
   "outputs": [],
   "source": [
    "sessions_per_week = pd.pivot_table(course_data_non_zero, values='new_session_id', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_PER_WEEK'})\n",
    "\n",
    "print(sessions_per_week.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0222d1c",
    "outputId": "44065835-dce7-4a73-e04a-0c6b99a5ae74"
   },
   "outputs": [],
   "source": [
    "sessions_total = pd.pivot_table(course_data_non_zero, values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_TOTAL'})\n",
    "\n",
    "print(sessions_total.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "783056e9",
    "outputId": "eed503ef-42f6-4931-c2da-00f38fafa787"
   },
   "outputs": [],
   "source": [
    "weeks_count_total = pd.pivot_table(course_data_non_zero, values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index()\n",
    "weeks_count_total['LOG NUM OF WEEKS'] = weeks_count_total.apply(lambda x: math.log2(x['WEEK and YEAR']), axis = 1)\n",
    "\n",
    "print(weeks_count_total.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ebaa67c",
    "outputId": "41da5178-0014-46b0-f56c-17e8c8ac1900"
   },
   "outputs": [],
   "source": [
    "sessions_per_week = sessions_per_week.merge(sessions_total, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "sessions_per_week['PROBA'] = (sessions_per_week['SESSIONS_PER_WEEK']/sessions_per_week['SESSIONS_TOTAL']).fillna(0)\n",
    "\n",
    "\n",
    "sessions_per_week_diversity = pd.DataFrame(sessions_per_week.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(n_weeks))\n",
    "sessions_per_week_diversity = sessions_per_week_diversity.merge(weeks_count_total, how = 'left', on = ['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_diversity['DIVERSITY'] =  (sessions_per_week_diversity['PROBA']/sessions_per_week_diversity['LOG NUM OF WEEKS'])\n",
    "\n",
    "print(sessions_per_week.head())\n",
    "print(sessions_per_week_diversity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17bc53e2",
    "outputId": "dfcecdae-db78-4304-b704-cf6f18f32d43",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sessions_per_week_diversity['DIVERSITY'], bins= 15)\n",
    "\n",
    "plt.title('DIVERSITY OVERALL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°2: DIVERSITY_EXAM_PERIOD\n",
    "\"A measure that refers  to how evenly sessions are distributed across weeks during the exam period\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca3d7771",
    "outputId": "72c9fc81-5eb4-40f2-eb63-624da8fad917"
   },
   "outputs": [],
   "source": [
    "sessions_per_week_exam = pd.pivot_table(course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(exam_weeks)], values='new_session_id', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_PER_WEEK_EXAM'})\n",
    "\n",
    "print(sessions_per_week_exam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f44c23c6",
    "outputId": "00cc7e74-a238-452f-85b8-ba154ce9ceba"
   },
   "outputs": [],
   "source": [
    "sessions_total_exam = pd.pivot_table(course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(exam_weeks)], values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_TOTAL_EXAM'})\n",
    "\n",
    "print(sessions_total_exam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ab04f6e"
   },
   "outputs": [],
   "source": [
    "weeks_count_total_exam = pd.pivot_table(course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(exam_weeks)], values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index()\n",
    "weeks_count_total_exam['LOG NUM OF WEEKS'] = weeks_count_total_exam.apply(lambda x: math.log2(x['WEEK and YEAR']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7532204e"
   },
   "outputs": [],
   "source": [
    "sessions_per_week_exam = sessions_per_week_exam.merge(sessions_total_exam, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "sessions_per_week_exam['PROBA'] = (sessions_per_week_exam['SESSIONS_PER_WEEK_EXAM']/sessions_per_week_exam['SESSIONS_TOTAL_EXAM']).fillna(0)\n",
    "\n",
    "\n",
    "sessions_per_week_diversity_exam = pd.DataFrame(sessions_per_week_exam.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(n_weeks))\n",
    "sessions_per_week_diversity_exam = sessions_per_week_diversity_exam.merge(weeks_count_total_exam, how = 'left', on = ['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_diversity_exam['DIVERSITY_EXAM_PERIOD'] =  (sessions_per_week_diversity_exam['PROBA']/sessions_per_week_diversity_exam['LOG NUM OF WEEKS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15e7142a",
    "outputId": "e30758ca-ec7f-4ab5-c0f8-5059e1d93160"
   },
   "outputs": [],
   "source": [
    "plt.hist(sessions_per_week_diversity_exam['DIVERSITY_EXAM_PERIOD'], bins= 15)\n",
    "\n",
    "plt.title('DIVERSITY EXAM PERIOD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1862a757"
   },
   "source": [
    "## Loyalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5b0b6b7"
   },
   "source": [
    "### Feature N°1: LOYALTY_OVERALL\n",
    "\"Proportion of sessions that took place in the top 3 weeks regarding session count. A higher value represents many sessions concentrated in only a few weeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a058e68"
   },
   "outputs": [],
   "source": [
    "top3_sessions = sessions_per_week.groupby(by = ['COURSE_PK1','USER_PK1']).apply(lambda x: x.nlargest(3, \"SESSIONS_PER_WEEK\")).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6c9cefe0"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_top3_weeks = pd.pivot_table(top3_sessions, values='SESSIONS_PER_WEEK', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SESSIONS_PER_WEEK':'SESSIONS_TOTAL_TOP3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d7973f9"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_top3_weeks = sessions_total_in_top3_weeks.merge(sessions_total, how='left', on = ['COURSE_PK1','USER_PK1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ef24b4b"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_top3_weeks['LOYALTY'] = sessions_total_in_top3_weeks['SESSIONS_TOTAL_TOP3']/sessions_total_in_top3_weeks['SESSIONS_TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5228215",
    "outputId": "15a8be3b-0382-494f-e398-a2bd048a77bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sessions_total_in_top3_weeks['LOYALTY'], bins= 15)\n",
    "\n",
    "plt.title('LOYALTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°2: LOYALTY_EXAM\n",
    "\"Proportion of sessions that took place in the top 3 weeks regarding session count, focusing on exam period\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9727967",
    "outputId": "a35e5521-5149-4c25-bdbf-d89b6d3256a8"
   },
   "outputs": [],
   "source": [
    "exam_sessions = sessions_per_week[sessions_per_week['WEEK and YEAR'].isin(exam_weeks)]\n",
    "\n",
    "print(exam_sessions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddc09b17"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_exam_weeks = pd.pivot_table(exam_sessions, values='SESSIONS_PER_WEEK', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SESSIONS_PER_WEEK':'SESSIONS_TOTAL_EXAM'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea71bd9f"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_exam_weeks = sessions_total_in_exam_weeks.merge(sessions_total, how='left', on = ['COURSE_PK1','USER_PK1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6b7e35a"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_exam_weeks['LOYALTY'] = sessions_total_in_exam_weeks['SESSIONS_TOTAL_EXAM']/sessions_total_in_exam_weeks['SESSIONS_TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f97505b",
    "outputId": "63d07672-e72c-44af-f23b-cf7276ca17a1"
   },
   "outputs": [],
   "source": [
    "plt.hist(sessions_total_in_exam_weeks['LOYALTY'], bins= 15)\n",
    "\n",
    "plt.title('LOYALTY EXAM PERIOD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e662756a"
   },
   "source": [
    "### Feature N°1: REGULARITY\n",
    "\"Measurement for the level of diversity in students’ behavior over time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a243b9f"
   },
   "source": [
    "### Diversity semester weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6b5234c2"
   },
   "outputs": [],
   "source": [
    "course_data_non_zero_semester = course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(semester_weeks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98ddcda5"
   },
   "outputs": [],
   "source": [
    "sessions_per_week_semester = pd.pivot_table(course_data_non_zero_semester, values='new_session_id', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_PER_WEEK'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fadde5ff"
   },
   "outputs": [],
   "source": [
    "sessions_total_semester = pd.pivot_table(course_data_non_zero_semester, values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_TOTAL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fed655af"
   },
   "outputs": [],
   "source": [
    "weeks_count_total_semester = pd.pivot_table(course_data_non_zero_semester, values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index()\n",
    "weeks_count_total_semester['LOG NUM OF WEEKS'] = weeks_count_total_semester.apply(lambda x: math.log2(x['WEEK and YEAR']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "146866d7"
   },
   "outputs": [],
   "source": [
    "sessions_per_week_first_month = sessions_per_week_semester.merge(sessions_total_semester, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "sessions_per_week_first_month['PROBA'] = (sessions_per_week_first_month['SESSIONS_PER_WEEK']/sessions_per_week_first_month['SESSIONS_TOTAL']).fillna(0)\n",
    "\n",
    "\n",
    "sessions_per_week_first_month_diversity = pd.DataFrame(sessions_per_week_first_month.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(len(semester_weeks)))\n",
    "sessions_per_week_first_month_diversity = sessions_per_week_first_month_diversity.merge(weeks_count_total_semester, how = 'left', on = ['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_first_month_diversity['DIVERSITY_SEMESTER'] =  sessions_per_week_first_month_diversity['PROBA']/sessions_per_week_first_month_diversity['LOG NUM OF WEEKS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b1a1bce"
   },
   "outputs": [],
   "source": [
    "diversity_for_regularity = sessions_per_week_diversity[['COURSE_PK1','USER_PK1','DIVERSITY']].merge(sessions_per_week_first_month_diversity[['COURSE_PK1','USER_PK1','DIVERSITY_SEMESTER']], how = 'left', on = ['COURSE_PK1','USER_PK1']).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "485f677f"
   },
   "source": [
    "### Loyalty semester weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b3c5cdd"
   },
   "outputs": [],
   "source": [
    "top3_sessions_semester = sessions_per_week[sessions_per_week['WEEK and YEAR'].isin(semester_weeks)].groupby(by = ['COURSE_PK1','USER_PK1']).apply(lambda x: x.nlargest(3, \"SESSIONS_PER_WEEK\")).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67965892"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_top3_weeks_semester = pd.pivot_table(top3_sessions_semester, values='SESSIONS_PER_WEEK', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SESSIONS_PER_WEEK':'SESSIONS_TOTAL_TOP3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a31abf91"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_top3_weeks_semester = sessions_total_in_top3_weeks_semester.merge(sessions_total_semester, how='left', on = ['COURSE_PK1','USER_PK1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c2dbbd6"
   },
   "outputs": [],
   "source": [
    "sessions_total_in_top3_weeks_semester['LOYALTY_SEMESTER'] = sessions_total_in_top3_weeks_semester['SESSIONS_TOTAL_TOP3']/sessions_total_in_top3_weeks_semester['SESSIONS_TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "526d580b"
   },
   "outputs": [],
   "source": [
    "regularity_df = sessions_per_week_diversity[['COURSE_PK1','USER_PK1','DIVERSITY']].merge(sessions_total_in_top3_weeks[['COURSE_PK1','USER_PK1','LOYALTY']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a72cd21"
   },
   "outputs": [],
   "source": [
    "regularity_df = regularity_df.merge(diversity_for_regularity[['COURSE_PK1','USER_PK1','DIVERSITY_SEMESTER']], how = 'left', on = ['COURSE_PK1','USER_PK1']).merge(sessions_total_in_top3_weeks_semester[['COURSE_PK1','USER_PK1','LOYALTY_SEMESTER']], how= 'left', on = ['COURSE_PK1','USER_PK1'] ).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c37d8dab"
   },
   "outputs": [],
   "source": [
    "regularity_df['REGULARITY'] = 1- np.sqrt((regularity_df['DIVERSITY_SEMESTER'] - regularity_df['DIVERSITY'] )**2 +  (regularity_df['LOYALTY_SEMESTER'] - regularity_df['LOYALTY'] )**2)/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0e1d2ac",
    "outputId": "ec60d911-7701-4cc3-94ff-42eb90018c5a"
   },
   "outputs": [],
   "source": [
    "plt.hist(regularity_df['REGULARITY'], bins= 15)\n",
    "\n",
    "plt.title('REGULARITY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc8b9035"
   },
   "source": [
    "# Final merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "780cf4bf"
   },
   "outputs": [],
   "source": [
    "grades['Score januari'] = grades.apply(lambda x: np.nan if (pd.isnull(x['Score januari']) or x['Score januari'] == 'GR' or not str(x['Score januari']).replace('.', '').isdigit()) else int(str(x['Score januari']).split('.')[0]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4947daa",
    "outputId": "2041d088-a3ea-4e19-ac09-81aeb1a4f01b"
   },
   "outputs": [],
   "source": [
    "# Merging all data together\n",
    "course_grades = grades[['USER_PK1', 'Score januari',\n",
    "                            'Score juni']].merge(session_count_zero_duration.rename(columns =\n",
    "                            {'SESSION_COUNT':'ZERO_SESSIONS_COUNT'}),\n",
    "                             how = 'left', on=['USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(session_count.rename(columns = {'SESSION_COUNT':'NON_ZERO_SESSION_COUNT'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(session_duration_user.rename(columns = {'SESSION_DURATION':'TOTAL_SESSION_DURATION'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(session_duration_user_avg.rename(columns = {'SESSION_DURATION_AVG':'AVERAGE_SESSION_DURATION'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(clicks_per_user.rename(columns = {'CLICKS_PER_SESSION_AVG':'AVERAGE_ACTIONS_PER_SESSION'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(num_active_weeks[['COURSE_PK1','USER_PK1','PROPORTION_ACTIVE_WEEKS']],\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(active_days[['COURSE_PK1','USER_PK1','ACTIVE_DAYS_PROPORTION']],\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(active_days_per_week_per_user, how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(avg_time_distance_user.rename(columns = {'AVG_DIFF':'AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades['COURSE_PK1'] = course_grades['COURSE_PK1'].astype(float).fillna(0)\n",
    "course_grades['USER_PK1']=course_grades['USER_PK1'].astype(int)\n",
    "course_grades['COURSE_PK1']=course_grades['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(active_days_posts_written.rename(columns = {'ACTIVE_DAYS_PROPORTION':\n",
    "                                'ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION'})[['COURSE_PK1','USER_PK1',\n",
    "                                'ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION']], how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(num_active_weeks_posts_written.rename(columns = {'PROPORTION_ACTIVE_WEEKS':\n",
    "                                'PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION'})[['COURSE_PK1','USER_PK1',\n",
    "                                'PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION']], how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "course_grades['USER_PK1']=course_grades['USER_PK1'].astype(int)\n",
    "\n",
    "proportion_posts_read['USER_PK1'] = proportion_posts_read['USER_PK1'].astype(int)\n",
    "course_grades = course_grades.merge(proportion_posts_read[['COURSE_PK1','USER_PK1','PROPORTION_POSTS_READ']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "posts_created['USER_PK1']=posts_created['USER_PK1'].astype(int)\n",
    "posts_created['COURSE_PK1']=posts_created['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(posts_created[['COURSE_PK1','USER_PK1','POSTS_CREATED']],how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "clicks_entropy['USER_PK1']=clicks_entropy['USER_PK1'].astype(int)\n",
    "clicks_entropy['COURSE_PK1']=clicks_entropy['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(clicks_entropy.rename(columns={'PROBA':'ENTROPY_CLICKS'}),how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "duration_entropy['USER_PK1']=duration_entropy['USER_PK1'].astype(int)\n",
    "duration_entropy['COURSE_PK1']=duration_entropy['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(duration_entropy.rename(columns={'PROBA':'ENTROPY_SESSION_LENGTH'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "weeks_with_active_first_day['USER_PK1']=weeks_with_active_first_day['USER_PK1'].astype(int)\n",
    "weeks_with_active_first_day['COURSE_PK1']=weeks_with_active_first_day['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(weeks_with_active_first_day.rename(columns={'PROPORTION_ACTIVE_WEEKS':\n",
    "                                    'PROPORTION_WEEKS_FIRST_DAY_ACTIVE'})[['COURSE_PK1','USER_PK1',\n",
    "                                    'PROPORTION_WEEKS_FIRST_DAY_ACTIVE']], how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "average_proportion_la_first_day['USER_PK1']=average_proportion_la_first_day['USER_PK1'].astype(int)\n",
    "average_proportion_la_first_day['COURSE_PK1']=average_proportion_la_first_day['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(average_proportion_la_first_day.rename(columns={'PROPORTION':'PROPORTION_LA_FIRST_DAY_OF_WEEK'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "posts_per_day['USER_PK1']=posts_per_day['USER_PK1'].astype(int)\n",
    "posts_per_day['COURSE_PK1']=posts_per_day['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(posts_per_day.rename(columns={'PROBA':'ENTROPY_FORUM_CONTRIBUTION_DAILY'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "posts_per_week['USER_PK1']=posts_per_week['USER_PK1'].astype(int)\n",
    "posts_per_week['COURSE_PK1']=posts_per_week['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(posts_per_week.rename(columns={'PROBA':'ENTROPY_FORUM_CONTRIBUTION_WEEKLY'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "sessions_per_week_diversity['USER_PK1']=sessions_per_week_diversity['USER_PK1'].astype(int)\n",
    "sessions_per_week_diversity['COURSE_PK1']=sessions_per_week_diversity['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_per_week_diversity.rename(columns={'DIVERSITY':\n",
    "                                'DIVERSITY_OVERALL'})[['COURSE_PK1','USER_PK1','DIVERSITY_OVERALL']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_diversity_exam['USER_PK1']=sessions_per_week_diversity_exam['USER_PK1'].astype(int)\n",
    "sessions_per_week_diversity_exam['COURSE_PK1']=sessions_per_week_diversity_exam['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_per_week_diversity_exam[['COURSE_PK1','USER_PK1','DIVERSITY_EXAM_PERIOD']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "sessions_total_in_top3_weeks['USER_PK1']=sessions_total_in_top3_weeks['USER_PK1'].astype(int)\n",
    "sessions_total_in_top3_weeks['COURSE_PK1']=sessions_total_in_top3_weeks['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_total_in_top3_weeks.rename(columns={'LOYALTY':\n",
    "                                'LOYALTY_OVERALL'})[['COURSE_PK1','USER_PK1','LOYALTY_OVERALL']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "sessions_total_in_exam_weeks['USER_PK1']=sessions_total_in_exam_weeks['USER_PK1'].astype(int)\n",
    "sessions_total_in_exam_weeks['COURSE_PK1']=sessions_total_in_exam_weeks['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_total_in_exam_weeks.rename(columns={'LOYALTY':\n",
    "                                'LOYALTY_EXAM'})[['COURSE_PK1','USER_PK1','LOYALTY_EXAM']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "regularity_df['USER_PK1']=regularity_df['USER_PK1'].astype(int)\n",
    "regularity_df['COURSE_PK1']=regularity_df['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(regularity_df[['COURSE_PK1','USER_PK1','REGULARITY']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "print(course_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b56ffcbc"
   },
   "outputs": [],
   "source": [
    "n_weeks = len(set(course_data_non_zero['WEEK and YEAR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "624f8927",
    "outputId": "b11be405-d79d-4b83-b6ee-298216f5f1b1"
   },
   "outputs": [],
   "source": [
    "course_grades['ZERO_SESSIONS_COUNT'] = course_grades['ZERO_SESSIONS_COUNT'].fillna(0)\n",
    "course_grades['NON_ZERO_SESSION_COUNT'] = course_grades['NON_ZERO_SESSION_COUNT'].fillna(0) # OLA_1\n",
    "course_grades['TOTAL_SESSION_DURATION'] = course_grades['TOTAL_SESSION_DURATION'].fillna(0) # OLA_2\n",
    "course_grades['AVERAGE_SESSION_DURATION'] = course_grades['AVERAGE_SESSION_DURATION'].fillna(0) # OLA_3\n",
    "course_grades['AVERAGE_ACTIONS_PER_SESSION'] = course_grades['AVERAGE_ACTIONS_PER_SESSION'].fillna(0) # OLA_4\n",
    "course_grades['PROPORTION_ACTIVE_WEEKS'] = course_grades['PROPORTION_ACTIVE_WEEKS'].fillna(0) # OLA_8\n",
    "course_grades['ACTIVE_DAYS_PROPORTION'] = course_grades['ACTIVE_DAYS_PROPORTION'].fillna(0) # OLA_5\n",
    "course_grades['AVERAGE_ACTIVE_DAYS_PER_WEEK'] = course_grades['AVERAGE_ACTIVE_DAYS_PER_WEEK'].fillna(0) # OLA_6\n",
    "course_grades['AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS'] = course_grades['AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS'].fillna(course_duration) # OLA_7\n",
    "course_grades['ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION'] = course_grades['ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION'].fillna(0) # LALA_4\n",
    "course_grades['PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION'] = course_grades['PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION'].fillna(0) # LALA_8\n",
    "course_grades['PROPORTION_POSTS_READ'] = course_grades['PROPORTION_POSTS_READ'].fillna(0) # Forum consumption\n",
    "course_grades['ENTROPY_CLICKS'] = course_grades['ENTROPY_CLICKS'].fillna(math.log2(median_number_sessions)) # ORS_1\n",
    "course_grades['ENTROPY_SESSION_LENGTH'] = course_grades['ENTROPY_SESSION_LENGTH'].fillna(math.log2(median_number_sessions)) # ORS_2\n",
    "course_grades['PROPORTION_WEEKS_FIRST_DAY_ACTIVE'] = course_grades['PROPORTION_WEEKS_FIRST_DAY_ACTIVE'].fillna(0) # ORS_3\n",
    "course_grades['PROPORTION_LA_FIRST_DAY_OF_WEEK'] = course_grades['PROPORTION_LA_FIRST_DAY_OF_WEEK'].fillna(0) # ORS_4\n",
    "course_grades['ENTROPY_FORUM_CONTRIBUTION_WEEKLY'] = course_grades['ENTROPY_FORUM_CONTRIBUTION_WEEKLY'].fillna(math.log2(n_weeks)) # LARS_8\n",
    "course_grades['DIVERSITY_OVERALL'] = course_grades['DIVERSITY_OVERALL'].fillna(math.log2(n_weeks)/math.log2(n_weeks)) # diversity overall\n",
    "course_grades['DIVERSITY_EXAM_PERIOD'] = course_grades['DIVERSITY_EXAM_PERIOD'].fillna(math.log2(len(exam_weeks))/math.log2(len(exam_weeks))) # diversity exam\n",
    "course_grades['LOYALTY_OVERALL'] = course_grades['LOYALTY_OVERALL'].fillna(1) # loyalty\n",
    "course_grades['LOYALTY_EXAM'] = course_grades['LOYALTY_EXAM'].fillna(1) # loyalty exam\n",
    "course_grades['REGULARITY'] = course_grades['REGULARITY'].fillna(1) # regularity\n",
    "course_grades['POSTS_CREATED'] = course_grades['POSTS_CREATED'].fillna(0) # forum contribution\n",
    "\n",
    "print(course_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae1de903",
    "outputId": "2e13fa46-d400-4754-a224-9b9881f27748"
   },
   "outputs": [],
   "source": [
    "#Merging our features together with the given features\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "course_grades['Score januari'] = course_grades['Score januari'].astype(int)\n",
    "course_grades['USER_PK1'] = course_grades['USER_PK1'].astype(str)\n",
    "\n",
    "#Merging it all together\n",
    "merged_data1 = pd.merge(course_grades,workingdata_FI1 , on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FI2, on=[\"USER_PK1\"] , how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FI3, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FI4, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1,workingdata_FI5, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FT2, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FT5, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, work, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FR1, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FR2, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FR3, on=[\"USER_PK1\"], how='left')\n",
    "merged_data1 = pd.merge(merged_data1, workingdata_FR4, on=[\"USER_PK1\"], how='left')\n",
    "\n",
    "#Table Check\n",
    "print(merged_data1)\n",
    "\n",
    "#Filling NaN\n",
    "merged_data1['Score_bins'] = merged_data1['Score_bins'].astype(str).fillna(0)\n",
    "merged_data1.fillna(0, inplace=True)\n",
    "print(merged_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bP2x6j8XTrx"
   },
   "source": [
    "# ------- TEST SET PREPARATION -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3598d870"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f074e5a",
    "outputId": "c55ca715-2684-4343-de29-573c66b90a6f"
   },
   "outputs": [],
   "source": [
    "#_15_attempts_and_grades.xlsx\n",
    "print(test_sets['test_df15'].info())\n",
    "test_sets['test_df15'][\"ATTEMPT_START_DATE\"] = pd.to_datetime(test_sets['test_df15'][\"ATTEMPT_START_DATE\"],dayfirst=True)\n",
    "test_sets['test_df15'][\"ATTEMPT_DATE\"] = pd.to_datetime(test_sets['test_df15'][\"ATTEMPT_DATE\"],dayfirst=True)\n",
    "test_sets['test_df15'][\"COURSE_PK1\"] = test_sets['test_df15'][\"COURSE_PK1\"].astype(str)\n",
    "test_sets['test_df15'][\"CONTENT_PK1\"] = test_sets['test_df15'][\"CONTENT_PK1\"].astype(str)\n",
    "test_sets['test_df15'][\"USER_PK1\"] = test_sets['test_df15'][\"USER_PK1\"].astype(str)\n",
    "test_sets['test_df15'][\"GRADEBOOK_COLUMN_PK1\"] = test_sets['test_df15'][\"GRADEBOOK_COLUMN_PK1\"].astype(str)\n",
    "one_hot_encoded2 = pd.get_dummies(test_sets['test_df15']['ASSESSMENT_TYPE'], prefix='ASSESSMENT_TYPE_attemptsandgrades')\n",
    "test_sets['test_df15'] = pd.concat([test_sets['test_df15'], one_hot_encoded2], axis=1)\n",
    "df15 = test_sets['test_df15']\n",
    "print(df15.info())\n",
    "df15['CONTENT_PK1'].fillna('nan',inplace=True)\n",
    "df15_filtered_FR4 = df15[df15['CONTENT_PK1'] != 'nan']\n",
    "print(df15_filtered_FR4.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5255b3b5",
    "outputId": "fd4da9cc-5933-464b-988c-9468d0293989"
   },
   "outputs": [],
   "source": [
    "#_16_posts_info.xlsx\n",
    "print(test_sets['test_df16'].info())\n",
    "test_sets['test_df16'][\"DTCREATED_postsinfo\"] = pd.to_datetime(test_sets['test_df16'][\"DTCREATED\"],dayfirst=True)\n",
    "test_sets['test_df16'][\"DTMODIFIED_postsinfo\"] = pd.to_datetime(test_sets['test_df16'][\"DTMODIFIED\"],dayfirst=True)\n",
    "test_sets['test_df16']['BINNED_MSG_TEXT_LENGTH'] = pd.cut(test_sets['test_df16']['MSG_TEXT_LENGTH'],bins=[-np.inf, 10, 50,100,200,400, np.inf],labels=[\"<10\",\"10-50\",\"50-100\",\"100-200\",\"200-400\",\">400\"])\n",
    "test_sets['test_df16'][\"CONTEXT_PK1\"] = test_sets['test_df16'][\"CONTEXT_PK1\"].astype(str)\n",
    "test_sets['test_df16'][\"THREAD_PK1\"] = test_sets['test_df16'][\"THREAD_PK1\"].astype(str)\n",
    "test_sets['test_df16'][\"REPLY_TO_POST_PK1\"] = test_sets['test_df16'][\"REPLY_TO_POST_PK1\"].astype(str)\n",
    "test_sets['test_df16'][\"POST_PK1\"] = test_sets['test_df16'][\"POST_PK1\"].astype(str)\n",
    "test_sets['test_df16'][\"USER_PK1\"] = test_sets['test_df16'][\"USER_PK1\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cf72790",
    "outputId": "b7973248-f5df-4bd0-b730-3f1cda64a5c4"
   },
   "outputs": [],
   "source": [
    "#_17_student_post_interaction.xlsx\n",
    "print(test_sets['test_df17'].info())\n",
    "test_sets['test_df17'][\"COURSE_PK1\"] = test_sets['test_df17'][\"COURSE_PK1\"].astype(str)\n",
    "test_sets['test_df17'][\"USER_PK1\"] = test_sets['test_df17'][\"USER_PK1\"].astype(str)\n",
    "test_sets['test_df17'][\"CONTEXT_PK1\"] = test_sets['test_df17'][\"CONTEXT_PK1\"].astype(str)\n",
    "one_hot_encoded3 = pd.get_dummies(test_sets['test_df17']['ACTIVITYTYPE'], prefix='ACTIVITYTYPE')\n",
    "test_sets['test_df17'] = pd.concat([test_sets['test_df17'], one_hot_encoded3], axis=1)\n",
    "test_sets['test_df17']['binned_NUM_READ_POSTS'] = pd.cut(test_sets['test_df17']['NUM_READ_POSTS'],bins=[-np.inf, 5, 10,20,50, np.inf],labels=[\"<5\",\"5-10\",\"10-20\",\"20-50\",\">50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83586745",
    "outputId": "c3c6feb7-33c8-45ca-9dc7-10492aad7060"
   },
   "outputs": [],
   "source": [
    "#_18_student_individual_posts_interaction.xlsx\n",
    "print(test_sets['test_df18'].info())\n",
    "test_sets['test_df18'][\"FIRST_VIEW\"] = pd.to_datetime(test_sets['test_df18'][\"FIRST_VIEW\"],dayfirst=True)\n",
    "test_sets['test_df18'][\"LAST_VIEW\"] = pd.to_datetime(test_sets['test_df18'][\"LAST_VIEW\"],dayfirst=True)\n",
    "test_sets['test_df18'][\"COURSE_PK1\"] = test_sets['test_df18'][\"COURSE_PK1\"].astype(str)\n",
    "test_sets['test_df18'][\"CONTEXT_PK1\"] = test_sets['test_df18'][\"CONTEXT_PK1\"].astype(str)\n",
    "test_sets['test_df18'][\"MESSAGE_PK1\"] = test_sets['test_df18'][\"MESSAGE_PK1\"].astype(str)\n",
    "test_sets['test_df18'][\"USER_PK1\"] = test_sets['test_df18'][\"USER_PK1\"].astype(str)\n",
    "one_hot_encoded4 = pd.get_dummies(test_sets['test_df18']['ACTIVITY_TYPE'], prefix='ACTIVITY_TYPE')\n",
    "test_sets['test_df18'] = pd.concat([test_sets['test_df18'], one_hot_encoded4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18f76b36",
    "outputId": "ac4d2206-d499-4e2a-a0bb-5ad6a04ea7f6"
   },
   "outputs": [],
   "source": [
    "#_19_activity_accumulator_logs.csv\n",
    "print(test_sets['test_df19'].info())\n",
    "test_sets['test_df19'][\"TIMESTAMP_activity_accumulator_logs\"] = pd.to_datetime(test_sets['test_df19'][\"TIMESTAMP\"],dayfirst=True)\n",
    "test_sets['test_df19'][\"COURSE_PK1\"] = test_sets['test_df19'][\"COURSE_PK1\"].astype(str)\n",
    "test_sets['test_df19'][\"USER_PK1\"] = test_sets['test_df19'][\"USER_PK1\"].astype(str)\n",
    "test_sets['test_df19'][\"CONTENT_PK1\"] = test_sets['test_df19'][\"CONTENT_PK1\"].astype(str)\n",
    "test_sets['test_df19'][\"SESSION_ID\"] = test_sets['test_df19'][\"SESSION_ID\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "595f620f",
    "outputId": "eb7bb77a-dd9d-402d-8baf-a00a30408bdf"
   },
   "outputs": [],
   "source": [
    "#_21_toledo_user_activity_logs.csv\n",
    "print(test_sets['test_df21'].info())\n",
    "test_sets['test_df21'][\"TIMESTAMP_toledo_user_activity_logs\"] = pd.to_datetime(test_sets['test_df21'][\"TIMESTAMP\"], dayfirst=True)\n",
    "test_sets['test_df21'][\"COURSE_PK1\"] = test_sets['test_df21'][\"COURSE_PK1\"].astype(str)\n",
    "test_sets['test_df21'][\"USER_PK1\"] = test_sets['test_df21'][\"USER_PK1\"].astype(str)\n",
    "test_sets['test_df21'][\"CONTENT_PK1\"] = test_sets['test_df21'][\"CONTENT_PK1\"].astype(str)\n",
    "one_hot_encoded6 = pd.get_dummies(test_sets['test_df21']['EVENT'], prefix='EVENT')\n",
    "test_sets['test_df21'] = pd.concat([test_sets['test_df21'], one_hot_encoded6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lya-0j_i3Fdq",
    "outputId": "5fe9d9bb-e126-44a4-81d4-0e93c35eb4b1"
   },
   "outputs": [],
   "source": [
    "# Preprocessing Y-variabele\n",
    "workgrades = test_sets['test_df_grades'][['USER_PK1','OPO_ID','Score januari','Score juni']]\n",
    "\n",
    "workgrades['Score januari'] = workgrades['Score januari'].replace({'': 0, '#': 0})\n",
    "workgrades['Score januari'] = workgrades['Score januari'].replace([np.nan, np.inf, -np.inf], 0)\n",
    "workgrades['Score januari'] = pd.to_numeric(workgrades['Score januari']).astype(int)\n",
    "\n",
    "bins = [0,7,9,15,20]\n",
    "labels = ['fail (0-7)','deliberation (8-9)','pass (10-15)','very good (16-20)']\n",
    "workgrades['Score_bins'] = pd.cut(workgrades['Score januari'],bins=bins,labels=labels,include_lowest=True)\n",
    "\n",
    "print((workgrades['Score_bins'] == 'fail (0-7)').min())\n",
    "print(workgrades.info())\n",
    "print(workgrades.head())\n",
    "workgrades['Score_bins'] = workgrades['Score_bins'].astype('category')\n",
    "workgrades_encoded = pd.get_dummies(workgrades, columns=['Score_bins'])\n",
    "print(workgrades_encoded)\n",
    "workgrades= pd.merge(workgrades, workgrades_encoded, on=['USER_PK1','Score januari','Score juni','OPO_ID'])\n",
    "workgrades['USER_PK1'] = workgrades['USER_PK1'].astype(str)\n",
    "print(workgrades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed7cf750"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e1c843c"
   },
   "source": [
    "## **Time Management and Regularity Features**\n",
    "\n",
    "Paper 1: \"Profiling students' self-regulation with learning analytics: a proof of concept\", by Liz-Dominguez et al. (2022)\n",
    "\n",
    "Paper 2: \"Predictive power of regularity of pre-class activities in a flipped classroom\" by Jovanovic et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0D0--MeRqVt"
   },
   "source": [
    "### Feature N°1: PERCENTAGE_WEEKS_ACTIVITY_ABOVE_MEDIAN\n",
    "\"The relative amount of weeks in which the activity of the user was higher than the median of all students’ activities that week. With activity each event in file 19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2_jWYnwRqVu"
   },
   "outputs": [],
   "source": [
    "time19 = test_sets['test_df19']['TIMESTAMP_activity_accumulator_logs']\n",
    "test_sets['test_df19']['week_of_year'] = time19.dt.isocalendar().week\n",
    "work19 = test_sets['test_df19'][['USER_PK1','week_of_year']]\n",
    "weeklygrouped = work19.groupby(['USER_PK1','week_of_year']).size().reset_index(name='user_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6dXweYpRqVu"
   },
   "source": [
    "Boxplots representing the user count for each \"week_of_year\" value representing the active students that week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa411600",
    "outputId": "a37f7fa3-f444-4e89-90c0-a1de48bf0c17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.boxplot(x='week_of_year',y='user_count',data=weeklygrouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9af7b648"
   },
   "source": [
    "A way to measure procrastiantion of students. Using median of medians of TRAIN set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63871b4f",
    "outputId": "e1b890e4-3510-43d7-859a-5663b0e318c6"
   },
   "outputs": [],
   "source": [
    "# Median of medians of all weeks\n",
    "if 'user_count_weekly_median' not in weeklygrouped.columns:\n",
    "    weeklygrouped = weeklygrouped.merge(median_per_week_train, on='week_of_year', suffixes=('','_weekly_median'))\n",
    "\n",
    "print(weeklygrouped.info())\n",
    "\n",
    "# Was the activity of user x in week y higher than the median activity of all users that week?\n",
    "weeklygrouped['above_median'] = weeklygrouped['user_count'].values > weeklygrouped['user_count_weekly_median'].values\n",
    "print(weeklygrouped.head())\n",
    "\n",
    "# How many weeks in total were user x's activity above the weekly median?\n",
    "above_median_count = weeklygrouped.groupby('USER_PK1')['above_median'].sum().reset_index()\n",
    "above_median_count.columns = ['USER_PK1','above_median_count']\n",
    "print(above_median_count.head())\n",
    "\n",
    "num_weeks = weeklygrouped['week_of_year'].nunique()\n",
    "print(num_weeks)\n",
    "\n",
    "above_median_count['percentageweeks_above_median'] = (above_median_count['above_median_count']/num_weeks)*100\n",
    "print(above_median_count.head(20))\n",
    "print(above_median_count.info())\n",
    "\n",
    "# Merging the data with the students' results\n",
    "test_sets['test_df_grades'][\"USER_PK1\"] = test_sets['test_df_grades'][\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "dfG['USER_PK1'] = dfG['USER_PK1'].astype(str)\n",
    "merge = pd.merge(above_median_count, dfG, on = 'USER_PK1', how = 'inner')\n",
    "work = merge[['USER_PK1','above_median_count','percentageweeks_above_median','Score_bins']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7cb3cb0"
   },
   "source": [
    "### Feature N°2: DAYS_UNTIL_FIRST_LOGIN\n",
    "\"Number of days it took each student to login on the course page for the first time. Taking the start of the semester as a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9177ad8e",
    "outputId": "1d726cb0-a665-4f54-c9c7-e86e65672622"
   },
   "outputs": [],
   "source": [
    "# Weekly activity (number of events) by students\n",
    "time19 = test_sets['test_df19']\n",
    "print(time19.head())\n",
    "grouped_data = time19.groupby('USER_PK1')\n",
    "\n",
    "start = pd.to_datetime('2020-09-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a53c0fbd",
    "outputId": "5562188d-94f7-4481-99f2-7a624ab8e6f2"
   },
   "outputs": [],
   "source": [
    "first_login = grouped_data['TIMESTAMP_activity_accumulator_logs'].min()\n",
    "days_until_first_login = (first_login - start).dt.days\n",
    "print(days_until_first_login)\n",
    "\n",
    "# Merging thee data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(days_until_first_login.reset_index(name='days_until_first_login'), on='USER_PK1', how='inner')\n",
    "workingdata_FT2 = merged[['USER_PK1','days_until_first_login']]\n",
    "print(workingdata_FT2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63e04b95"
   },
   "source": [
    "### Feature N°3: ENTROPY_WEEKLY_SESSION_COUNTS\n",
    "\"Entropy of weekly session counts per student. Once again, the sessions are here the events as in file 19, with no operations performed on these\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnfCA-oP6tSP"
   },
   "outputs": [],
   "source": [
    "weekly_session_counts_per_student = time19.groupby(['USER_PK1', 'week_of_year']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbb181e5",
    "outputId": "b7063094-fa0e-43ef-de24-67d55b1ea349"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Calculating probability distribution for each student\n",
    "prob_distribution_per_student = weekly_session_counts_per_student.apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "# Calculating entropy for each student in the first semester\n",
    "entropy_per_student = prob_distribution_per_student.apply(lambda x: entropy(x, base=2) if x.sum() > 0 else 0, axis=1)\n",
    "\n",
    "print(f\"Entropy of weekly session counts per student for the first semester:\")\n",
    "print(entropy_per_student)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(entropy_per_student.reset_index(name='entropy_per_student'), on='USER_PK1', how='inner')\n",
    "workingdata_FT5 = merged[['USER_PK1','entropy_per_student']]\n",
    "print(workingdata_FT5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bbd9d37"
   },
   "source": [
    "## Regularity features\n",
    "\n",
    "Paper 1: \"Differences by course discipline on student behaviorn persistence, and achievement in online courses of undergraduate general education\" by Finnegan et al.\n",
    "\n",
    "Paper 2: \"Predictive power of regularity of pre-class activities in a flipped classroom\" by Jovanovic et al.\n",
    "\n",
    "Paper 3: \"How learning analytics can early predict under-achieving students in a blended medical education course\" by Saqr et al.\n",
    "\n",
    "Paper 4: \"Students matter the most in learning analytics: The effects of internal and instructional conditions in predicting academic success\" by Jovanovic et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzPRnX8JRqVw"
   },
   "source": [
    "### Feature N°1: FREQ_CHANGE_ENGAGEMENT_PATTERN\n",
    "\"Frequency of change in a student’s engagement pattern over the days of the week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1df1fd8e",
    "outputId": "4d48d80f-8dcb-45f7-d315-eeda90ef0d32"
   },
   "outputs": [],
   "source": [
    "df19 = test_sets['test_df19']\n",
    "print(df19.info())\n",
    "\n",
    "session_duration = df19.groupby(\"SESSION_ID\")[\"TIMESTAMP_activity_accumulator_logs\"].agg(['min','max'])\n",
    "session_duration['duration'] = session_duration['max'] - session_duration['min']\n",
    "\n",
    "# Convert column to hours\n",
    "session_duration['duration_hours'] = session_duration['duration'] / pd.Timedelta(hours=1)\n",
    "\n",
    "# Limit durations longer than 8 hours to 8 hours\n",
    "session_duration['duration_hours'] = session_duration['duration_hours'].clip(upper=8)\n",
    "\n",
    "# Convert duration back to normal format\n",
    "session_duration['duration'] = pd.to_timedelta(session_duration['duration_hours'], unit='h')\n",
    "print(session_duration.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e905730",
    "outputId": "f76b5d4d-a654-4cd8-dbd4-968f50cc85cc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creation of DataFrame with daily activity relative to their weekly total for each student\n",
    "# Extract week number\n",
    "df19['week_number'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.isocalendar().week\n",
    "# Extract day of the week\n",
    "df19['day_of_week'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.dayofweek + 1\n",
    "\n",
    "# Specifying the Semester\n",
    "work19_FR1 = df19[(df19['week_number']>=39) | (df19['week_number']<=5)]\n",
    "all_weeks = list(range(39, 53)) + list(range(1,6)) # Assuming weeks are numbered from 1 to 52\n",
    "grouped_data_FR1 = work19_FR1.groupby(['USER_PK1', 'week_number', 'day_of_week'])\n",
    "\n",
    "# Calculating total sessions per day per week per student\n",
    "total_sessions_per_day_per_week_per_student = grouped_data_FR1['SESSION_ID'].nunique().unstack(fill_value=0)\n",
    "print(total_sessions_per_day_per_week_per_student.head(10))\n",
    "\n",
    "# Calculating total sessions per week per student\n",
    "total_sessions_per_week_per_student = total_sessions_per_day_per_week_per_student.sum(axis=1)\n",
    "\n",
    "# Reindexing to ensure all combinations of students and weeks are included, filling missing values with zeros\n",
    "all_students = work19_FR1['USER_PK1'].unique()\n",
    "new_index = pd.MultiIndex.from_product([all_students, all_weeks], names=['USER_PK1', 'week_number'])\n",
    "total_sessions_per_day_per_week_per_student = total_sessions_per_day_per_week_per_student.reindex(new_index, fill_value=0)\n",
    "\n",
    "# Replacing NaN values in weeks with no activity with 0\n",
    "total_sessions_per_day_per_week_per_student.fillna(0, inplace=True)\n",
    "\n",
    "# Recalculating total sessions per week per student after reindexing\n",
    "total_sessions_per_week_per_student = total_sessions_per_day_per_week_per_student.sum(axis=1)\n",
    "print(total_sessions_per_week_per_student.head())\n",
    "\n",
    "# Calculating relative weights per week per student\n",
    "relative_weights_per_week_per_student = total_sessions_per_day_per_week_per_student.div(total_sessions_per_week_per_student, axis=0)\n",
    "\n",
    "# Replacing NaN values in weeks with no activity with 0\n",
    "relative_weights_per_week_per_student.fillna(0,inplace=True)\n",
    "\n",
    "print(relative_weights_per_week_per_student.head())\n",
    "\n",
    "# Calculating the mean squared difference between the vectors of two consecutive weeks\n",
    "# Calculating the difference between consecutive weeks\n",
    "diff_between_weeks = relative_weights_per_week_per_student.diff(axis=0)\n",
    "\n",
    "# Squaring each difference\n",
    "squared_diff = diff_between_weeks ** 2\n",
    "\n",
    "# Calculating the mean squared difference for each student\n",
    "mean_squared_diff_per_student = squared_diff.mean(axis=1)\n",
    "\n",
    "# Calculating the aggregate number over all weeks for each student\n",
    "aggregate_mean_squared_diff_per_student = mean_squared_diff_per_student.groupby('USER_PK1').sum()\n",
    "\n",
    "print(\"Mean squared difference per student between consecutive weeks:\")\n",
    "print(mean_squared_diff_per_student.head())\n",
    "\n",
    "print(\"\\nAggregate mean squared difference over all weeks for each student:\")\n",
    "print(aggregate_mean_squared_diff_per_student)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged_FR1 = dfG.merge(aggregate_mean_squared_diff_per_student.reset_index(name='aggregate_mean_squared_diff_per_student'), on='USER_PK1', how='inner')\n",
    "print(merged_FR1.head())\n",
    "workingdata_FR1 = merged_FR1[['USER_PK1', 'aggregate_mean_squared_diff_per_student']]\n",
    "print(workingdata_FR1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85c17f7b"
   },
   "source": [
    "### Feature N°2: WEEKLY_LOGIN_ENGAGEMENT\n",
    "\"Measuring engagement by looking at login behavior. A student was considered engaged in a certain week when having logged in 3 days or more in that week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1d48ec66",
    "outputId": "a4201c5e-535f-4bff-b42e-daaa127d217f"
   },
   "outputs": [],
   "source": [
    "df19 = test_sets['test_df19']\n",
    "\n",
    "# 1. Creating a new column to register the active days per week\n",
    "df19['week_number'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.isocalendar().week\n",
    "df19['day_of_week'] = df19['TIMESTAMP_activity_accumulator_logs'].dt.dayofweek + 1\n",
    "\n",
    "# 2. Grouping the data based on the student and the week number\n",
    "grouped_data_FR2 = df19.groupby(['USER_PK1', 'week_number'])\n",
    "\n",
    "# 3. Calculating the total active days per week for each student\n",
    "active_days_per_week = grouped_data_FR2['day_of_week'].nunique()\n",
    "print(active_days_per_week)\n",
    "\n",
    "# 4. Assigning a score of 1 or 0 based on whether the total active days per week is >= 3\n",
    "weekly_scores = (active_days_per_week >= 3).astype(int)\n",
    "print(weekly_scores)\n",
    "\n",
    "# 5. Summing the weekly scores to calculate the total score for each student\n",
    "total_scores_per_student_FR2 = weekly_scores.groupby('USER_PK1').sum().reset_index(name='total_score_FR2')\n",
    "\n",
    "print(total_scores_per_student_FR2)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged_FR2 = dfG.merge(total_scores_per_student_FR2.reset_index(), on='USER_PK1', how='inner')\n",
    "print(merged_FR2.head())\n",
    "workingdata_FR2 = merged_FR2[['USER_PK1', 'total_score_FR2']]\n",
    "print(workingdata_FR2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d190cb9"
   },
   "source": [
    "### Feature N°3: FORUM_ENGAGEMENT\n",
    "\"Measuring engagement by looking at the forum posts views. A score of one was assigned when a student views the course materials more than a Z-score of mean course views (using -1.96 and +1.96 for a two-sided 95% confidence interval)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b839e68",
    "outputId": "19b2c051-d6b3-428f-cd42-90c7ff803b35"
   },
   "outputs": [],
   "source": [
    "df18 = test_sets['test_df18']\n",
    "\n",
    "num_content_items = df18['MESSAGE_PK1'].nunique()\n",
    "print(num_content_items)\n",
    "\n",
    "message_avg_views = df18.groupby('MESSAGE_PK1')['NUM_VIEWS'].mean()\n",
    "df18 = df18.merge(message_avg_views, on='MESSAGE_PK1', suffixes=('','_avg'))\n",
    "df18['Z_score'] = (df18['NUM_VIEWS'] - df18['NUM_VIEWS_avg']) / df18['NUM_VIEWS'].std()\n",
    "\n",
    "Z_score_threshold = 1.96\n",
    "df18['exceptional_view'] = np.where(df18['Z_score'] > Z_score_threshold, 1, 0)\n",
    "\n",
    "student_scores_FR3 = df18.groupby('USER_PK1')['exceptional_view'].sum().reset_index()\n",
    "student_scores_FR3['exceptional_view_percentage'] = round((student_scores_FR3['exceptional_view'] / num_content_items) * 100, 2)\n",
    "print(student_scores_FR3.head(30))\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged_FR3 = dfG.merge(student_scores_FR3.reset_index(), on='USER_PK1', how='inner')\n",
    "print(merged_FR3.head())\n",
    "workingdata_FR3 = merged_FR3[['USER_PK1', 'exceptional_view']]\n",
    "print(workingdata_FR3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23c6673b"
   },
   "source": [
    "### Feature N°4: ASSESSMENT_ENGAGEMENT\n",
    "\"Measuring engagement by looking at assessment attempts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e389f795",
    "outputId": "873120b7-66e6-42d3-f230-b85212a7f22b"
   },
   "outputs": [],
   "source": [
    "# Creation of column where each row represents an attempt of a specific USER_PK1 & CONTENT_PK1 combination\n",
    "df15_filtered_FR4['attempts'] = 1\n",
    "attempts_counts_FR4 = df15_filtered_FR4.groupby(['USER_PK1','CONTENT_PK1'])['attempts'].size().reset_index(name=('attempts'))\n",
    "print(attempts_counts_FR4[attempts_counts_FR4['attempts']>1])\n",
    "print(attempts_counts_FR4.head())\n",
    "total_attempts_per_student_FR4 = attempts_counts_FR4.groupby('USER_PK1')['attempts'].sum().reset_index(name='total_score')\n",
    "print(total_attempts_per_student_FR4)\n",
    "\n",
    "# Look at each unique value of CONTENT_PK1 and check whether this appears in the students' attempts\n",
    "content_items = df15['CONTENT_PK1'].unique()\n",
    "print(len(content_items)) # Number of content items in total\n",
    "\n",
    "user_scores_FR4 = {}\n",
    "for content in content_items:\n",
    "    content_data = df15[df15['CONTENT_PK1'] == content]\n",
    "    content_data = content_data.merge(attempts_counts_FR4, on=['USER_PK1', 'CONTENT_PK1'], how='left')\n",
    "    processed_users = set() # Keep track of processed users such that a user-content item combination is not counted twice\n",
    "    for index, row in content_data.iterrows():\n",
    "        user = row['USER_PK1']\n",
    "        if user in processed_users:\n",
    "            continue # Skipping the following loop if user has already been processed for this content item\n",
    "        # Does the user already exist in the dictionary?!\n",
    "        if user in user_scores_FR4:\n",
    "            user_scores_FR4[user] += 1 if row['attempts'] > 0 else 0\n",
    "        else:\n",
    "            user_scores_FR4[user] = 1 if row['attempts'] > 0 else 0\n",
    "        processed_users.add(user) # Adding the user to the set of processed users\n",
    "total_scores_FR4 = pd.DataFrame(list(user_scores_FR4.items()), columns=['USER_PK1','total_score'])\n",
    "print(total_scores_FR4)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged_FR4 = dfG.merge(total_scores_FR4.reset_index(), on='USER_PK1', how='inner')\n",
    "print(merged_FR4.head())\n",
    "workingdata_FR4 = merged_FR4[['USER_PK1', 'total_score']]\n",
    "print(workingdata_FR4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nj1dGOcFRqVx"
   },
   "source": [
    "## **INTERACTION FEATURES**\n",
    "\n",
    "Paper: \"Learning at distance: Effects of interaction traces on academic achievement\" by Joksimovic et al. (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e4bd262"
   },
   "outputs": [],
   "source": [
    "df17 = test_sets['test_df17']\n",
    "grouped_data = df17.groupby('USER_PK1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b177294a"
   },
   "source": [
    "### Feature N°1: TOTAL_NUM_POSTS_READ\n",
    "\"Number of forum posts read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dc388c7",
    "outputId": "b138862e-e29e-4c48-e81a-a466d69e6882"
   },
   "outputs": [],
   "source": [
    "sum_read_posts = grouped_data['NUM_READ_POSTS'].sum()\n",
    "print(sum_read_posts)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_read_posts.reset_index(name='sum_read_posts'), on='USER_PK1', how='inner')\n",
    "workingdata_FI1 = merged[['USER_PK1','sum_read_posts']]\n",
    "print(workingdata_FI1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9867bd8"
   },
   "source": [
    "### Feature N°2: TOTAL_NUM_COMMENTS\n",
    " \"Number of comments on forum posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4a52285",
    "outputId": "f49b63a0-2186-4b59-cc6a-115612b78826"
   },
   "outputs": [],
   "source": [
    "sum_comments = grouped_data['NUM_REPLIES_COMMENTS'].sum()\n",
    "print(sum_comments)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_comments.reset_index(name='sum_comments'), on='USER_PK1', how='inner')\n",
    "workingdata_FI2 = merged[['USER_PK1','sum_comments']]\n",
    "print(workingdata_FI2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aed50298"
   },
   "outputs": [],
   "source": [
    "df18 = test_sets['test_df18']\n",
    "grouped_data = df18.groupby('USER_PK1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_WVfob3RqVy"
   },
   "source": [
    "### Feature N°3: TOTAL_NUM_VIEWS\n",
    "\"Total number of views on posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba2497b8",
    "outputId": "252f36e0-9096-4424-fe0a-91296af6f22b"
   },
   "outputs": [],
   "source": [
    "sum_views = grouped_data['NUM_VIEWS'].sum()\n",
    "print(sum_views)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_views.reset_index(name='sum_views'), on='USER_PK1', how='inner')\n",
    "workingdata_FI3 = merged[['USER_PK1','sum_views']]\n",
    "print(workingdata_FI3.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef13ed7d"
   },
   "source": [
    "### Feature N°4: TOTAL_POSTS_INDICATED_READ\n",
    "\"Number of posts indicated as read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b67b5b37",
    "outputId": "a5e0d055-af13-401e-c8ef-dd2b4932927e"
   },
   "outputs": [],
   "source": [
    "sum_ind_read = grouped_data['READ_STATE_IND'].sum()\n",
    "print(grouped_data.head())\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(sum_ind_read.reset_index(name='sum_ind_read'), on='USER_PK1', how='inner')\n",
    "workingdata_FI4 = merged[['USER_PK1','sum_ind_read']]\n",
    "print(workingdata_FI4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de30fc68"
   },
   "source": [
    "### Feature N°5: MEAN_TEXT_LENGTH_OF_POSTS\n",
    "\"Mean of students’ post length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df52052c"
   },
   "outputs": [],
   "source": [
    "df16 = test_sets['test_df16']\n",
    "grouped_data = df16.groupby('USER_PK1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3f8b04a",
    "outputId": "075160c8-f19a-4c1f-df10-59271a78f41c"
   },
   "outputs": [],
   "source": [
    "mean_text_length = grouped_data['MSG_TEXT_LENGTH'].mean()\n",
    "print(mean_text_length)\n",
    "\n",
    "# Merging the data with the students' results\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "dfG = workgrades\n",
    "merged = dfG.merge(mean_text_length.reset_index(name='mean_text_length'), on='USER_PK1', how='inner')\n",
    "workingdata_FI5 = merged[['USER_PK1','mean_text_length']]\n",
    "print(workingdata_FI5.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0701fd5"
   },
   "source": [
    "## Second set of features\n",
    "\n",
    "Paper: \"Discovering Unusual Study Patterns Using Anomaly Detection and XAI\" by Tiukhova et al. (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5a5a5a1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import entropy\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1b8c0b17"
   },
   "outputs": [],
   "source": [
    "data_19 = test_sets['test_df19'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddf550da"
   },
   "outputs": [],
   "source": [
    "data_19 = data_19.assign(TIMESTAMP_NEW=pd.to_datetime(data_19['TIMESTAMP'],dayfirst=True))\n",
    "data_19['WEEK'] = data_19.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[1], axis = 1)\n",
    "data_19['YEAR'] =  data_19.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[0], axis = 1)\n",
    "data_19['WEEK'] = data_19['WEEK'].astype(str)\n",
    "data_19['YEAR'] = data_19['YEAR'].astype(str)\n",
    "data_19['WEEK and YEAR'] = 'Week ' + data_19['WEEK'] + ' of ' + data_19['YEAR']\n",
    "data_19 = data_19.drop(columns = ['Unnamed: 0'], errors = 'ignore')\n",
    "\n",
    "data_19['CONTENT_PK1'] = data_19['CONTENT_PK1'].fillna(0)\n",
    "data_19['CONTENT_PK1'] = data_19['CONTENT_PK1'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f8b881b"
   },
   "outputs": [],
   "source": [
    "test_sets['test_df7']['CONTENT_PK1'] = test_sets['test_df7']['CONTENT_PK1'].astype(str)\n",
    "test_sets['test_df7']['COURSE_PK1'] = test_sets['test_df7']['COURSE_PK1'].astype(str)\n",
    "data_19['CONTENT_PK1'] = data_19['CONTENT_PK1'].astype(str)\n",
    "data_19['COURSE_PK1'] = data_19['COURSE_PK1'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80a484f2"
   },
   "outputs": [],
   "source": [
    "# we merge activity data with the course content information to get more details on the activity\n",
    "data_19 = data_19.merge(test_sets['test_df7'][['COURSE_PK1', 'CONTENT_PK1',\n",
    "         'CONTENT_TYPE', 'TITLE', 'PATH']], on = ['COURSE_PK1', 'CONTENT_PK1'], how = 'left')\n",
    "data_19['CONTENT_TYPE'] = data_19.apply(lambda x: x['DATA'] if pd.isna(x['CONTENT_TYPE']) else x['CONTENT_TYPE'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d21537af"
   },
   "outputs": [],
   "source": [
    "def new_session_id_func(session_ids, df):\n",
    "    sessions= []\n",
    "    for session_id in tqdm(session_ids):\n",
    "        session = df[df['SESSION_ID'] == session_id]\n",
    "        session = session.reset_index()\n",
    "        indices = session[session['TIME_DIFF_SESSION']>= 7200].index\n",
    "        n = len(indices)-1\n",
    "        for i in sorted(indices,reverse=True):\n",
    "            session.loc[:i-1, 'new_session_id'] = f'session_{session_id}_{n}'\n",
    "            n = n - 1\n",
    "\n",
    "        session['new_session_id'] = session['new_session_id'].fillna(f'session_{session_id}_{len(indices)}')\n",
    "        sessions.append(session)\n",
    "    return pd.concat(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70f46e1b"
   },
   "outputs": [],
   "source": [
    "def change_sessions(data):\n",
    "    data = data.sort_values(by = ['SESSION_ID', 'TIMESTAMP_NEW']) # first sort data within a session\n",
    "    data['TIME_DIFF_SESSION'] = data.groupby(by = ['SESSION_ID'])['TIMESTAMP_NEW'].diff().dt.seconds.fillna(0) #calculate the time difference between learning activities within a session\n",
    "    sessions_to_divide = set(data[data['TIME_DIFF_SESSION'] > 7200]['SESSION_ID']) #get the sessions where the difference between learning activities is larger than 2h\n",
    "    imputed_sessions = new_session_id_func(sessions_to_divide, data[data['SESSION_ID'].isin(sessions_to_divide)]) #apply a function that will create a new session id - subsession\n",
    "    imputed_sessions = imputed_sessions.drop(columns = 'index')\n",
    "    imputed_sessions = imputed_sessions.sort_values(by = ['SESSION_ID', 'TIMESTAMP_NEW'])\n",
    "    imputed_sessions = imputed_sessions.reset_index()\n",
    "    imputed_sessions = imputed_sessions.drop(columns = 'index')\n",
    "\n",
    "    left_session = data[~data['SESSION_ID'].isin(sessions_to_divide)] #get the rest of sessions that does not need to be separated\n",
    "    left_session['new_session_id'] = left_session['SESSION_ID'].copy()\n",
    "    new_data = pd.concat([imputed_sessions, left_session])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62e7ce55",
    "outputId": "13571a53-5b5d-4ce6-f21d-b05ffab7d656"
   },
   "outputs": [],
   "source": [
    "data_19_new = pd.DataFrame()\n",
    "for course in set(data_19['COURSE_PK1']):\n",
    "    course_data = data_19[data_19['COURSE_PK1'] == course]\n",
    "    course_data_new = change_sessions(course_data)\n",
    "    data_19_new = pd.concat([data_19_new, course_data_new])\n",
    "\n",
    "    print(course_data)\n",
    "\n",
    "# each row in the table_19 represents an event (a unique combination of timestamp and content PK)\n",
    "data_19_new['EVENTS'] = 1\n",
    "\n",
    "print(data_19_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af02f4e0"
   },
   "outputs": [],
   "source": [
    "# we take only those posts that we have interactions for.\n",
    "data_16 = test_sets['test_df16'][test_sets['test_df16']['CONTEXT_PK1'].isin(list(set(test_sets['test_df17']['CONTEXT_PK1'])))]\n",
    "data_16 = data_16.assign(DTCREATED_NEW=pd.to_datetime(data_16['DTCREATED'],dayfirst=True))\n",
    "data_16['WEEK_CREATED'] = data_16.apply(lambda x: x['DTCREATED_NEW'].isocalendar()[1], axis = 1)\n",
    "data_16['YEAR'] = data_16.apply(lambda x: x['DTCREATED_NEW'].isocalendar()[0], axis = 1)\n",
    "data_16['WEEK_CREATED'] = data_16['WEEK_CREATED'].astype(str)\n",
    "data_16['YEAR'] = data_16['YEAR'].astype(str)\n",
    "data_16['WEEK and YEAR created'] = 'Week ' + data_16['WEEK_CREATED'] + ' of ' + data_16['YEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2ba4ca4"
   },
   "outputs": [],
   "source": [
    "data_17 = test_sets['test_df17'].copy()\n",
    "\n",
    "workgrades[\"USER_PK1\"] = workgrades[\"USER_PK1\"].astype(str)\n",
    "grades = workgrades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24457cbb"
   },
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48f6a863",
    "outputId": "52da2200-82c3-4ff6-80e1-97cea8c9d61e"
   },
   "outputs": [],
   "source": [
    "course_start, course_finish, exam_weeks, semester_weeks  = pd.to_datetime('2020-09-21'),end_sem1,('Week 1 of 2021','Week 2 of 2021','Week 3 of 2021','Week 4 of 2021', 'Week 5 of 2021'), ('Week 39 of 2020','Week 40 of 2020','Week 41 of 2020','Week 42 of 2020','Week 43 of 2020','Week 44 of 2020','Week 45 of 2020','Week 46 of 2020','Week 47 of 2020','Week 48 of 2020','Week 49 of 2020','Week 50 of 2020','Week 51 of 2020','Week 52 of 2020')\n",
    "\n",
    "print(course_start)\n",
    "print(exam_weeks)\n",
    "\n",
    "course_duration = (course_finish - course_start).days\n",
    "course_duration_weeks = course_duration/7\n",
    "\n",
    "print(course_duration_weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2b887f7",
    "outputId": "d5f29d6a-6e35-4974-cdaf-a47ae85e7e01"
   },
   "outputs": [],
   "source": [
    "course_data = data_19_new\n",
    "# filter out the data outside the course timespan\n",
    "course_data = course_data[(course_data['TIMESTAMP_NEW'] > course_start) &(course_data['TIMESTAMP_NEW'] < course_finish)]\n",
    "course_data['DATE'] = course_data['TIMESTAMP_NEW'].dt.date\n",
    "course_data['DATE'] = pd.to_datetime(course_data['DATE'])\n",
    "course_data['PATH'] = course_data['PATH'].fillna('not specified')\n",
    "\n",
    "print(course_data)\n",
    "\n",
    "course_data_posts = data_16\n",
    "# we only want to take into account the posts of the current academic year\n",
    "course_data_posts = course_data_posts[(course_data_posts['DTCREATED_NEW']>=course_start) & (course_data_posts['DTCREATED_NEW'] <= course_finish)]\n",
    "course_data_posts['DATE_CREATED'] = course_data_posts['DTCREATED_NEW'].dt.date\n",
    "\n",
    "print(course_data_posts)\n",
    "\n",
    "course_data_posts_consume = data_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee58ea2b"
   },
   "source": [
    "## Overall Level of Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°1: ZERO_SESSION_COUNT\n",
    "\"Number of sessions with duration of zero\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cad96c9c",
    "outputId": "43cba10f-a32f-452b-b183-aea23e0bc5fa"
   },
   "outputs": [],
   "source": [
    "# then we need to calculate each session's duration. To do that, we find the max and min timestamp value per session and substract latter from the former\n",
    "# we do it to filter out very long/short sessions\n",
    "session_duration = pd.pivot_table(course_data, values='TIMESTAMP_NEW', index=['COURSE_PK1','USER_PK1', 'new_session_id'],aggfunc='max').fillna(0) - pd.pivot_table(course_data, values='TIMESTAMP_NEW', index=['COURSE_PK1','USER_PK1', 'new_session_id'],aggfunc='min').fillna(0)\n",
    "session_duration = session_duration.reset_index()\n",
    "\n",
    "session_duration['SECONDS'] = session_duration['TIMESTAMP_NEW'] / np.timedelta64(1, 's')\n",
    "session_duration['MINUTES'] = session_duration['TIMESTAMP_NEW'] / np.timedelta64(1, 'm')\n",
    "session_duration['HOURS'] = session_duration['TIMESTAMP_NEW'] / np.timedelta64(1, 'h')\n",
    "\n",
    "print(session_duration.info())\n",
    "\n",
    "# we calculate the number of sessions with no duration - could be quick access to the announcements\n",
    "session_count_zero_duration = pd.pivot_table(session_duration[session_duration['SECONDS'] == 0], values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'new_session_id':'SESSION_COUNT'})\n",
    "\n",
    "print(session_count_zero_duration.head())\n",
    "\n",
    "plt.hist(session_count_zero_duration['SESSION_COUNT'], bins= 20)\n",
    "plt.title('TOTAL ZERO LENGTH SESSION COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°2: NON_ZERO_SESSION_COUNT\n",
    "\"Number of sessions, excluding the ones with duration of zero\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "798d6b6f"
   },
   "outputs": [],
   "source": [
    "# We exclude sessions of length 0\n",
    "course_data_non_zero = course_data.drop(course_data[course_data['new_session_id'].isin(set(session_duration[session_duration['SECONDS'] == 0]['new_session_id']))].index).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f24764a"
   },
   "source": [
    "### Reducing length of the 8 hours+ sessions to 8 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4e95022",
    "outputId": "ed20d72f-b758-458b-f932-0e2398dab751"
   },
   "outputs": [],
   "source": [
    "# We limit sessions of a duration of more than 8 hours to 8 hours\n",
    "session_duration['HOURS'] = np.where(session_duration['HOURS'] > 8, 8, session_duration['HOURS'])\n",
    "\n",
    "# We exclude sessions with duration of 0 \n",
    "session_duration = session_duration.drop(session_duration[session_duration['SECONDS']  == 0].index) # including sessions with 0 duration can spoil the average\n",
    "print(session_duration.count())\n",
    "\n",
    "session_duration_plt = plt.hist(session_duration['MINUTES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81d6a683",
    "outputId": "ded1f88b-a776-4a5b-95a9-7cf4ed2a476c"
   },
   "outputs": [],
   "source": [
    "# getting session count - non-zero sessions\n",
    "session_count = pd.pivot_table(course_data_non_zero, values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'new_session_id':'SESSION_COUNT'})\n",
    "\n",
    "print(session_count.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9054486",
    "outputId": "2fc36a3a-6f1c-45a4-dcfa-d030d0c39730"
   },
   "outputs": [],
   "source": [
    "median_number_sessions = int(session_count['SESSION_COUNT'].median())\n",
    "\n",
    "print(median_number_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2307bf80",
    "outputId": "8043fd17-49bb-4340-a643-58d976018a0d"
   },
   "outputs": [],
   "source": [
    "plt.hist(session_count['SESSION_COUNT'], bins= 20)\n",
    "plt.title('TOTAL SESSION COUNT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature N°3: AVERAGE_ACTIONS_PER_SESSION\n",
    "\"Average number of  clicks / learning actions each session\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6728807",
    "outputId": "0e8829a0-9bb9-41c8-f34c-e9e06141fa48"
   },
   "outputs": [],
   "source": [
    "# getting the number of events per timestamp for future filtering\n",
    "clicks_per_timestamp = pd.pivot_table(course_data_non_zero, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'new_session_id','TIMESTAMP_NEW'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70fac3a4",
    "outputId": "bc067fd0-f118-4071-a3ca-dd0512fc4c63"
   },
   "outputs": [],
   "source": [
    "# replace the rows with #events > 1 by 1\n",
    "# as Toledo logs all the subfolders of an opened parent folder as a separate event, we replace this multiple events by just 1 event of openening a folder\n",
    "mask = clicks_per_timestamp['EVENTS'] > 1\n",
    "column_name = 'EVENTS'\n",
    "clicks_per_timestamp.loc[mask, column_name] = 1\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efe85dd1",
    "outputId": "db86dcc4-b953-4ebe-e6a6-e15c6650a888"
   },
   "outputs": [],
   "source": [
    "# getting the median number of events (learning actions) per sessions of a user\n",
    "clicks_per_session = pd.pivot_table(clicks_per_timestamp, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'new_session_id'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "clicks_per_user = pd.pivot_table(clicks_per_session, values='EVENTS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').fillna(0).reset_index().rename(columns = {'EVENTS':'CLICKS_PER_SESSION_AVG'})\n",
    "\n",
    "print(clicks_per_session.head())\n",
    "print(clicks_per_user.head()) \n",
    "\n",
    "plt.hist(clicks_per_user['CLICKS_PER_SESSION_AVG'], bins= 20)\n",
    "plt.title('MEDIAN #LEARNING ACTIONS PER SESSION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70ce11e6"
   },
   "source": [
    "### Feature N°4: TOTAL_SESSION_DURATION\n",
    "\"The total duration of all sessions of each student (in seconds)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16eaeb8c",
    "outputId": "6dfa702d-9007-41a7-fcaf-17e328e821e7"
   },
   "outputs": [],
   "source": [
    "# getting total session length per user (in seconds)\n",
    "session_duration_user = pd.pivot_table(session_duration, values='SECONDS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SECONDS':'SESSION_DURATION'})\n",
    "\n",
    "plt.hist(session_duration_user['SESSION_DURATION'], bins= 20)\n",
    "plt.title('TOTAL SESSION LENGTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°5: AVERAGE_SESSION_DURATION\n",
    "\"The median session length of each user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1c64a308",
    "outputId": "7429485a-1b5d-4424-e807-7f00b7afae1b"
   },
   "outputs": [],
   "source": [
    "# getting average (median) session length per user\n",
    "session_duration_user_avg = pd.pivot_table(session_duration, values='SECONDS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').fillna(0).reset_index().rename(columns = {'SECONDS':'SESSION_DURATION_AVG'})\n",
    "\n",
    "plt.hist(session_duration_user_avg['SESSION_DURATION_AVG'], bins= 20)\n",
    "plt.title('MEDIAN SESSION LENGTH - SECONDS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°6: PROPORTION_ACTIVE_WEEKS\n",
    "\"The proportion of weeks a user has had an active week - with this being defined as follows; a week is active when the number of active days is higher than the average\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59fff332",
    "outputId": "0e989111-d690-45c6-8ca9-372e624749e1"
   },
   "outputs": [],
   "source": [
    "active_days = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE':'ACTIVE_DAYS'})\n",
    "\n",
    "# getting the number of active days per week per user\n",
    "active_days_per_week = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE':'ACTIVE_DAYS'})\n",
    "\n",
    "print(active_days_per_week.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75ce8820",
    "outputId": "1b21e283-d7a2-4e1a-bce8-133dc67eb097"
   },
   "outputs": [],
   "source": [
    "# getting the average number of active days per week for this course\n",
    "active_days_avg_course = pd.pivot_table(active_days_per_week, values='ACTIVE_DAYS', index=['COURSE_PK1'],\n",
    "                     aggfunc='mean')\n",
    "active_days_avg_course = active_days_avg_course.to_dict()\n",
    "\n",
    "print(active_days_avg_course)\n",
    "\n",
    "# leaving only those weeks with #active days higher than average #active days in a week per course\n",
    "active_days_per_week = active_days_per_week[active_days_per_week.apply(lambda x: x['ACTIVE_DAYS'] >= active_days_avg_course_train['ACTIVE_DAYS'][x['COURSE_PK1']], axis = 1)]\n",
    "\n",
    "print(active_days_per_week.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8344f762",
    "outputId": "7ce5fe0e-18a7-4e0d-a12d-e70252d32bec"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active weeks\n",
    "num_active_weeks = pd.pivot_table(active_days_per_week, values='WEEK and YEAR', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'WEEK and YEAR':'ACTIVE_WEEKS'})\n",
    "num_active_weeks['PROPORTION_ACTIVE_WEEKS'] = num_active_weeks['ACTIVE_WEEKS']/course_duration_weeks\n",
    "\n",
    "print(num_active_weeks.head())\n",
    "\n",
    "plt.hist(num_active_weeks['PROPORTION_ACTIVE_WEEKS'], bins= 10)\n",
    "plt.title('PROPORTION OF ACTIVE WEEKS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Feature N°7: ACTIVE_DAYS_PROPORTION\n",
    "\"By considering the course duration, a calculation is made on the amount of active days of each user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b15b0a6",
    "outputId": "b3854b2d-8111-43fa-f504-872e4d3473e1"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active days\n",
    "active_days['ACTIVE_DAYS_PROPORTION'] = active_days['ACTIVE_DAYS']/course_duration\n",
    "\n",
    "print(active_days.head())\n",
    "\n",
    "plt.hist(active_days['ACTIVE_DAYS_PROPORTION'], bins= 20)\n",
    "plt.title('PROPORTION OF ACTIVE DAYS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°8: AVERAGE_ACTIVE_DAYS_PER_WEEK\n",
    "\"The median number of active days per week for each user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9222db2e",
    "outputId": "91db3fec-a593-432a-e549-768bbe7e3a94"
   },
   "outputs": [],
   "source": [
    "# getting a median number of active days per week\n",
    "active_days_per_week = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index()\n",
    "active_days_per_week_per_user = pd.pivot_table(active_days_per_week, values='DATE', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').reset_index().rename(columns = {'DATE':'AVERAGE_ACTIVE_DAYS_PER_WEEK'})\n",
    "\n",
    "print(active_days_per_week.head())\n",
    "print(active_days_per_week_per_user.head())\n",
    "\n",
    "plt.hist(active_days_per_week_per_user['AVERAGE_ACTIVE_DAYS_PER_WEEK'], bins= 10)\n",
    "plt.title('MEDIAN OF #ACTIVE DAYS PER WEEK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°9: AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS\n",
    "\"For each user, the median time distance between two consecutive active days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be32c257",
    "outputId": "854a8dc3-a11d-4cb7-f8d8-e30b6c0db6d8"
   },
   "outputs": [],
   "source": [
    "course_data = course_data.sort_values(by = ['COURSE_PK1','USER_PK1', 'TIMESTAMP_NEW'])\n",
    "\n",
    "max_day_session = pd.pivot_table(course_data, values='DATE', index=['COURSE_PK1','USER_PK1', 'new_session_id'],\n",
    "                     aggfunc='max').reset_index().sort_values(by = ['COURSE_PK1','USER_PK1','DATE'])\n",
    "\n",
    "max_day_session['DIFF'] = max_day_session['DATE'].diff().dt.days\n",
    "max_day_session['DIFF'] = max_day_session['DIFF'].clip(lower=0)\n",
    "max_day_session['DIFF'] = max_day_session['DIFF'].fillna(0)\n",
    "\n",
    "print(max_day_session.head())\n",
    "\n",
    "# We are considering only the date and drop the dubplicates so that we do not affect the difference between days when there are more than 1 active session\n",
    "\n",
    "max_day_session = max_day_session.drop_duplicates(subset = ['COURSE_PK1','USER_PK1','DATE'], keep = 'first')\n",
    "\n",
    "# getting median time distance between two consecutive active days\n",
    "avg_time_distance_user = pd.pivot_table(max_day_session, values='DIFF', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').reset_index().rename(columns = {'DIFF':'AVG_DIFF'})\n",
    "\n",
    "print(avg_time_distance_user.head())\n",
    "\n",
    "plt.hist(avg_time_distance_user['AVG_DIFF'], bins= 20)\n",
    "plt.title('MEDIAN TIME DISTANCE BETWEEN CONSECUTIVE ACTIVE DAYS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b0d77d0"
   },
   "source": [
    "## Learning Action Specific Level of Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bac47195"
   },
   "source": [
    "### Feature N°1: ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION\n",
    "\"Proportion of active days for forum contribution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6795147c",
    "outputId": "36eb2d06-f4a6-481b-f574-ff14b0766203"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active days for the posts written\n",
    "active_days_posts_written = pd.pivot_table(course_data_posts, values='DATE_CREATED', index=['COURSE_PK1','USER_PK1'],aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE_CREATED':'ACTIVE_DAYS'})\n",
    "active_days_posts_written['ACTIVE_DAYS_PROPORTION'] = active_days_posts_written['ACTIVE_DAYS']/course_duration\n",
    "\n",
    "plt.hist(active_days_posts_written['ACTIVE_DAYS_PROPORTION'], bins= 10)\n",
    "plt.title('PROPORTION OF ACTIVE DAYS FOR FORUM CONTRIBUTION')\n",
    "\n",
    "# getting the number of active days with posts written per week per user\n",
    "active_days_per_week_posts_written = pd.pivot_table(course_data_posts, values='DATE_CREATED', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR created'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'DATE_CREATED':'ACTIVE_DAYS'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "301cd64f"
   },
   "outputs": [],
   "source": [
    "# getting the average number of active days per week per course\n",
    "active_days_avg_posts_written = pd.pivot_table(active_days_per_week_posts_written, values='ACTIVE_DAYS', index=['COURSE_PK1'],\n",
    "                     aggfunc='mean')\n",
    "active_days_avg_posts_written = active_days_avg_posts_written.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "454fad18"
   },
   "outputs": [],
   "source": [
    "# leaving only those weeks with #active days higher than average #active days in a week per course\n",
    "active_days_per_week_posts_written = active_days_per_week_posts_written[active_days_per_week_posts_written.apply(lambda x: x['ACTIVE_DAYS'] >= active_days_avg_posts_written_train['ACTIVE_DAYS'][x['COURSE_PK1']], axis = 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87eaed0d",
    "outputId": "092dd4bd-2e06-4e7a-d4fc-08619f424b2d"
   },
   "outputs": [],
   "source": [
    "# getting a proportion of active weeks for the writing posts activity type\n",
    "num_active_weeks_posts_written = pd.pivot_table(active_days_per_week_posts_written, values='WEEK and YEAR created', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'WEEK and YEAR created':'ACTIVE_WEEKS'})\n",
    "num_active_weeks_posts_written['PROPORTION_ACTIVE_WEEKS'] = num_active_weeks_posts_written['ACTIVE_WEEKS']/course_duration_weeks\n",
    "\n",
    "plt.hist(num_active_weeks_posts_written['PROPORTION_ACTIVE_WEEKS'], bins= 5)\n",
    "plt.title('PROPORTION OF ACTIVE WEEKS FOR FORUM CONTRIBUTION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a67f8fa8"
   },
   "source": [
    "###  Feature N°2: PROPORTION_POSTS_READ\n",
    "\"The proportion of posts a student has read\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91043c98",
    "outputId": "b42a06f3-8479-4527-9ded-8b85c385b846"
   },
   "outputs": [],
   "source": [
    "total_number_posts = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'TOTAL'})\n",
    "\n",
    "print(total_number_posts)\n",
    "\n",
    "pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'TOTAL'})\n",
    "\n",
    "pd.pivot_table(course_data_posts_consume, values='NUM_READ_POSTS', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc='max').reset_index()\n",
    "\n",
    "pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index()\n",
    "\n",
    "course_data_posts['COURSE_PK1'] = course_data_posts['COURSE_PK1'].astype(int)\n",
    "course_data_posts['CONTEXT_PK1'] = course_data_posts['CONTEXT_PK1'].astype(int)\n",
    "\n",
    "course_data_posts_consume['COURSE_PK1'] =course_data_posts_consume['COURSE_PK1'].astype(int)\n",
    "course_data_posts_consume['CONTEXT_PK1'] = course_data_posts_consume['CONTEXT_PK1'].astype(int)\n",
    "\n",
    "course_data_posts_consume = course_data_posts_consume.merge(pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'CONTEXT_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index(), on = ['COURSE_PK1','CONTEXT_PK1'], how = 'left')\n",
    "\n",
    "course_data_posts_consume['NEW_NUM_READ_POSTS'] = course_data_posts_consume.apply(lambda x: x['NUM_READ_POSTS'] if x['NUM_READ_POSTS'] < x['POST_PK1'] else x['POST_PK1'], axis = 1)\n",
    "\n",
    "proportion_posts_read = pd.pivot_table(course_data_posts_consume, values='NEW_NUM_READ_POSTS', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc='sum').reset_index()\n",
    "proportion_posts_read = proportion_posts_read.merge(total_number_posts, on = ['COURSE_PK1'])\n",
    "\n",
    "proportion_posts_read['PROPORTION_POSTS_READ'] = proportion_posts_read['NEW_NUM_READ_POSTS']/proportion_posts_read['TOTAL']\n",
    "\n",
    "plt.hist(proportion_posts_read['PROPORTION_POSTS_READ'], bins= 5)\n",
    "plt.title('PROPORTION OF POSTS READ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°3: POSTS_CREATED\n",
    "\"The amount of posts a student has created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e94b273",
    "outputId": "ba4d9a78-010d-45a3-f061-1e2c20890bac"
   },
   "outputs": [],
   "source": [
    "posts_created = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_CREATED'})\n",
    "\n",
    "print(posts_created.head())\n",
    "\n",
    "plt.hist(posts_created['POSTS_CREATED'], bins= 15)\n",
    "plt.title('POSTS_CREATED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b7dc2f0"
   },
   "source": [
    "## Overall Regularity of Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvBZRe7pRqV5"
   },
   "source": [
    "###  Feature N°1: ENTROPY_CLICKS\n",
    "\"Entropy of clicks/learning action counts per session\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8d9a7ad",
    "outputId": "216a6324-55db-4ff0-a042-a4aaac724243"
   },
   "outputs": [],
   "source": [
    "# calculating total number of clicks per student per course\n",
    "sum_clicks_dict = pd.pivot_table(clicks_per_session, values='EVENTS', index=['COURSE_PK1', 'USER_PK1'],\n",
    "                     aggfunc='sum').to_dict()\n",
    "\n",
    "clicks_per_session['TOTAL_CLICKS'] = clicks_per_session.apply(lambda x: sum_clicks_dict['EVENTS'][(x['COURSE_PK1'], x['USER_PK1'])], axis = 1)\n",
    "\n",
    "print(clicks_per_session.head())\n",
    "\n",
    "clicks_per_session['PROBA'] = clicks_per_session['EVENTS']/clicks_per_session['TOTAL_CLICKS']\n",
    "\n",
    "print(clicks_per_session.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f704fad",
    "outputId": "b7af8a81-efdd-4608-cde7-2e7a0cd6b314"
   },
   "outputs": [],
   "source": [
    "# Entropy of learning action counts per session (session of length of 0 are excluded)\n",
    "clicks_entropy = pd.DataFrame(clicks_per_session.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index()\n",
    "\n",
    "print(clicks_entropy.head())\n",
    "\n",
    "plt.hist(clicks_entropy['PROBA'], bins= 10)\n",
    "\n",
    "plt.title('ENTROPY OF LEARNING ACTION COUNTS PER SESSION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b07c50f6"
   },
   "source": [
    "### Feature N°2: ENTROPY_SESSION_LENGTH\n",
    "\"Entropy of students’ session lengths\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f963c7e8",
    "outputId": "88f205dd-372c-4f7d-a0d8-14e04da7987e"
   },
   "outputs": [],
   "source": [
    "session_duration_user_dict = pd.pivot_table(session_duration, values='SECONDS', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).rename(columns = {'SECONDS':'SESSION_DURATION'}).to_dict()\n",
    "session_duration['TOTAL_DURATION'] = session_duration.apply(lambda x: session_duration_user_dict['SESSION_DURATION'][(x['COURSE_PK1'], x['USER_PK1'])], axis = 1)\n",
    "session_duration['PROBA'] = session_duration['SECONDS']/session_duration['TOTAL_DURATION']\n",
    "\n",
    "# Entropy of session length\n",
    "duration_entropy = pd.DataFrame(session_duration.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index()\n",
    "\n",
    "print(duration_entropy.head())\n",
    "\n",
    "plt.hist(duration_entropy['PROBA'], bins= 10)\n",
    "\n",
    "plt.title('ENTROPY OF SESSION LENGTH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e98c0992"
   },
   "source": [
    "### Feature N°3: PROPORTION_WEEKS_FIRST_DAY_ACTIVE\n",
    "\"The proportion of weeks for which the students were active on monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3de4afa0",
    "outputId": "a4070988-df87-4237-a916-c95ef6f117bf"
   },
   "outputs": [],
   "source": [
    "course_data['DAY_OF_WEEK'] = course_data.apply(lambda x: x['DATE'].isoweekday(), axis = 1)\n",
    "\n",
    "weeks_with_active_first_day = pd.pivot_table(course_data[course_data['DAY_OF_WEEK'] == 1], values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'WEEK and YEAR':'ACTIVE_WEEKS'})\n",
    "weeks_with_active_first_day\n",
    "weeks_with_active_first_day['PROPORTION_ACTIVE_WEEKS'] = weeks_with_active_first_day['ACTIVE_WEEKS']/course_duration_weeks\n",
    "\n",
    "print(weeks_with_active_first_day.head())\n",
    "\n",
    "plt.hist(weeks_with_active_first_day['PROPORTION_ACTIVE_WEEKS'], bins= 10)\n",
    "\n",
    "plt.title('PROPORTION OF WEEKS WITH ACTIVE FIRST DAY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e795510"
   },
   "source": [
    "### Feature N°4: PROPORTION_LA_FIRST_DAY_OF_WEEK\n",
    "\"Proportion of learning actions on monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f40ddd0",
    "outputId": "43ea7e7a-bb77-424d-8de5-63dd593ee20a"
   },
   "outputs": [],
   "source": [
    "course_data_non_zero['DAY_OF_WEEK'] = course_data_non_zero.apply(lambda x: x['DATE'].isoweekday(), axis = 1)\n",
    "\n",
    "clicks_per_timestamp = pd.pivot_table(course_data_non_zero, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'new_session_id','TIMESTAMP_NEW'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)\n",
    "\n",
    "# replace the rows with #events > 1 by 1\n",
    "# as Toledo logs all the subfolders of an opened parent folder as a separate event, we replace this multiple events by just 1 event of openening a folder\n",
    "mask = clicks_per_timestamp['EVENTS'] > 1\n",
    "column_name = 'EVENTS'\n",
    "clicks_per_timestamp.loc[mask, column_name] = 1\n",
    "clicks_per_timestamp.sort_values(by = 'EVENTS', ascending = False)\n",
    "\n",
    "clicks_per_timestamp['DAY_OF_WEEK'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].isoweekday(), axis = 1)\n",
    "\n",
    "clicks_per_timestamp['WEEK'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[1], axis = 1)\n",
    "clicks_per_timestamp['YEAR'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].isocalendar()[0], axis = 1)\n",
    "clicks_per_timestamp['WEEK'] = clicks_per_timestamp['WEEK'].astype(str)\n",
    "clicks_per_timestamp['YEAR'] = clicks_per_timestamp['YEAR'].astype(str)\n",
    "clicks_per_timestamp['WEEK and YEAR'] = 'Week ' + clicks_per_timestamp['WEEK'] + ' of ' + clicks_per_timestamp['YEAR']\n",
    "clicks_per_timestamp['DATE'] = clicks_per_timestamp.apply(lambda x: x['TIMESTAMP_NEW'].date(), axis = 1)\n",
    "\n",
    "total_learning_actions = pd.pivot_table(clicks_per_timestamp, values='EVENTS', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "print(total_learning_actions.head())\n",
    "\n",
    "learning_actions_first_day = pd.pivot_table(clicks_per_timestamp[clicks_per_timestamp['DAY_OF_WEEK'] == 1], values='EVENTS', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'EVENTS':'EVENTS_FIRST_DAY'})\n",
    "\n",
    "print(learning_actions_first_day.head())\n",
    "\n",
    "total_learning_actions = total_learning_actions.merge(learning_actions_first_day, on = ['COURSE_PK1','USER_PK1','WEEK and YEAR'], how = 'left').fillna(0)\n",
    "total_learning_actions['PROPORTION'] = total_learning_actions['EVENTS_FIRST_DAY']/total_learning_actions['EVENTS']\n",
    "\n",
    "print(total_learning_actions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "af543343",
    "outputId": "d3fe90c9-0581-40fc-f8bb-1cdd62795da3"
   },
   "outputs": [],
   "source": [
    "average_proportion_la_first_day = pd.pivot_table(total_learning_actions, values='PROPORTION', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='median').fillna(0).reset_index()\n",
    "\n",
    "print(average_proportion_la_first_day.head())\n",
    "\n",
    "plt.hist(average_proportion_la_first_day['PROPORTION'], bins= 15)\n",
    "\n",
    "plt.title('PROPORTION OF LA ON THE FIRST DAY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f794e23e"
   },
   "source": [
    "## Learning Actions Specific Regularity of Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°1: ENTROPY_FORUM_CONTRIBUTION_DAILY\n",
    "\"Entropy of daily posts written \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f308950f",
    "outputId": "59994548-6348-4387-c12c-b42c2ab0e712"
   },
   "outputs": [],
   "source": [
    "n_weeks = len(set(course_data_non_zero['WEEK and YEAR']))\n",
    "\n",
    "posts_per_day = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1','USER_PK1', 'DATE_CREATED'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_PER_DAY'})\n",
    "print(posts_per_day.head())\n",
    "\n",
    "posts_total = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_TOTAL'})\n",
    "print(posts_total.head())\n",
    "\n",
    "posts_per_day = posts_per_day.merge(posts_total, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "posts_per_day['PROBA'] = (posts_per_day['POSTS_PER_DAY']/posts_per_day['POSTS_TOTAL']).fillna(0)\n",
    "posts_per_day = pd.DataFrame(posts_per_day.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(course_duration))\n",
    "print(posts_per_day.head())\n",
    "\n",
    "plt.hist(posts_per_day['PROBA'], bins= 10)\n",
    "plt.title('ENTROPY OF DAILY POSTS WRITTEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature N°2: ENTROPY_FORUM_CONTRIBUTION_WEEKLY\n",
    "\"Entropy of weekly posts written\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "822637db",
    "outputId": "8eb73d65-c650-4111-8146-a1d5fd75353e"
   },
   "outputs": [],
   "source": [
    "posts_per_week = pd.pivot_table(course_data_posts, values='POST_PK1', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR created'],\n",
    "                     aggfunc=pd.Series.nunique).reset_index().rename(columns = {'POST_PK1':'POSTS_PER_WEEK'})\n",
    "\n",
    "posts_per_week = posts_per_week.merge(posts_total, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "posts_per_week['PROBA'] = (posts_per_week['POSTS_PER_WEEK']/posts_per_week['POSTS_TOTAL']).fillna(0)\n",
    "\n",
    "posts_per_week = pd.DataFrame(posts_per_week.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(n_weeks))\n",
    "\n",
    "plt.hist(posts_per_week['PROBA'], bins= 15)\n",
    "plt.title('ENTROPY OF WEEKLY POSTS WRITTEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b883a21"
   },
   "source": [
    "# MoneyWalks feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7db3a84"
   },
   "source": [
    "### Feature N°1: DIVERSITY_OVERALL\n",
    "\"A measure that refers  to how evenly sessions are distributed across weeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "298af728",
    "outputId": "fbcd1dec-42cb-44d1-c427-23e3c99d4eea"
   },
   "outputs": [],
   "source": [
    "sessions_per_week = pd.pivot_table(course_data_non_zero, values='new_session_id', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_PER_WEEK'})\n",
    "print(sessions_per_week.head())\n",
    "\n",
    "sessions_total = pd.pivot_table(course_data_non_zero, values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_TOTAL'})\n",
    "print(sessions_total.head())\n",
    "\n",
    "weeks_count_total = pd.pivot_table(course_data_non_zero, values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index()\n",
    "weeks_count_total['LOG NUM OF WEEKS'] = weeks_count_total.apply(lambda x: math.log2(x['WEEK and YEAR']), axis = 1)\n",
    "print(weeks_count_total.head())\n",
    "\n",
    "sessions_per_week = sessions_per_week.merge(sessions_total, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "sessions_per_week['PROBA'] = (sessions_per_week['SESSIONS_PER_WEEK']/sessions_per_week['SESSIONS_TOTAL']).fillna(0)\n",
    "sessions_per_week_diversity = pd.DataFrame(sessions_per_week.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(n_weeks))\n",
    "sessions_per_week_diversity = sessions_per_week_diversity.merge(weeks_count_total, how = 'left', on = ['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_diversity['DIVERSITY'] =  (sessions_per_week_diversity['PROBA']/sessions_per_week_diversity['LOG NUM OF WEEKS'])\n",
    "print(sessions_per_week.head())\n",
    "print(sessions_per_week_diversity.head())\n",
    "\n",
    "plt.hist(sessions_per_week_diversity['DIVERSITY'], bins= 15)\n",
    "plt.title('DIVERSITY OVERALL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7764b65e"
   },
   "source": [
    "### Feature N°2: DIVERSITY_EXAM_PERIOD\n",
    "\"A measure that refers  to how evenly sessions are distributed across weeks during the exam period\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d75c3e17",
    "outputId": "4c831328-8206-4ee7-9f9a-a77d6aa62d85"
   },
   "outputs": [],
   "source": [
    "sessions_per_week_exam = pd.pivot_table(course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(exam_weeks)], values='new_session_id', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_PER_WEEK_EXAM'})\n",
    "print(sessions_per_week_exam.head())\n",
    "\n",
    "sessions_total_exam = pd.pivot_table(course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(exam_weeks)], values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_TOTAL_EXAM'})\n",
    "print(sessions_total_exam)\n",
    "\n",
    "weeks_count_total_exam = pd.pivot_table(course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(exam_weeks)], values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index()\n",
    "weeks_count_total_exam['LOG NUM OF WEEKS'] = weeks_count_total_exam.apply(lambda x: math.log2(x['WEEK and YEAR']), axis = 1)\n",
    "\n",
    "sessions_per_week_exam = sessions_per_week_exam.merge(sessions_total_exam, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "sessions_per_week_exam['PROBA'] = (sessions_per_week_exam['SESSIONS_PER_WEEK_EXAM']/sessions_per_week_exam['SESSIONS_TOTAL_EXAM']).fillna(0)\n",
    "sessions_per_week_diversity_exam = pd.DataFrame(sessions_per_week_exam.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(n_weeks))\n",
    "sessions_per_week_diversity_exam = sessions_per_week_diversity_exam.merge(weeks_count_total_exam, how = 'left', on = ['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_diversity_exam['DIVERSITY_EXAM_PERIOD'] =  (sessions_per_week_diversity_exam['PROBA']/sessions_per_week_diversity_exam['LOG NUM OF WEEKS'])\n",
    "\n",
    "plt.hist(sessions_per_week_diversity_exam['DIVERSITY_EXAM_PERIOD'], bins= 15)\n",
    "plt.title('DIVERSITY EXAM PERIOD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxRThN3NRqWE"
   },
   "source": [
    "## Loyalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25d3b74d"
   },
   "source": [
    "### Feature N°1: LOYALTY_OVERALL\n",
    "\"Proportion of sessions that took place in the top 3 weeks regarding session count. A higher value represents many sessions concentrated in only a few weeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc6df24b",
    "outputId": "29243313-1977-442e-d0ad-029917e8eae0"
   },
   "outputs": [],
   "source": [
    "top3_sessions = sessions_per_week.groupby(by = ['COURSE_PK1','USER_PK1']).apply(lambda x: x.nlargest(3, \"SESSIONS_PER_WEEK\")).reset_index(drop=True)\n",
    "\n",
    "sessions_total_in_top3_weeks = pd.pivot_table(top3_sessions, values='SESSIONS_PER_WEEK', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SESSIONS_PER_WEEK':'SESSIONS_TOTAL_TOP3'})\n",
    "\n",
    "sessions_total_in_top3_weeks = sessions_total_in_top3_weeks.merge(sessions_total, how='left', on = ['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "sessions_total_in_top3_weeks['LOYALTY'] = sessions_total_in_top3_weeks['SESSIONS_TOTAL_TOP3']/sessions_total_in_top3_weeks['SESSIONS_TOTAL']\n",
    "\n",
    "plt.hist(sessions_total_in_top3_weeks['LOYALTY'], bins= 15)\n",
    "\n",
    "plt.title('LOYALTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "028a6a8d"
   },
   "source": [
    "### Feature N°2: LOYALTY_EXAM\n",
    "\"Proportion of sessions that took place in the top 3 weeks regarding session count, focusing on exam period\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba610378",
    "outputId": "86eae7a3-6eff-4de5-a8cd-ff4a4bb0c1fe"
   },
   "outputs": [],
   "source": [
    "exam_sessions = sessions_per_week[sessions_per_week['WEEK and YEAR'].isin(exam_weeks)]\n",
    "\n",
    "print(exam_sessions.head())\n",
    "\n",
    "sessions_total_in_exam_weeks = pd.pivot_table(exam_sessions, values='SESSIONS_PER_WEEK', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SESSIONS_PER_WEEK':'SESSIONS_TOTAL_EXAM'})\n",
    "\n",
    "sessions_total_in_exam_weeks = sessions_total_in_exam_weeks.merge(sessions_total, how='left', on = ['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "sessions_total_in_exam_weeks['LOYALTY'] = sessions_total_in_exam_weeks['SESSIONS_TOTAL_EXAM']/sessions_total_in_exam_weeks['SESSIONS_TOTAL']\n",
    "\n",
    "plt.hist(sessions_total_in_exam_weeks['LOYALTY'], bins= 15)\n",
    "\n",
    "plt.title('LOYALTY EXAM PERIOD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f4354ea"
   },
   "source": [
    "## Regularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ff6ea1f"
   },
   "source": [
    "### Feature N°1: REGULARITY\n",
    "\"Measurement for the level of diversity in students’ behavior over time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e623589a"
   },
   "outputs": [],
   "source": [
    "course_data_non_zero_semester = course_data_non_zero[course_data_non_zero['WEEK and YEAR'].isin(semester_weeks)]\n",
    "\n",
    "sessions_per_week_semester = pd.pivot_table(course_data_non_zero_semester, values='new_session_id', index=['COURSE_PK1','USER_PK1', 'WEEK and YEAR'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_PER_WEEK'})\n",
    "\n",
    "sessions_total_semester = pd.pivot_table(course_data_non_zero_semester, values='new_session_id', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index().rename(columns = {'new_session_id':'SESSIONS_TOTAL'})\n",
    "\n",
    "weeks_count_total_semester = pd.pivot_table(course_data_non_zero_semester, values='WEEK and YEAR', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc=pd.Series.nunique).fillna(0).reset_index()\n",
    "weeks_count_total_semester['LOG NUM OF WEEKS'] = weeks_count_total_semester.apply(lambda x: math.log2(x['WEEK and YEAR']), axis = 1)\n",
    "\n",
    "sessions_per_week_first_month = sessions_per_week_semester.merge(sessions_total_semester, how = 'left', on = ['COURSE_PK1', 'USER_PK1'])\n",
    "sessions_per_week_first_month['PROBA'] = (sessions_per_week_first_month['SESSIONS_PER_WEEK']/sessions_per_week_first_month['SESSIONS_TOTAL']).fillna(0)\n",
    "\n",
    "\n",
    "sessions_per_week_first_month_diversity = pd.DataFrame(sessions_per_week_first_month.groupby(by = ['COURSE_PK1', 'USER_PK1'])['PROBA'].apply(lambda x: entropy(x, base=2))).reset_index().fillna(math.log2(len(semester_weeks)))\n",
    "sessions_per_week_first_month_diversity = sessions_per_week_first_month_diversity.merge(weeks_count_total_semester, how = 'left', on = ['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_first_month_diversity['DIVERSITY_SEMESTER'] =  sessions_per_week_first_month_diversity['PROBA']/sessions_per_week_first_month_diversity['LOG NUM OF WEEKS']\n",
    "\n",
    "diversity_for_regularity = sessions_per_week_diversity[['COURSE_PK1','USER_PK1','DIVERSITY']].merge(sessions_per_week_first_month_diversity[['COURSE_PK1','USER_PK1','DIVERSITY_SEMESTER']], how = 'left', on = ['COURSE_PK1','USER_PK1']).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa4346fd"
   },
   "source": [
    "### Regularity N°2: Loyalty semester weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "694d1ba1",
    "outputId": "a07c1d8b-7b8b-4c0c-fc7c-0844e4cca420"
   },
   "outputs": [],
   "source": [
    "top3_sessions_semester = sessions_per_week[sessions_per_week['WEEK and YEAR'].isin(semester_weeks)].groupby(by = ['COURSE_PK1','USER_PK1']).apply(lambda x: x.nlargest(3, \"SESSIONS_PER_WEEK\")).reset_index(drop=True)\n",
    "\n",
    "sessions_total_in_top3_weeks_semester = pd.pivot_table(top3_sessions_semester, values='SESSIONS_PER_WEEK', index=['COURSE_PK1','USER_PK1'],\n",
    "                     aggfunc='sum').fillna(0).reset_index().rename(columns = {'SESSIONS_PER_WEEK':'SESSIONS_TOTAL_TOP3'})\n",
    "\n",
    "sessions_total_in_top3_weeks_semester = sessions_total_in_top3_weeks_semester.merge(sessions_total_semester, how='left', on = ['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "sessions_total_in_top3_weeks_semester['LOYALTY_SEMESTER'] = sessions_total_in_top3_weeks_semester['SESSIONS_TOTAL_TOP3']/sessions_total_in_top3_weeks_semester['SESSIONS_TOTAL']\n",
    "\n",
    "regularity_df = sessions_per_week_diversity[['COURSE_PK1','USER_PK1','DIVERSITY']].merge(sessions_total_in_top3_weeks[['COURSE_PK1','USER_PK1','LOYALTY']])\n",
    "\n",
    "regularity_df = regularity_df.merge(diversity_for_regularity[['COURSE_PK1','USER_PK1','DIVERSITY_SEMESTER']], how = 'left', on = ['COURSE_PK1','USER_PK1']).merge(sessions_total_in_top3_weeks_semester[['COURSE_PK1','USER_PK1','LOYALTY_SEMESTER']], how= 'left', on = ['COURSE_PK1','USER_PK1'] ).fillna(0)\n",
    "\n",
    "regularity_df['REGULARITY'] = 1- np.sqrt((regularity_df['DIVERSITY_SEMESTER'] - regularity_df['DIVERSITY'] )**2 +  (regularity_df['LOYALTY_SEMESTER'] - regularity_df['LOYALTY'] )**2)/np.sqrt(2)\n",
    "\n",
    "plt.hist(regularity_df['REGULARITY'], bins= 15)\n",
    "\n",
    "plt.title('REGULARITY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bce01ba"
   },
   "source": [
    "# Final Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba57295d"
   },
   "outputs": [],
   "source": [
    "grades['Score januari'] = grades.apply(lambda x: np.nan if (pd.isnull(x['Score januari']) or x['Score januari'] == 'GR' or not str(x['Score januari']).replace('.', '').isdigit()) else int(str(x['Score januari']).split('.')[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4bj7Y8xdDMx",
    "outputId": "5a0b7e6c-7dd5-4f4b-fd65-c9f5acdb6183"
   },
   "outputs": [],
   "source": [
    "# Merging given features\n",
    "course_grades = grades[['USER_PK1', 'Score januari',\n",
    "                            'Score juni']].merge(session_count_zero_duration.rename(columns =\n",
    "                            {'SESSION_COUNT':'ZERO_SESSIONS_COUNT'}),\n",
    "                             how = 'left', on=['USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(session_count.rename(columns = {'SESSION_COUNT':'NON_ZERO_SESSION_COUNT'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(session_duration_user.rename(columns = {'SESSION_DURATION':'TOTAL_SESSION_DURATION'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(session_duration_user_avg.rename(columns = {'SESSION_DURATION_AVG':'AVERAGE_SESSION_DURATION'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(clicks_per_user.rename(columns = {'CLICKS_PER_SESSION_AVG':'AVERAGE_ACTIONS_PER_SESSION'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(num_active_weeks[['COURSE_PK1','USER_PK1','PROPORTION_ACTIVE_WEEKS']],\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(active_days[['COURSE_PK1','USER_PK1','ACTIVE_DAYS_PROPORTION']],\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(active_days_per_week_per_user, how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(avg_time_distance_user.rename(columns = {'AVG_DIFF':'AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS'}),\n",
    "                             how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades['COURSE_PK1'] = course_grades['COURSE_PK1'].astype(float).fillna(0)\n",
    "course_grades['USER_PK1']=course_grades['USER_PK1'].astype(str)\n",
    "course_grades['COURSE_PK1']=course_grades['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(active_days_posts_written.rename(columns = {'ACTIVE_DAYS_PROPORTION':\n",
    "                                'ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION'})[['COURSE_PK1','USER_PK1',\n",
    "                                'ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION']], how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "course_grades = course_grades.merge(num_active_weeks_posts_written.rename(columns = {'PROPORTION_ACTIVE_WEEKS':\n",
    "                                'PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION'})[['COURSE_PK1','USER_PK1',\n",
    "                                'PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION']], how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "course_grades['USER_PK1']=course_grades['USER_PK1'].astype(int)\n",
    "\n",
    "proportion_posts_read['USER_PK1'] = proportion_posts_read['USER_PK1'].astype(int)\n",
    "course_grades = course_grades.merge(proportion_posts_read[['COURSE_PK1','USER_PK1','PROPORTION_POSTS_READ']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "posts_created['USER_PK1']=posts_created['USER_PK1'].astype(int)\n",
    "posts_created['COURSE_PK1']=posts_created['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(posts_created[['COURSE_PK1','USER_PK1','POSTS_CREATED']],how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "clicks_entropy['USER_PK1']=clicks_entropy['USER_PK1'].astype(int)\n",
    "clicks_entropy['COURSE_PK1']=clicks_entropy['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(clicks_entropy.rename(columns={'PROBA':'ENTROPY_CLICKS'}),how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "duration_entropy['USER_PK1']=duration_entropy['USER_PK1'].astype(int)\n",
    "duration_entropy['COURSE_PK1']=duration_entropy['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(duration_entropy.rename(columns={'PROBA':'ENTROPY_SESSION_LENGTH'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "weeks_with_active_first_day['USER_PK1']=weeks_with_active_first_day['USER_PK1'].astype(int)\n",
    "weeks_with_active_first_day['COURSE_PK1']=weeks_with_active_first_day['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(weeks_with_active_first_day.rename(columns={'PROPORTION_ACTIVE_WEEKS':\n",
    "                                    'PROPORTION_WEEKS_FIRST_DAY_ACTIVE'})[['COURSE_PK1','USER_PK1',\n",
    "                                    'PROPORTION_WEEKS_FIRST_DAY_ACTIVE']], how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "average_proportion_la_first_day['USER_PK1']=average_proportion_la_first_day['USER_PK1'].astype(int)\n",
    "average_proportion_la_first_day['COURSE_PK1']=average_proportion_la_first_day['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(average_proportion_la_first_day.rename(columns={'PROPORTION':'PROPORTION_LA_FIRST_DAY_OF_WEEK'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "posts_per_day['USER_PK1']=posts_per_day['USER_PK1'].astype(int)\n",
    "posts_per_day['COURSE_PK1']=posts_per_day['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(posts_per_day.rename(columns={'PROBA':'ENTROPY_FORUM_CONTRIBUTION_DAILY'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "posts_per_week['USER_PK1']=posts_per_week['USER_PK1'].astype(int)\n",
    "posts_per_week['COURSE_PK1']=posts_per_week['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(posts_per_week.rename(columns={'PROBA':'ENTROPY_FORUM_CONTRIBUTION_WEEKLY'}),\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "sessions_per_week_diversity['USER_PK1']=sessions_per_week_diversity['USER_PK1'].astype(int)\n",
    "sessions_per_week_diversity['COURSE_PK1']=sessions_per_week_diversity['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_per_week_diversity.rename(columns={'DIVERSITY':\n",
    "                                'DIVERSITY_OVERALL'})[['COURSE_PK1','USER_PK1','DIVERSITY_OVERALL']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "sessions_per_week_diversity_exam['USER_PK1']=sessions_per_week_diversity_exam['USER_PK1'].astype(int)\n",
    "sessions_per_week_diversity_exam['COURSE_PK1']=sessions_per_week_diversity_exam['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_per_week_diversity_exam[['COURSE_PK1','USER_PK1','DIVERSITY_EXAM_PERIOD']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "\n",
    "sessions_total_in_top3_weeks['USER_PK1']=sessions_total_in_top3_weeks['USER_PK1'].astype(int)\n",
    "sessions_total_in_top3_weeks['COURSE_PK1']=sessions_total_in_top3_weeks['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_total_in_top3_weeks.rename(columns={'LOYALTY':\n",
    "                                'LOYALTY_OVERALL'})[['COURSE_PK1','USER_PK1','LOYALTY_OVERALL']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "sessions_total_in_exam_weeks['USER_PK1']=sessions_total_in_exam_weeks['USER_PK1'].astype(int)\n",
    "sessions_total_in_exam_weeks['COURSE_PK1']=sessions_total_in_exam_weeks['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(sessions_total_in_exam_weeks.rename(columns={'LOYALTY':\n",
    "                                'LOYALTY_EXAM'})[['COURSE_PK1','USER_PK1','LOYALTY_EXAM']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "regularity_df['USER_PK1']=regularity_df['USER_PK1'].astype(int)\n",
    "regularity_df['COURSE_PK1']=regularity_df['COURSE_PK1'].astype(int)\n",
    "\n",
    "course_grades = course_grades.merge(regularity_df[['COURSE_PK1','USER_PK1','REGULARITY']],\n",
    "                                 how = 'left', on=['COURSE_PK1','USER_PK1'])\n",
    "print(course_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T97b8gOGVYr5",
    "outputId": "600f06f5-cac6-42be-9a29-f09ecd7331a9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "course_grades['Score januari'] = course_grades['Score januari'].astype(int)\n",
    "course_grades['USER_PK1'] = course_grades['USER_PK1'].astype(str)\n",
    "\n",
    "merged_data_test = pd.merge(course_grades,workingdata_FI1 , on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FI2, on=[\"USER_PK1\"] , how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FI3, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FI4, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test,workingdata_FI5, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FT2, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FT5, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, work, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FR1, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FR2, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FR3, on=[\"USER_PK1\"], how='left')\n",
    "merged_data_test = pd.merge(merged_data_test, workingdata_FR4, on=[\"USER_PK1\"], how='left')\n",
    "\n",
    "print(merged_data_test)\n",
    "\n",
    "merged_data_test['Score_bins'] = merged_data_test['Score_bins'].astype(str).fillna(0)\n",
    "merged_data_test.fillna(0, inplace=True)\n",
    "print(merged_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTTRLqTw0Kfi"
   },
   "outputs": [],
   "source": [
    "merged_data1 = merged_data1.drop(merged_data_test[merged_data_test['Score januari'] == 0].index)\n",
    "merged_data_test = merged_data_test.drop(merged_data_test[merged_data_test['Score januari'] == 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErI2vCfyRqWd"
   },
   "source": [
    "### Defining X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1ZV6noeaN3u",
    "outputId": "f6eddff1-5b77-4f51-c8de-d339eb859040"
   },
   "outputs": [],
   "source": [
    "X_train = merged_data1.drop(columns=[ 'Score januari' , 'Score juni','USER_PK1','COURSE_PK1','Score_bins'])\n",
    "y_train = merged_data1.drop(columns=['sum_read_posts', 'sum_comments', 'sum_views', 'sum_ind_read', 'mean_text_length','days_until_first_login','entropy_per_student','above_median_count','percentageweeks_above_median', 'aggregate_mean_squared_diff_per_student','total_score_FR2', 'exceptional_view','total_score','ZERO_SESSIONS_COUNT', 'NON_ZERO_SESSION_COUNT', 'TOTAL_SESSION_DURATION',\n",
    "            'AVERAGE_SESSION_DURATION', 'AVERAGE_ACTIONS_PER_SESSION', 'PROPORTION_ACTIVE_WEEKS',\n",
    "            'ACTIVE_DAYS_PROPORTION', 'AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS',\n",
    "            'ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION', 'PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION',\n",
    "            'PROPORTION_POSTS_READ', 'POSTS_CREATED', 'ENTROPY_CLICKS', 'ENTROPY_SESSION_LENGTH',\n",
    "            'PROPORTION_WEEKS_FIRST_DAY_ACTIVE', 'PROPORTION_LA_FIRST_DAY_OF_WEEK',\n",
    "            'ENTROPY_FORUM_CONTRIBUTION_DAILY', 'ENTROPY_FORUM_CONTRIBUTION_WEEKLY', 'DIVERSITY_OVERALL',\n",
    "            'DIVERSITY_EXAM_PERIOD', 'LOYALTY_OVERALL', 'LOYALTY_EXAM', 'REGULARITY','AVERAGE_ACTIVE_DAYS_PER_WEEK','COURSE_PK1', 'Score juni','USER_PK1','Score_bins'])\n",
    "\n",
    "X_test= merged_data_test.drop(columns=['Score januari',  'Score juni','USER_PK1','COURSE_PK1','Score_bins'])\n",
    "y_test= merged_data_test.drop(columns=['sum_read_posts', 'sum_comments', 'sum_views', 'sum_ind_read', 'mean_text_length','days_until_first_login','entropy_per_student','above_median_count','percentageweeks_above_median', 'aggregate_mean_squared_diff_per_student','total_score_FR2', 'exceptional_view','total_score','ZERO_SESSIONS_COUNT', 'NON_ZERO_SESSION_COUNT', 'TOTAL_SESSION_DURATION',\n",
    "            'AVERAGE_SESSION_DURATION', 'AVERAGE_ACTIONS_PER_SESSION', 'PROPORTION_ACTIVE_WEEKS',\n",
    "            'ACTIVE_DAYS_PROPORTION', 'AVG_TIME_DIFF_BETWEEN_ACTIVE_DAYS',\n",
    "            'ACTIVE_DAYS_PROPORTION_FORUM_CONTRIBUTION', 'PROPORTION_ACTIVE_WEEKS_FORUM_CONTRIBUTION',\n",
    "            'PROPORTION_POSTS_READ', 'POSTS_CREATED', 'ENTROPY_CLICKS', 'ENTROPY_SESSION_LENGTH',\n",
    "            'PROPORTION_WEEKS_FIRST_DAY_ACTIVE', 'PROPORTION_LA_FIRST_DAY_OF_WEEK',\n",
    "            'ENTROPY_FORUM_CONTRIBUTION_DAILY', 'ENTROPY_FORUM_CONTRIBUTION_WEEKLY', 'DIVERSITY_OVERALL',\n",
    "            'DIVERSITY_EXAM_PERIOD', 'LOYALTY_OVERALL', 'LOYALTY_EXAM', 'REGULARITY','AVERAGE_ACTIVE_DAYS_PER_WEEK','COURSE_PK1', 'Score juni','USER_PK1','Score_bins'])\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s86nW0wFRqWe"
   },
   "source": [
    "# ------- FEATURE SELECTION -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNTGBOGcnqzQ",
    "outputId": "e23ead74-f9a3-4d34-f442-689b9e24969b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Assuming X_train is your feature matrix and y_train is your target variable\n",
    "# DataFrame\n",
    "X = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Perform stepwise selection\n",
    "def stepwise_selection(X, y,\n",
    "                       initial_list=[],\n",
    "                       threshold_in=0.30,\n",
    "                       threshold_out=0.40,\n",
    "                       verbose=True):\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed = False\n",
    "        # Forward step\n",
    "        excluded = list(set(X.columns) - set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # Backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # Use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max()  # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed = True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            worst_feature_name = pvalues.index[worst_feature]  # Get the name of the worst feature\n",
    "            included.remove(worst_feature_name)  # Remove the feature name instead of its index\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature_name, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    # Fit the final model\n",
    "    final_model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "    # Print the coefficients\n",
    "    print(\"Coefficients of selected features:\")\n",
    "    print(final_model.params)\n",
    "\n",
    "    # Calculate standard deviations of predictors\n",
    "    std_devs = X[included].std()\n",
    "    # Calculate standardized coefficients\n",
    "    standardized_coefs = final_model.params / std_devs\n",
    "    # Print the standardized coefficients\n",
    "    print(\"\\nStandardized coefficients of selected features:\")\n",
    "    print(standardized_coefs)\n",
    "\n",
    "    return included\n",
    "\n",
    "# Performing stepwise selection\n",
    "selected_features_stepwise = stepwise_selection(X, y_train)\n",
    "\n",
    "print(\"\\nSelected features using stepwise regression:\")\n",
    "print(selected_features_stepwise)\n",
    "\n",
    "# Printing the features not selected by stepwise regression\n",
    "not_selected_features_stepwise = list(set(X.columns) - set(selected_features_stepwise))\n",
    "\n",
    "print(\"\\nFeatures not selected by stepwise regression:\")\n",
    "print(not_selected_features_stepwise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stEOFfz_YEJY"
   },
   "source": [
    "Implement results Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3TR7vXrH0g_e",
    "outputId": "4f0da39c-57e6-4413-f00d-5742ccc98659"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = not_selected_features_stepwise)\n",
    "\n",
    "X_test= X_test.drop(columns = not_selected_features_stepwise)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------- Machine Learning Techniques -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AnhvrE34RqWp"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJYuS0ILYEJe",
    "outputId": "11c71c9c-a761-41f5-c344-04d10371ba9d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define edges for the bins\n",
    "bin_edges = [-0.0001, 9, 20]\n",
    "\n",
    "# Transform target variables to bins\n",
    "y_values = y_train['Score januari'].values\n",
    "y_bins = pd.cut(y_values, bins=bin_edges, labels=False)\n",
    "\n",
    "# Defining scorers\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, average='weighted'),\n",
    "           'recall': make_scorer(recall_score, average='weighted'),\n",
    "           'f1': make_scorer(f1_score, average='weighted'),\n",
    "           'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "           'specificity': make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0,0] / (confusion_matrix(y, y_pred)[0,0] + confusion_matrix(y, y_pred)[0,1]))}\n",
    "\n",
    "# Model initialization\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Defining the number of folds for inner CV\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining hyperparameters with smart parameters\n",
    "param_dist = {\n",
    "    'C': randint(1, 100),  # Regularization parameter\n",
    "    'penalty': ['l1', 'l2'],  # Penalty norm\n",
    "    'max_iter': randint(100, 1000)  # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Performing nested cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv_scores = []\n",
    "test_fold_scores = []\n",
    "best_params_list = []\n",
    "for fold_idx, (train_index, test_index) in enumerate(outer_cv.split(X_train_scaled, y_bins), 1):\n",
    "    X_train_fold, X_test_fold = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_bins[train_index], y_bins[test_index]\n",
    "\n",
    "    # Performing hyperparameter tuning with inner CV\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=inner_cv,\n",
    "                                       scoring='accuracy', verbose=2, n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train_fold, y_train_fold)\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_params_list.append(best_params)\n",
    "    \n",
    "    # Evaluating the best model on the test fold\n",
    "    y_pred = best_model.predict(X_test_fold)\n",
    "    accuracy = balanced_accuracy_score(y_test_fold, y_pred)\n",
    "    outer_cv_scores.append(accuracy)\n",
    "    test_fold_scores.append(accuracy)\n",
    "\n",
    "    print('        Best ACCURACY y1 model %.2f%%' % (random_search.best_score_ * 100))\n",
    "    print('        Best parameters model:', random_search.best_params_)\n",
    "\n",
    "    print('        Test ACCURACY  (on outer test fold) %.2f%%' % (outer_cv_scores[-1]*100))\n",
    "\n",
    "# Print the average test fold performance\n",
    "print(\"\\nAverage Test Fold Performance:\")\n",
    "print(\"Mean Accuracy:\", np.mean(test_fold_scores))\n",
    "print(\"Std Accuracy:\", np.std(test_fold_scores))\n",
    "\n",
    "# Training the model on the train data set\n",
    "model.fit(X_train_scaled, y_bins)\n",
    "\n",
    "# Predicting test data\n",
    "predictions_test = model.predict(X_test_scaled)\n",
    "\n",
    "# Transforming bin edges for test data\n",
    "y_test_values = y_test['Score januari'].values\n",
    "y_test_bins = pd.cut(y_test_values, bins=bin_edges, labels=False)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_test = accuracy_score(y_test_bins, predictions_test)\n",
    "precision_test = precision_score(y_test_bins, predictions_test, average='weighted')\n",
    "recall_test = recall_score(y_test_bins, predictions_test, average='weighted')\n",
    "f1_test = f1_score(y_test_bins, predictions_test, average='weighted')\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test_bins, predictions_test)\n",
    "specificity_test = confusion_matrix(y_test_bins, predictions_test)[0,0] / (confusion_matrix(y_test_bins, predictions_test)[0,0] + confusion_matrix(y_test_bins, predictions_test)[0,1])\n",
    "\n",
    "# Printing evaluation metrics\n",
    "print(\"\\nTest data:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1 Score:\", f1_test)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_test)\n",
    "print(\"Specificity:\", specificity_test)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_test = confusion_matrix(y_test_bins, predictions_test)\n",
    "print(\"\\nConfusion Matrix (Test data):\")\n",
    "print(conf_matrix_test)\n",
    "\n",
    "print(outer_cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I66_6f8QRqWr"
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYtaYIONr-Qu",
    "outputId": "41d57f86-f13a-4d45-a50c-c26a05411fdd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Defining edges for the bins\n",
    "bin_edges = [-0.0001, 9, 20]\n",
    "\n",
    "# Transforming target variables to bins\n",
    "y_values = y_train['Score januari'].values\n",
    "y_bins = pd.cut(y_values, bins=bin_edges, labels=False)\n",
    "\n",
    "# Defining scorers for cross-validation\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, average='weighted'),\n",
    "           'recall': make_scorer(recall_score, average='weighted'),\n",
    "           'f1': make_scorer(f1_score, average='weighted'),\n",
    "           'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "           'specificity': make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0,0] / (confusion_matrix(y, y_pred)[0,0] + confusion_matrix(y, y_pred)[0,1]))}\n",
    "\n",
    "# Model initialization\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Defining hyperparameters to tune\n",
    "param_dist = {\n",
    "    'n_estimators': randint(300, 600),\n",
    "    'max_depth': randint(20, 100),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Defining the number of folds for inner CV\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Performing nested cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv_scores = []\n",
    "test_fold_scores = []\n",
    "best_params_list = []\n",
    "for fold_idx, (train_index, test_index) in enumerate(outer_cv.split(X_train_scaled, y_bins), 1):\n",
    "    X_train_fold, X_test_fold = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_bins[train_index], y_bins[test_index]\n",
    "\n",
    "    # Performing hyperparameter tuning with inner CV\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=inner_cv, scoring=scoring, refit='accuracy', verbose=2, n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train_fold, y_train_fold)\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "\n",
    "    # Evaluating the best model on the test fold\n",
    "    y_pred = best_model.predict(X_test_fold)\n",
    "    accuracy = balanced_accuracy_score(y_test_fold, y_pred)\n",
    "    outer_cv_scores.append(accuracy)\n",
    "    test_fold_scores.append(accuracy)\n",
    "\n",
    "    print('        Best ACCURACY y1 model %.2f%%' % (random_search.best_score_ * 100))\n",
    "    print('        Best parameters model:', random_search.best_params_)\n",
    "\n",
    "    print('        Test ACCURACY  (on outer test fold) %.2f%%' % (outer_cv_scores[-1]*100))\n",
    "\n",
    "# Printing the average test fold performance\n",
    "print(\"\\nAverage Test Fold Performance:\")\n",
    "print(\"Mean Accuracy:\", np.mean(test_fold_scores))\n",
    "print(\"Std Accuracy:\", np.std(test_fold_scores))\n",
    "\n",
    "# Training the model on the train data set\n",
    "model.fit(X_train_scaled, y_bins)\n",
    "\n",
    "# Predicting test data\n",
    "predictions_test = model.predict(X_test_scaled)\n",
    "\n",
    "# Transforming bin edges for test data\n",
    "y_test_values = y_test['Score januari'].values\n",
    "y_test_bins = pd.cut(y_test_values, bins=bin_edges, labels=False)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_test = accuracy_score(y_test_bins, predictions_test)\n",
    "precision_test = precision_score(y_test_bins, predictions_test, average='weighted')\n",
    "recall_test = recall_score(y_test_bins, predictions_test, average='weighted')\n",
    "f1_test = f1_score(y_test_bins, predictions_test, average='weighted')\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test_bins, predictions_test)\n",
    "specificity_test = confusion_matrix(y_test_bins, predictions_test)[0,0] / (confusion_matrix(y_test_bins, predictions_test)[0,0] + confusion_matrix(y_test_bins, predictions_test)[0,1])\n",
    "\n",
    "# Printing evaluation metrics\n",
    "print(\"\\nTest data:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1 Score:\", f1_test)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_test)\n",
    "print(\"Specificity:\", specificity_test)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_test = confusion_matrix(y_test_bins, predictions_test)\n",
    "print(\"\\nConfusion Matrix (Test data):\")\n",
    "print(conf_matrix_test)\n",
    "\n",
    "print(outer_cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFlGZcyVRqWr"
   },
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQ_Kopc4rz8A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Defining edges for the bins\n",
    "bin_edges = [-0.0001, 9, 20]\n",
    "\n",
    "# Transforming target variables to bins\n",
    "y_values = y_train['Score januari'].values\n",
    "y_bins = pd.cut(y_values, bins=bin_edges, labels=False)\n",
    "\n",
    "# Defining scorers for cross-validation\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "           'precision': make_scorer(precision_score, average='weighted'),\n",
    "           'recall': make_scorer(recall_score, average='weighted'),\n",
    "           'f1': make_scorer(f1_score, average='weighted'),\n",
    "           'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "           'specificity': make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0,0] / (confusion_matrix(y, y_pred)[0,0] + confusion_matrix(y, y_pred)[0,1]))}\n",
    "\n",
    "# Model initialization\n",
    "model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Defining the number of folds for inner CV\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Defining hyperparameters with smart parameters\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "# Performing nested cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_cv_scores = []\n",
    "test_fold_scores = []\n",
    "best_params_list = []\n",
    "for fold_idx, (train_index, test_index) in enumerate(outer_cv.split(X_train_scaled, y_bins), 1):\n",
    "    X_train_fold, X_test_fold = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "    y_train_fold, y_test_fold = y_bins[train_index], y_bins[test_index]\n",
    "\n",
    "    # Performing hyperparameter tuning with inner CV\n",
    "    random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=inner_cv,\n",
    "                                       scoring='accuracy', verbose=2, n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train_fold, y_train_fold)\n",
    "    best_model = random_search.best_estimator_\n",
    "    best_params = random_search.best_params_\n",
    "    best_params_list.append(best_params)\n",
    "\n",
    "\n",
    "    # Evaluating the best model on the test fold\n",
    "    y_pred = best_model.predict(X_test_fold)\n",
    "    accuracy = balanced_accuracy_score(y_test_fold, y_pred)\n",
    "    outer_cv_scores.append(accuracy)\n",
    "    test_fold_scores.append(accuracy)\n",
    "\n",
    "    print('        Best ACCURACY y1 model %.2f%%' % (random_search.best_score_ * 100))\n",
    "    print('        Best parameters model:', random_search.best_params_)\n",
    "\n",
    "    print('        Test ACCURACY  (on outer test fold) %.2f%%' % (outer_cv_scores[-1]*100))\n",
    "\n",
    "\n",
    "# Printing the average test fold performance\n",
    "print(\"\\nAverage Test Fold Performance:\")\n",
    "print(\"Mean Accuracy:\", np.mean(test_fold_scores))\n",
    "print(\"Std Accuracy:\", np.std(test_fold_scores))\n",
    "\n",
    "# Training the model on the entire training data\n",
    "model.fit(X_train_scaled, y_bins)\n",
    "\n",
    "# Predicting test data\n",
    "predictions_test = model.predict(X_test_scaled)\n",
    "\n",
    "# Transforming bin edges for test data\n",
    "y_test_values = y_test['Score januari'].values\n",
    "y_test_bins = pd.cut(y_test_values, bins=bin_edges, labels=False)\n",
    "\n",
    "# Evaluation\n",
    "accuracy_test = accuracy_score(y_test_bins, predictions_test)\n",
    "precision_test = precision_score(y_test_bins, predictions_test, average='weighted')\n",
    "recall_test = recall_score(y_test_bins, predictions_test, average='weighted')\n",
    "f1_test = f1_score(y_test_bins, predictions_test, average='weighted')\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test_bins, predictions_test)\n",
    "specificity_test = confusion_matrix(y_test_bins, predictions_test)[0,0] / (confusion_matrix(y_test_bins, predictions_test)[0,0] + confusion_matrix(y_test_bins, predictions_test)[0,1])\n",
    "\n",
    "# Printing evaluation metrics\n",
    "print(\"\\nTest data:\")\n",
    "print(\"Accuracy:\", accuracy_test)\n",
    "print(\"Precision:\", precision_test)\n",
    "print(\"Recall:\", recall_test)\n",
    "print(\"F1 Score:\", f1_test)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy_test)\n",
    "print(\"Specificity:\", specificity_test)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix_test = confusion_matrix(y_test_bins, predictions_test)\n",
    "print(\"\\nConfusion Matrix (Test data):\")\n",
    "print(conf_matrix_test)\n",
    "\n",
    "print(outer_cv_scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
